{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from xml.dom.minidom import parse\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from unidecode import unidecode\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getElementByCAML(dom, tagName):\n",
    "    tag = f'caml:{tagName}'\n",
    "    return dom.getElementsByTagName(tag)\n",
    "\n",
    "def xml_data(dom):\n",
    "    data = {}\n",
    "    xml = dom.childNodes[0]\n",
    "    for tag in ['Id', 'VersionNum', 'SessionNum', 'SessionYear', 'MeasureType', 'MeasureNum', 'VersionNum', 'MeasureState', 'VoteRequired', 'FiscalCommittee', 'Appropriation', 'LocalProgram', 'Urgency', 'TaxLevy']:\n",
    "        try:\n",
    "            data[tag] = getElementByCAML(xml, tag)[0].firstChild.data\n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        title = getElementByCAML(xml, 'Title')[0].childNodes\n",
    "        if len(title) == 1:\n",
    "            data['Title'] = title[0].firstChild.data\n",
    "        else:\n",
    "            tilt = [node.data for node in title if node.nodeName == '#text']\n",
    "            data['Title'] = ''.join(tilt)\n",
    "    except:\n",
    "        try:\n",
    "            title = getElementByCAML(xml, 'Title')[0].childNodes\n",
    "            if len(title) == 1:\n",
    "                data['Title'] = title[0].data\n",
    "            else:\n",
    "                tilt = [node.data for node in title if node.nodeName == '#text']\n",
    "                data['Title'] = ''.join(tilt)\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        history = getElementByCAML(xml, 'History')[0].getElementsByTagName('caml:Action')\n",
    "        actions = {}\n",
    "        for action in history:\n",
    "            act = action.childNodes[0].firstChild.data\n",
    "            date = action.childNodes[1].firstChild.data\n",
    "            actions[act] = date\n",
    "        data['History'] = actions\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        auth = []\n",
    "        authors = getElementByCAML(xml, 'Authors')[0].childNodes\n",
    "        for author in authors:\n",
    "            contribution = author.childNodes[0].firstChild.data\n",
    "            house = author.childNodes[1].firstChild.data\n",
    "            name = author.childNodes[2].firstChild.data\n",
    "            auth.append({'contribution': contribution, 'house': house, 'name': name})\n",
    "        data['Authors'] = auth\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        subject = getElementByCAML(xml, 'GeneralSubject')[0].childNodes[0].childNodes\n",
    "        if len(subject) > 1:\n",
    "            node_content = [node.data for node in subject if node.nodeName == '#text']\n",
    "            data['GeneralSubject'] = ''.join(node_content)\n",
    "        else:\n",
    "            data['GeneralSubject'] = subject[0].firstChild.data\n",
    "    except:\n",
    "        try:\n",
    "            subject = getElementByCAML(xml, 'GeneralSubject')[0].childNodes\n",
    "            if len(subject) > 1:\n",
    "                node_content = [node.data for node in subject if node.nodeName == '#text']\n",
    "                data['GeneralSubject'] = ''.join(node_content)\n",
    "            else:\n",
    "                data['GeneralSubject'] = subject[0].firstChild.data\n",
    "        except:\n",
    "            data['GeneralSubject'] = None\n",
    "    try:\n",
    "        digest = getElementByCAML(xml, 'DigestText')[0].childNodes\n",
    "        if all(node.nodeName == 'p' for node in digest):\n",
    "            digestText = \" \".join(digest[i].firstChild.data for i in range(len(digest)))\n",
    "        else:\n",
    "            digestText = \" \".join(digest[i].firstChild.data for i in range(len(digest)) if digest[i].nodeName == 'p')\n",
    "            changes = {'added': [], 'removed': []}\n",
    "            for i in range(len(digest)):\n",
    "                if digest[i].nodeName == 'xm-insertion_mark_start':\n",
    "                    changes['added'].append(digest[i+1].firstChild.data)\n",
    "                elif digest[i].nodeName == 'xm-deletion_mark':\n",
    "                    datt = digest[i].data\n",
    "                    d1 = re.sub(r'&lt;/p>&lt;p>', ' ', datt)\n",
    "                    d2 = re.sub(r'(?:data=\\\"&lt;p>)*\\(\\d\\)&lt;span class=&quot;EnSpace&quot;/>', ' ', d1)\n",
    "                    d3 = re.sub(r'&lt;/*p>', ' ', d2)\n",
    "                    d4 = re.sub(r'\\\"', r'', unidecode(d3))\n",
    "                    d5 = re.sub(r'[\\n\\t\\\\]', ' ', d4)\n",
    "                    d6 = re.sub(r'\\s+', ' ', d5).strip()\n",
    "                    changes['removed'].append(d6)\n",
    "            data['DigestChanges'] = changes\n",
    "        data['DigestText'] = digestText\n",
    "    except:\n",
    "        pass\n",
    "    if 'Id' in data.keys():\n",
    "        id = re.sub(r'_+', '', data['Id'])\n",
    "    else:\n",
    "        try:\n",
    "            session_year = data['SessionYear']\n",
    "            session_num = data['SessionNum']\n",
    "            measure_type = data['MeasureType']\n",
    "            measure_num = data['MeasureNum']\n",
    "            version_num = data['VersionNum']\n",
    "            measure_state = data['MeasureState']\n",
    "            id = f\"{session_year}{session_num}{measure_type}{measure_num}{version_num}{measure_state}\"\n",
    "        except KeyError:\n",
    "            id = None\n",
    "    return id, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml(dom):\n",
    "    return xml_data(dom)\n",
    "\n",
    "def chunked(iterable, chunk_size):\n",
    "    for i in range(0, len(iterable), chunk_size):\n",
    "        yield iterable[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_zip(zip_file_path):\n",
    "    xml_files = []\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "        bill_version_files = [f for f in z.namelist() if (f.startswith('bill_version/')) and (f.endswith('.lob'))]\n",
    "        for file in bill_version_files:\n",
    "            with z.open(file) as f:\n",
    "                try:\n",
    "                    tree = parse(BytesIO(f.read()))\n",
    "                    xml_files.append(tree)\n",
    "                except:\n",
    "                    print(f\"Error parsing {file} in {zip_file_path}\")\n",
    "    print(f\"Processed {zip_file_path}\")\n",
    "    bill_versions = {}\n",
    "    max_workers = 6\n",
    "    chunk_size = 10000\n",
    "\n",
    "    for chunk in chunked(xml_files, chunk_size):\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for id, data in executor.map(process_xml, chunk):\n",
    "                if id:\n",
    "                    bill_versions[id] = data\n",
    "\n",
    "    return bill_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/1999_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/18 [00:56<15:56, 56.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/2001_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2/18 [02:06<17:12, 64.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/2005_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3/18 [03:18<16:56, 67.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/2015_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 4/18 [04:57<18:43, 80.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/1989_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5/18 [05:34<13:58, 64.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/2021_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6/18 [07:25<16:04, 80.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/1997_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7/18 [07:31<10:17, 56.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/2025_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 8/18 [08:45<10:18, 61.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/1993_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 9/18 [08:51<06:40, 44.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/2019_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 10/18 [10:03<07:02, 52.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/1991_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 11/18 [10:35<05:24, 46.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/2009_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 12/18 [12:08<06:04, 60.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/1995_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13/18 [12:15<03:41, 44.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/2023_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 14/18 [14:45<05:04, 76.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/2007_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 15/18 [16:48<04:30, 90.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/2003_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 16/18 [18:10<02:55, 87.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/2013_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 17/18 [19:45<01:30, 90.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/2017_lob_files.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [22:09<00:00, 73.89s/it] \n",
      "100%|██████████| 273274/273274 [00:00<00:00, 375356.87it/s]\n"
     ]
    }
   ],
   "source": [
    "zipped_files = [file for file in os.listdir(os.path.dirname('../etl_data/LOB/')) if file.endswith('.zip')]\n",
    "all_bill_versions = {}\n",
    "\n",
    "for zip_filename in tqdm(zipped_files):\n",
    "    zip_file_path = os.path.join('../etl_data/LOB/', zip_filename)\n",
    "    zip_bill_versions = process_single_zip(zip_file_path)\n",
    "    all_bill_versions.update(zip_bill_versions)\n",
    "\n",
    "\n",
    "flat_keys = [\n",
    "    'MeasureType', 'Urgency', 'MeasureNum', 'GeneralSubject', 'VersionNum',\n",
    "    'Appropriation', 'SessionYear', 'SessionNum', 'VoteRequired',\n",
    "    'LocalProgram', 'FiscalCommittee', 'MeasureState', 'TaxLevy', 'Title'\n",
    "]\n",
    "\n",
    "def extract_entry(item):\n",
    "    id_val, d = item\n",
    "    digest_text = d.get('DigestText', '')\n",
    "    authors = d.get('Authors', [])\n",
    "    history = d.get('History', {})\n",
    "    return {\n",
    "        'digest': (id_val, digest_text),\n",
    "        'authors': [\n",
    "            (id_val, a.get('contribution'), a.get('house'), a.get('name'))\n",
    "            for a in authors if isinstance(a, dict)\n",
    "        ],\n",
    "        'history': [\n",
    "            (id_val, action, date)\n",
    "            for action, date in history.items()\n",
    "        ],\n",
    "        'flat': [id_val] + [d.get(k) for k in flat_keys]\n",
    "    }\n",
    "\n",
    "digest_rows = []\n",
    "author_rows = []\n",
    "history_rows = []\n",
    "other_rows = []\n",
    "max_workers = 4\n",
    "item_list = list(all_bill_versions.items())\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    for result in tqdm(executor.map(extract_entry, item_list), total=len(item_list)):\n",
    "        digest_rows.append(result['digest'])\n",
    "        author_rows.extend(result['authors'])\n",
    "        history_rows.extend(result['history'])\n",
    "        other_rows.append(result['flat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_df = pd.DataFrame(digest_rows, columns=['bill_id', 'DigestText'])\n",
    "authors_df = pd.DataFrame(author_rows, columns=['bill_id', 'Contribution', 'House', 'Name'])\n",
    "history_df = pd.DataFrame(history_rows, columns=['bill_id', 'Action', 'Date'])\n",
    "other_df = pd.DataFrame(other_rows, columns=['bill_id'] + flat_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest = pd.read_csv('../legislation_data/digest.csv', dtype=str)\n",
    "authors = pd.read_csv('../legislation_data/authors.csv', dtype=str)\n",
    "history = pd.read_csv('../legislation_data/history.csv', dtype=str)\n",
    "other = pd.read_csv('../legislation_data/bill_versions.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dig = digest.loc[~digest['bill_id'].isin(digest_df['bill_id'])]\n",
    "digest_df = pd.concat([digest_df, dig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aut = authors.loc[~authors['bill_id'].isin(authors_df['bill_id'])]\n",
    "authors_df = pd.concat([authors_df, aut])\n",
    "his = history.loc[~history['bill_id'].isin(history_df['bill_id'])]\n",
    "history_df = pd.concat([history_df, his])\n",
    "oth = other.loc[~other['bill_id'].isin(other_df['bill_id'])]\n",
    "other_df = pd.concat([other_df, oth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_df.to_csv('../legislation_data/digest.csv', index=False)\n",
    "authors_df.to_csv('../legislation_data/authors.csv', index=False)\n",
    "history_df.to_csv('../legislation_data/history.csv', index=False)\n",
    "other_df.to_csv('../legislation_data/bill_versions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca_leg_etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
