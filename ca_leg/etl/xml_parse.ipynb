{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from xml.dom.minidom import parse\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from unidecode import unidecode\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../etl_data/LOB/1999_lob_files.zip\n",
      "Processed ../etl_data/LOB/2001_lob_files.zip\n",
      "Processed ../etl_data/LOB/2005_lob_files.zip\n",
      "Processed ../etl_data/LOB/2015_lob_files.zip\n",
      "Processed ../etl_data/LOB/1989_lob_files.zip\n",
      "Processed ../etl_data/LOB/2021_lob_files.zip\n",
      "Processed ../etl_data/LOB/1997_lob_files.zip\n",
      "Processed ../etl_data/LOB/2025_lob_files.zip\n",
      "Processed ../etl_data/LOB/1993_lob_files.zip\n",
      "Processed ../etl_data/LOB/2019_lob_files.zip\n",
      "Processed ../etl_data/LOB/1991_lob_files.zip\n",
      "Processed ../etl_data/LOB/2009_lob_files.zip\n",
      "Processed ../etl_data/LOB/1995_lob_files.zip\n",
      "Processed ../etl_data/LOB/2023_lob_files.zip\n",
      "Processed ../etl_data/LOB/2007_lob_files.zip\n",
      "Processed ../etl_data/LOB/2003_lob_files.zip\n",
      "Processed ../etl_data/LOB/2013_lob_files.zip\n",
      "Processed ../etl_data/LOB/2017_lob_files.zip\n"
     ]
    }
   ],
   "source": [
    "xml_files = []\n",
    "zipped_files = [file for file in os.listdir(os.path.dirname('../etl_data/LOB/')) if file.endswith('.zip')]\n",
    "\n",
    "for file in zipped_files:\n",
    "    zip_file = os.path.join('../etl_data/LOB/', file)\n",
    "    with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "        bill_version_files = [f for f in z.namelist() if (f.startswith('bill_version/')) and (f.endswith('.lob'))]\n",
    "        for file in bill_version_files:\n",
    "            with z.open(file) as f:\n",
    "                try:\n",
    "                    tree = parse(BytesIO(f.read()))\n",
    "                    xml_files.append(tree)\n",
    "                except:\n",
    "                    print(file)\n",
    "    print(f\"Processed {zip_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getElementByCAML(dom, tagName):\n",
    "    tag = f'caml:{tagName}'\n",
    "    return dom.getElementsByTagName(tag)\n",
    "\n",
    "def xml_data(dom):\n",
    "    data = {}\n",
    "    xml = dom.childNodes[0]\n",
    "    for tag in ['Id', 'VersionNum', 'SessionNum', 'SessionYear', 'MeasureType', 'MeasureNum', 'VersionNum', 'MeasureState', 'VoteRequired', 'FiscalCommittee', 'Appropriation', 'LocalProgram', 'Urgency', 'TaxLevy']:\n",
    "        try:\n",
    "            data[tag] = getElementByCAML(xml, tag)[0].firstChild.data\n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        title = getElementByCAML(xml, 'Title')[0].childNodes\n",
    "        if len(title) == 1:\n",
    "            data['Title'] = title[0].firstChild.data\n",
    "        else:\n",
    "            tilt = [node.data for node in title if node.nodeName == '#text']\n",
    "            data['Title'] = ''.join(tilt)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        history = getElementByCAML(xml, 'History')[0].getElementsByTagName('caml:Action')\n",
    "        actions = {}\n",
    "        for action in history:\n",
    "            act = action.childNodes[0].firstChild.data\n",
    "            date = action.childNodes[1].firstChild.data\n",
    "            actions[act] = date\n",
    "        data['History'] = actions\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        auth = []\n",
    "        authors = getElementByCAML(xml, 'Authors')[0].childNodes\n",
    "        for author in authors:\n",
    "            contribution = author.childNodes[0].firstChild.data\n",
    "            house = author.childNodes[1].firstChild.data\n",
    "            name = author.childNodes[2].firstChild.data\n",
    "            auth.append({'contribution': contribution, 'house': house, 'name': name})\n",
    "        data['Authors'] = auth\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        subject = getElementByCAML(xml, 'GeneralSubject')[0].childNodes[0].childNodes\n",
    "        if len(subject) > 1:\n",
    "            node_content = [node.data for node in subject if node.nodeName == '#text']\n",
    "            data['GeneralSubject'] = ''.join(node_content)\n",
    "        else:\n",
    "            data['GeneralSubject'] = subject[0].firstChild.data\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        digest = getElementByCAML(xml, 'DigestText')[0].childNodes\n",
    "        if all(node.nodeName == 'p' for node in digest):\n",
    "            digestText = \" \".join(digest[i].firstChild.data for i in range(len(digest)))\n",
    "        else:\n",
    "            digestText = \" \".join(digest[i].firstChild.data for i in range(len(digest)) if digest[i].nodeName == 'p')\n",
    "            changes = {'added': [], 'removed': []}\n",
    "            for i in range(len(digest)):\n",
    "                if digest[i].nodeName == 'xm-insertion_mark_start':\n",
    "                    changes['added'].append(digest[i+1].firstChild.data)\n",
    "                elif digest[i].nodeName == 'xm-deletion_mark':\n",
    "                    datt = digest[i].data\n",
    "                    d1 = re.sub(r'&lt;/p>&lt;p>', ' ', datt)\n",
    "                    d2 = re.sub(r'(?:data=\\\"&lt;p>)*\\(\\d\\)&lt;span class=&quot;EnSpace&quot;/>', ' ', d1)\n",
    "                    d3 = re.sub(r'&lt;/*p>', ' ', d2)\n",
    "                    d4 = re.sub(r'\\\"', r'', unidecode(d3))\n",
    "                    d5 = re.sub(r'[\\n\\t\\\\]', ' ', d4)\n",
    "                    d6 = re.sub(r'\\s+', ' ', d5).strip()\n",
    "                    changes['removed'].append(d6)\n",
    "            data['DigestChanges'] = changes\n",
    "        data['DigestText'] = digestText\n",
    "    except:\n",
    "        pass\n",
    "    if 'Id' in data.keys():\n",
    "        id = re.sub(r'_+', '', data['Id'])\n",
    "    else:\n",
    "        try:\n",
    "            session_year = data['SessionYear']\n",
    "            session_num = data['SessionNum']\n",
    "            measure_type = data['MeasureType']\n",
    "            measure_num = data['MeasureNum']\n",
    "            version_num = data['VersionNum']\n",
    "            measure_state = data['MeasureState']\n",
    "            id = f\"{session_year}{session_num}{measure_type}{measure_num}{version_num}{measure_state}\"\n",
    "        except KeyError:\n",
    "            id = None\n",
    "    return id, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing XML files: 100%|██████████| 267408/267408 [05:58<00:00, 746.64it/s] \n"
     ]
    }
   ],
   "source": [
    "def process_xml(dom):\n",
    "    return xml_data(dom)\n",
    "\n",
    "def chunked(iterable, chunk_size):\n",
    "    for i in range(0, len(iterable), chunk_size):\n",
    "        yield iterable[i:i + chunk_size]\n",
    "\n",
    "bill_versions = {}\n",
    "max_workers = 6\n",
    "chunk_size = 10000\n",
    "with tqdm(total=len(xml_files), desc=\"Processing XML files\") as pbar:\n",
    "    for chunk in chunked(xml_files, chunk_size):\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for id, data in executor.map(process_xml, chunk):\n",
    "                if id:\n",
    "                    bill_versions[id] = data\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267229/267229 [00:01<00:00, 137764.46it/s]\n"
     ]
    }
   ],
   "source": [
    "flat_keys = [\n",
    "    'MeasureType', 'Urgency', 'MeasureNum', 'GeneralSubject', 'VersionNum',\n",
    "    'Appropriation', 'SessionYear', 'SessionNum', 'VoteRequired',\n",
    "    'LocalProgram', 'FiscalCommittee', 'MeasureState', 'TaxLevy', 'Title'\n",
    "]\n",
    "\n",
    "def extract_entry(item):\n",
    "    id_val, d = item\n",
    "    digest_text = d.get('DigestText', '')\n",
    "    authors = d.get('Authors', [])\n",
    "    history = d.get('History', {})\n",
    "    return {\n",
    "        'digest': (id_val, digest_text),\n",
    "        'authors': [\n",
    "            (id_val, a.get('contribution'), a.get('house'), a.get('name'))\n",
    "            for a in authors if isinstance(a, dict)\n",
    "        ],\n",
    "        'history': [\n",
    "            (id_val, action, date)\n",
    "            for action, date in history.items()\n",
    "        ],\n",
    "        'flat': [id_val] + [d.get(k) for k in flat_keys]\n",
    "    }\n",
    "digest_rows = []\n",
    "author_rows = []\n",
    "history_rows = []\n",
    "other_rows = []\n",
    "max_workers = 4\n",
    "item_list = list(bill_versions.items())\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    for result in tqdm(executor.map(extract_entry, item_list), total=len(item_list)):\n",
    "        digest_rows.append(result['digest'])\n",
    "        author_rows.extend(result['authors'])\n",
    "        history_rows.extend(result['history'])\n",
    "        other_rows.append(result['flat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_df = pd.DataFrame(digest_rows, columns=['bill_id', 'DigestText'])\n",
    "authors_df = pd.DataFrame(author_rows, columns=['bill_id', 'Contribution', 'House', 'Name'])\n",
    "history_df = pd.DataFrame(history_rows, columns=['bill_id', 'Action', 'Date'])\n",
    "other_df = pd.DataFrame(other_rows, columns=['bill_id'] + flat_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_df.to_csv('../legislation_data/digest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df.to_csv('../legislation_data/authors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df.to_csv('../legislation_data/history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "other_df.to_csv('../legislation_data/bill_versions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca_leg_etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
