{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bcb83fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d65caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_votes = pd.read_csv('ca_leg/legislation_data/bill_summary_vote_tbl.csv')\n",
    "\n",
    "bill_history = pd.read_csv('ca_leg/legislation_data/bill_history_tbl.csv', dtype={'action_status': str, 'primary_location': str, 'secondary_location': str, 'end_status': str})\n",
    "\n",
    "authors = pd.read_csv('ca_leg/legislation_data/authors.csv')\n",
    "\n",
    "history = pd.read_csv('ca_leg/legislation_data/history.csv')\n",
    "\n",
    "versions = pd.read_csv('ca_leg/legislation_data/bill_versions.csv')\n",
    "\n",
    "bill_votes = pd.read_csv('ca_leg/legislation_data/bill_detail_vote_tbl.csv', parse_dates=['session_date'])\n",
    "\n",
    "bill_summary = pd.read_csv('ca_leg/legislation_data/bill_summary_vote_tbl.csv')\n",
    "\n",
    "bill_motions = pd.read_csv('ca_leg/legislation_data/bill_motion_tbl.csv')\n",
    "\n",
    "locations = pd.read_csv('ca_leg/legislation_data/committee_codes.csv')\n",
    "\n",
    "\n",
    "politicians = pd.read_csv('ca_leg/legislation_data/politicians.csv')\n",
    "\n",
    "\n",
    "lobbying = pd.read_csv('calaccess/lobbying_clean2.csv', dtype={'PAYEE_NAMS': str, 'BAKREF_TID': str})\n",
    "\n",
    "\n",
    "expend_assembly = pd.read_csv('calaccess/expend_assembly_matched.csv', dtype={'TargetPropositionName': str})\n",
    "\n",
    "\n",
    "expend_senate = pd.read_csv('calaccess/expend_senate_matched.csv', dtype={'TargetPropositionName': str})\n",
    "\n",
    "\n",
    "digests = pd.read_csv('ca_leg/legislation_data/digest.csv')\n",
    "\n",
    "\n",
    "hearings = pd.read_csv('ca_leg/legislation_data/committee_hearing_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071a2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes['vote_date_time'] = pd.to_datetime(bill_votes['vote_date_time']).apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "bill_votes['legislator_name'] = bill_votes['legislator_name'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
    "\n",
    "ACTION_KEYWORDS = [\n",
    "    \"Assembly Third Reading\", \"Assembly 3rd reading\", 'senate 3rd reading', \"Senate Third Reading\",\"Concurrence - Urgency Added\", \"Concurrence in Senate Amendments\", \"Do pass as amended, and re-refer\", \"Do pass as amended, but re-refer\", \"Do pass as amended\", \"Do pass and be re-referred\",\n",
    "    \"Concurrence\", \"Consent Calendar\", \"Urgency Clause\", \"Special Consent\",\n",
    "    \"Motion to Reconsider\", \"Do pass\", \"Reconsideration\", \"Committee amendments\",\n",
    "    \"W/O REF. TO FILE\", \"Be re-referred to the Committee\",\n",
    "    \"Lay on the Table\", \"Amend by\", \"Unfinished Business\", \"Placed on Appropriations Suspense File\",\n",
    "]\n",
    "\n",
    "def extract_action(motion_text):\n",
    "    if not isinstance(motion_text, str) or motion_text is None:\n",
    "        return None\n",
    "    motion = motion_text.upper()\n",
    "\n",
    "    action = next((act for act in ACTION_KEYWORDS if act.upper() in motion), None)\n",
    "    if action != 'Reconsideration' and 'RECONSIDER' in motion:\n",
    "        if action is not None:\n",
    "            action += ' Reconsideration'\n",
    "        else:\n",
    "            action = 'Reconsideration'\n",
    "\n",
    "\n",
    "    return action if action else None\n",
    "\n",
    "bill_motions['simplified_motion'] = bill_motions['motion_text'].apply(extract_action)\n",
    "\n",
    "clean_coms = {}\n",
    "for i, row in locations.iterrows():\n",
    "    if row['committee_code'].startswith('CZ'):\n",
    "        continue\n",
    "    name = row['committee_name']\n",
    "    if row['committee_code'].startswith('CS'):\n",
    "        if name.startswith('Sen.'):\n",
    "            cname = re.sub(r'Sen. ', 'senate ', name).lower()\n",
    "        elif name.startswith('Senate '):\n",
    "            cname = name.lower()\n",
    "        else:\n",
    "            cname = 'senate ' + name.lower()\n",
    "    elif row['committee_code'].startswith('CX'):\n",
    "        if name.lower().startswith('assembly'):\n",
    "            cname = name.lower()\n",
    "        else:\n",
    "            cname = 'assembly ' + name.lower()\n",
    "    if re.search(r'x\\d$', cname) is not None:\n",
    "        cname = re.sub(r'x(?=\\d$)', 'no. ', cname)\n",
    "    clean_coms[row['committee_code']] = cname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3500d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "leg_committees = [f\"{row['chamber']} {row['committee_clean']}\".lower() for _, row in politicians[['committee_clean', 'chamber']].drop_duplicates().iterrows()]\n",
    "\n",
    "def match_committees(_names, clean_coms, threshold=92):\n",
    "    clean_c = list(clean_coms.values())\n",
    "    clean_codes = list(clean_coms.keys())\n",
    "    name_mapping = {}\n",
    "    for i, clean in enumerate(clean_c):\n",
    "        code = clean_codes[i]\n",
    "        matches = []\n",
    "        matches.append(process.extractOne(\n",
    "            clean,\n",
    "            _names,\n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            score_cutoff=threshold\n",
    "        ))\n",
    "        matches.append(process.extractOne(\n",
    "            clean,\n",
    "            _names,\n",
    "            scorer=fuzz.partial_ratio,\n",
    "            score_cutoff=threshold\n",
    "        ))\n",
    "        valid_matches = [m for m in matches if m is not None]\n",
    "        if len(valid_matches) > 0:\n",
    "            best_match = max(valid_matches, key=lambda x: x[1])\n",
    "            name_mapping[code] = best_match[0]\n",
    "        else:\n",
    "            fall_back = process.extractOne(\n",
    "                clean,\n",
    "                _names,\n",
    "                scorer=fuzz.token_sort_ratio,\n",
    "                score_cutoff=threshold - 8\n",
    "            )\n",
    "            if fall_back is not None:\n",
    "                name_mapping[code] = fall_back[0]\n",
    "            else:\n",
    "                name_mapping[code] = None\n",
    "    return name_mapping\n",
    "\n",
    "committee_matches = match_committees(leg_committees, clean_coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ea4e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations['committee_clean'] = locations['committee_code'].map(committee_matches)\n",
    "\n",
    "locations.loc[locations['committee_name'] == 'EDUCATION X5', 'committee_clean'] = 'Budget and Fiscal Review: Education'\n",
    "locations.loc[locations['committee_code'] == 'CX12', 'committee_clean'] = 'Budget No. 1 on Health and Human Services'\n",
    "locations.loc[locations['committee_code'] == 'CS68', 'committee_clean'] = 'Budget No. 3 - Health and Human Services'\n",
    "locations.loc[locations['committee_code'] == 'CS66', 'committee_clean'] = 'Senate Veterans Affairs'\n",
    "locations.loc[locations['committee_code'] == 'CS56', 'committee_clean'] = 'Senate Public Employment and Retirement'\n",
    "locations.loc[locations['committee_code'] == 'CS62', 'committee_clean'] = 'Senate Budget and Fiscal Review'\n",
    "locations.loc[locations['committee_code'] == 'CX23', 'committee_clean'] = 'Assembly Utilities and Commerce'\n",
    "\n",
    "motion_codes = {\n",
    "    row['motion_id']: row['simplified_motion']\n",
    "    for _, row in bill_motions.iterrows()\n",
    "}\n",
    "\n",
    "summary_votes['motion_text'] = summary_votes['motion_id'].map(motion_codes)\n",
    "\n",
    "def repair_bill_id(id):\n",
    "    front, end = id[:4], id[4:]\n",
    "    if re.search(r'\\d{4}$', front):\n",
    "        return f\"{front}{int(front) + 1}{end}\"\n",
    "    else:\n",
    "        return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dddf82d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "versions['ID'] = versions['bill_id'].apply(lambda x: repair_bill_id(x))\n",
    "\n",
    "bill_vers = versions.loc[versions['bill_id'].str.startswith('2')]\n",
    "for i, row in bill_vers.iterrows():\n",
    "    tail = f\"{row['VersionNum']}{row['MeasureState']}\"\n",
    "    repaired = repair_bill_id(re.sub(tail, '', row['bill_id']))\n",
    "    end = int(repaired[-4:])\n",
    "\n",
    "    bill_vers.loc[i, 'bill_ID'] = f\"{repaired[:-4]}{end}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d93fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislators = {i: pol for i, pol in enumerate(politicians['full_name'].unique().tolist())}\n",
    "\n",
    "leg_parties = {row['full_name']: row['Party'] for _, row in politicians[['full_name', 'Party']].drop_duplicates().iterrows()}\n",
    "leg_occupations = {row['full_name']: row['Occupation'] for _, row in politicians[['full_name', 'Occupation']].drop_duplicates().iterrows()}\n",
    "committees = {i: com for i, com in enumerate(politicians['committee_clean'].unique().tolist())}\n",
    "lobby_firms = {i: firm for i, firm in enumerate(lobbying['FIRM_NAME'].unique().tolist())}\n",
    "\n",
    "donor_names = list(set(expend_assembly['ExpenderName'].unique().tolist() + expend_senate['ExpenderName'].unique().tolist()))\n",
    "donors = {i: donor for i, donor in enumerate(donor_names)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c1476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_titles = {row['bill_ID']: row['Title'] for _, row in bill_vers[['bill_ID', 'Title']].drop_duplicates().iterrows()}\n",
    "\n",
    "bill_subjects = {row['bill_ID']: row['GeneralSubject'] for _, row in bill_vers.loc[bill_vers['GeneralSubject'].apply(lambda x: x is not None and isinstance(x, str)), ['bill_ID', 'GeneralSubject']].drop_duplicates().iterrows()}\n",
    "\n",
    "bill_ids = list(set(bill_votes.loc[bill_votes['bill_id'].str.startswith('2'), 'bill_id'].unique().tolist() + summary_votes.loc[summary_votes['bill_id'].str.startswith('2'), 'bill_id'].unique().tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6965def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_id_codes = {row['bill_id']: row['bill_ID'] for _, row in bill_vers.drop_duplicates(subset=['bill_id', 'bill_ID']).iterrows()}\n",
    "history['bill_ID'] = history['bill_id'].map(bill_id_codes)\n",
    "\n",
    "history['Date'] = pd.to_datetime(history['Date'])\n",
    "\n",
    "introduction_dates = {}\n",
    "for v, group in history.loc[history['bill_ID'].isin(bill_ids)].groupby('bill_ID'):\n",
    "    introduction_dates[v] = {'Dates': group['Date'].unique().tolist(), 'Actions': group.sort_values('Date', ascending=True).drop_duplicates(subset=['Action', 'Date'])['Action'].tolist()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89455924",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_id_mapping = {i: list(group.values) for i, group in bill_vers.groupby('bill_ID')['ID']}\n",
    "\n",
    "version_id_mapping2 = {i: list(group.values) for i, group in bill_vers.groupby('bill_ID')['bill_id']}\n",
    "bv2b = {v: k for k, val in version_id_mapping2.items() for v in val}\n",
    "history['bill_ID'] = history['bill_id'].map(bv2b)\n",
    "\n",
    "date_ranges = {}\n",
    "\n",
    "for k, v in introduction_dates.items():\n",
    "    first, last = min(v['Dates']), max(v['Dates'])\n",
    "    date_ranges[k] = {'First_action': first, 'Last_action': last}\n",
    "\n",
    "outcomes = history.loc[history['bill_ID'].notna()].sort_values('Date', ascending=False).groupby('bill_ID').first().reset_index()[['bill_ID', 'Action']]\n",
    "outcomes.loc[outcomes['Action'].isin(['CHAPTERED', 'ENROLLED', 'FILED', 'APPROVED']), 'Outcome'] = 1\n",
    "outcomes.loc[outcomes['Action'] == 'VETOED', 'Outcome'] = -1\n",
    "outcomes.loc[outcomes['Outcome'].isna(), 'Outcome'] = 0\n",
    "\n",
    "outcome = outcomes.set_index('bill_ID')['Outcome'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59d42998",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_bill_ids = {}\n",
    "for i in summary_votes.loc[summary_votes['bill_id'].isin(bill_ids)].groupby(['year', 'motion_id'])['bill_id'].value_counts().index:\n",
    "    year, motion_id, bill_id = i\n",
    "    if (year, motion_id) not in vote_bill_ids.keys():\n",
    "        vote_bill_ids[(year, motion_id)] = [bill_id]\n",
    "    else:\n",
    "        vote_bill_ids[(year, motion_id)].append(bill_id)\n",
    "\n",
    "\n",
    "bill_vers_dig = bill_vers.merge(digests, on='bill_id', how='inner')\n",
    "\n",
    "legislators_last_names = {}\n",
    "for _, row in politicians[['chamber', 'Last', 'Term', 'full_name']].drop_duplicates().iterrows():\n",
    "    legislators_last_names[(row['chamber'], row['Last'].lower(), row['Term'])] = row['full_name']\n",
    "\n",
    "features = {row['ID']: {\n",
    "    'digest': row['DigestText'],\n",
    "    'MeasureState': row['MeasureState'],\n",
    "    'VoteRequired': row['VoteRequired'] if row['VoteRequired'] is not None else 'No',\n",
    "    'VersionNum': row['VersionNum'] if row['VersionNum'] is not None else 'No',\n",
    "    'LocalProgram': row['LocalProgram'] if row['LocalProgram'] is not None else 'No',\n",
    "    'FiscalCommittee': row['FiscalCommittee'] if row['FiscalCommittee'] is not None else 'No',\n",
    "    'TaxLevy': row['TaxLevy'] if row['TaxLevy'] is not None else 'No',\n",
    "    'Urgency': row['Urgency'] if row['Urgency'] is not None else 'No'} for _, row in bill_vers_dig.iterrows()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4961885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislator_codes = {v: k for k, v in legislators.items()}\n",
    "\n",
    "committee_codes = {v.lower(): k for k, v in committees.items()}\n",
    "\n",
    "bill_votes['chamber'] = bill_votes['location_code'].apply(lambda x: 'assembly' if x == 'AFLOOR' or x.startswith('CX') else 'senate' if x == 'SFLOOR' or x.startswith('CS') else 'full')\n",
    "bill_votes['vote_date_time'] = pd.to_datetime(bill_votes['vote_date_time'])\n",
    "bill_votes['term'] = bill_votes['vote_date_time'].apply(lambda x: f\"{x.year}-{x.year + 1}\" if x.year % 2 == 1  else f\"{x.year - 1}-{x.year}\" if x.year % 2 == 0 and x < pd.Timestamp(year=x.year, month=11, day=2) else f\"{x.year + 1}-{x.year + 2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c21d075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_locations = authors.loc[(authors['House'] == 'UNKNOWN') & (authors['bill_id'].map(bill_id_codes).isin(bill_ids)), ['bill_id', 'Name']].drop_duplicates()\n",
    "for i, row in author_locations.iterrows():\n",
    "    if 'AB' in row['bill_id']:\n",
    "        author_locations.loc[i, 'name'] = 'Assembly ' + row['Name']\n",
    "    elif 'SB' in row['bill_id']:\n",
    "        author_locations.loc[i, 'name'] = 'Senate ' + row['Name']\n",
    "    else:\n",
    "        author_locations.loc[i, 'name'] = 'Joint ' + row['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65ba96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fuzzy_strings(source_list, target_list):\n",
    "    def preprocess_name(name):\n",
    "        if not isinstance(name, str):\n",
    "            return \"\"\n",
    "        name = name.lower()\n",
    "        name = re.sub(r'\\(.*?\\)', '', name)\n",
    "        name = re.sub(r'committee on', '', name)\n",
    "        name = re.sub(r'[^a-z\\s]', ' ', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        return name\n",
    "\n",
    "    clean_source = [preprocess_name(c) for c in source_list]\n",
    "    clean_target = [preprocess_name(c) for c in target_list]\n",
    "\n",
    "    keywords = [\"education\", \"health\", \"finance\", \"budget\", \"transportation\",\n",
    "                \"judiciary\", \"environment\", \"agriculture\", \"energy\", \"labor\",\n",
    "                \"housing\", \"veterans affairs\", \"public safety\", \"insurance\", \"banking\", \"public health\", \"small business\", \"redistricting\",\n",
    "                \"public utilities\", \"natural resources\", \"water\",\n",
    "                \"technology\", \"communications\", \"elections\", \"government\",\n",
    "                \"appropriations\", \"rules\", \"ethics\", 'criminal justice', \"environmental protection\", \"college and university\", \"human services\", \"reproductive health\", \"mental health\", \"technology\", \"aggriculture\", \"urban development\", \"renewable energy\", \"gun violence\", \"commerce\", \"privacy\", \"cybersecurity\", \"infrastructure\", \"disaster preparedness\", \"prisons\", \"aging\"]\n",
    "\n",
    "    def get_committee_keywords(name):\n",
    "        return set(kw for kw in keywords if kw in name)\n",
    "\n",
    "    target_keywords = [get_committee_keywords(name) for name in clean_target]\n",
    "\n",
    "    def calculate_similarity(source_idx, target_idx):\n",
    "        source = clean_source[source_idx]\n",
    "        target = clean_target[target_idx]\n",
    "\n",
    "        if not source or not target:\n",
    "            return 0\n",
    "\n",
    "        if source == target:\n",
    "            return 100\n",
    "\n",
    "        token_sort = fuzz.token_sort_ratio(source, target)\n",
    "        token_set = fuzz.token_set_ratio(source, target)\n",
    "        partial = fuzz.partial_ratio(source, target)\n",
    "\n",
    "        source_kw = get_committee_keywords(source)\n",
    "        keyword_overlap = len(source_kw.intersection(target_keywords[target_idx]))\n",
    "        keyword_bonus = min(20, keyword_overlap * 10)\n",
    "        weighted_score = (token_sort * 0.3) + (token_set * 0.5) + (partial * 0.2) + keyword_bonus\n",
    "\n",
    "        return weighted_score\n",
    "\n",
    "    matches = {}\n",
    "    for i, source in enumerate(source_list):\n",
    "        scores = [calculate_similarity(i, j) for j in range(len(target_list))]\n",
    "\n",
    "        if not scores or max(scores) < 60:\n",
    "            matches[source] = None\n",
    "        else:\n",
    "            best_idx = np.argmax(scores)\n",
    "            confidence = scores[best_idx]\n",
    "\n",
    "            if confidence >= 60:\n",
    "                matches[source] = target_list[best_idx]\n",
    "            else:\n",
    "                matches[source] = None\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6027033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_com_matches = fuzzy_strings(author_locations['name'].unique().tolist(), leg_committees)\n",
    "\n",
    "author_locations['name'] = author_locations['name'].map(author_com_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f34c237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sponsors = authors.loc[authors['bill_id'].map(bill_id_codes).isin(bill_ids)]\n",
    "sponsors['term'] = sponsors['bill_id'].apply(lambda x: f\"{x[:4]}-{int(x[:4]) + 1}\" if int(x[:4]) % 2 == 1 else f\"{int(x[:4]) - 1}-{x[:4]}\" if int(x[:4]) % 2 == 0 and int(x[:4]) < 2009 else f\"{x[:4]}-{int(x[:4]) + 1}\")\n",
    "\n",
    "lob = lobbying.loc[lobbying['clean_beneficiary'].notna(), ['FIRM_NAME', 'EXPN_DSCR', 'clean_beneficiary', 'EXPN_DATE', 'BENE_AMT']]\n",
    "lob['EXPN_DATE'] = pd.to_datetime(lob['EXPN_DATE'])\n",
    "lob['term'] = lob['EXPN_DATE'].apply(lambda x: f\"{x.year}-{x.year + 1}\" if x.year % 2 == 1  else f\"{x.year - 1}-{x.year}\" if x.year % 2 == 0 and x < pd.Timestamp(year=x.year, month=11, day=2) else f\"{x.year}-{x.year + 1}\")\n",
    "\n",
    "for i, row in politicians.loc[politicians['full_name'].apply(lambda x: isinstance(x, float)), ['Term', 'Last', 'chamber']].drop_duplicates().iterrows():\n",
    "    term, last = row['Term'], row['Last']\n",
    "    a = politicians.loc[(politicians['Last'] == last) & (politicians['Term'] == term) & (politicians['full_name'].apply(lambda x: isinstance(x, str)))]\n",
    "    if len(a) > 0:\n",
    "        politicians.loc[(politicians['Term'] == term) & (politicians['Last'] == last) & (politicians['chamber'] == row['chamber']), 'full_name'] = a['full_name'].values[0]\n",
    "        continue\n",
    "    else:\n",
    "        a = politicians.loc[(politicians['Last'] == last) & (politicians['full_name'].apply(lambda x: isinstance(x, str)))]\n",
    "    if len(a) > 0:\n",
    "        politicians.loc[(politicians['Term'] == term) & (politicians['Last'] == last) & (politicians['chamber'] == row['chamber']), 'full_name'] = a['full_name'].values[0]\n",
    "    else:\n",
    "        print(last, term)\n",
    "\n",
    "\n",
    "pol_names_terms = {}\n",
    "for _, row in politicians[['full_name', 'Term', 'chamber']].drop_duplicates().iterrows():\n",
    "    if ',' in row['full_name']:\n",
    "        name = row['full_name'].split(',')[1].strip() + ' ' + row['full_name'].split(',')[0].strip()\n",
    "    else:\n",
    "        name = row['full_name']\n",
    "    pol_names_terms[(row['full_name'].lower(), row['Term'])] = {'chamber': row['chamber'], 'name': name}\n",
    "\n",
    "expend_assembly = expend_assembly.rename(columns={'term': 'Term'})\n",
    "expend_assembly['chamber'] = 'assembly'\n",
    "expend_senate = expend_senate.rename(columns={'term': 'Term'})\n",
    "expend_senate['chamber'] = 'senate'\n",
    "\n",
    "campaign_contributions = pd.concat([expend_assembly.loc[expend_assembly['matched_target_name'].notna(), ['ExpenderName', 'Amount', 'matched_target_name', 'Term', 'chamber', 'DateEnd']].drop_duplicates(subset=['ExpenderName', 'Amount', 'matched_target_name', 'DateEnd']), expend_senate.loc[expend_senate['matched_target_name'].notna(), ['ExpenderName', 'Amount', 'matched_target_name', 'Term', 'chamber', 'DateEnd']].drop_duplicates(subset=['ExpenderName', 'Amount', 'matched_target_name', 'DateEnd'])])\n",
    "\n",
    "campaign_contributions['DateEnd'] = pd.to_datetime(campaign_contributions['DateEnd'])\n",
    "\n",
    "\n",
    "sponsors['bill_ID'] = sponsors['bill_id'].apply(repair_bill_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07d4151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = history.merge(bill_votes, left_on=['bill_ID', 'Date'], right_on=['bill_id', 'vote_date_time'], how='inner').rename(columns={'bill_id_x': 'bill_version'}).drop('bill_id_y', axis=1)\n",
    "voting['bv_id'] = voting['bill_version'].apply(repair_bill_id)\n",
    "\n",
    "voting_places = {}\n",
    "for i, row in voting.groupby(['motion_id', 'term', 'chamber', 'Date']).agg({'legislator_name': lambda x: list(x)}).iterrows():\n",
    "    motion_id, term, chamber, date = i\n",
    "    g = politicians.loc[(politicians['chamber'] == chamber) & (politicians['Term'] == term) & (politicians['Last'].isin(row['legislator_name']))]\n",
    "    voting_places[(motion_id, term, chamber, date)] = {\n",
    "        'most_common_committee': g.groupby('committee_clean').size().sort_values(ascending=False).head(1).index[0] if len(g) > 0 else None\n",
    "    }\n",
    "voting['voting_place'] = voting.apply(lambda row: voting_places.get((row['motion_id'], row['term'], row['chamber'], row['Date']), {}).get('most_common_committee', None), axis=1)\n",
    "\n",
    "\n",
    "hear = hearings[['bill_id', 'location_code']].merge(locations[['committee_code', 'committee_clean']], left_on='location_code', right_on='committee_code', how='left')[['bill_id', 'committee_clean']].drop_duplicates()\n",
    "hear['year'] = hear['bill_id'].apply(lambda x: int(x[:4]))\n",
    "\n",
    "positions = {p: 'Democratic Alternate' if re.search(r'Democratic\\s*Alternate', p) is not None else 'Vice Chair' if re.search(r'V\\s*i\\s*c\\s*e\\s*-*\\s*C\\s*h\\s*a\\s*i\\s*r\\s*', p) is not None else 'Co-Chair' if re.search(r'Co\\s*-\\s*Chair', p) is not None else 'Chair' if re.search(r'Cha\\s*i\\s*r', p) is not None else 'Republican Alternate' if re.search(r'\\s*Republican\\s*Alternate', p) is not None else p for p in politicians['position'].unique()}\n",
    "\n",
    "vnums = bill_vers.set_index('ID')['VersionNum'].to_dict()\n",
    "vid_map = {v: k for k, val in version_id_mapping.items() for v in val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50edf6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes.merge(bill_vers[['ID', 'bill_ID']], left_on='bill_id', right_on='ID', how='left').drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3591ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, ast, datetime as _dt\n",
    "import geopandas as gpd\n",
    "import tempfile, zipfile, pathlib\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _safe_dt(s):\n",
    "    return pd.to_datetime(s, errors='coerce')\n",
    "\n",
    "def _canon_name(n):\n",
    "    n = re.sub(r'[^\\w\\s]', ' ', str(n)).lower()\n",
    "    n = re.sub(r'\\s+', ' ', n).strip()\n",
    "    return n\n",
    "\n",
    "def _infer_origin_chamber_from_bill_id(bill_id):\n",
    "    s = str(bill_id)\n",
    "    if 'AB' in s: return 'assembly'\n",
    "    if 'SB' in s: return 'senate'\n",
    "    return None\n",
    "\n",
    "def _term_from_date(ts):\n",
    "    # California 2-year terms; November crossover\n",
    "    if pd.isna(ts): return np.nan\n",
    "    y = ts.year\n",
    "    if y % 2 == 1:\n",
    "        return f\"{y}-{y+1}\"\n",
    "    else:\n",
    "        if ts.month < 11:\n",
    "            return f\"{y-1}-{y}\"\n",
    "        return f\"{y+1}-{y+2}\"\n",
    "\n",
    "def _tokenize(s):\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r'[^a-z0-9\\s]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return [t for t in s.split(' ') if t]\n",
    "\n",
    "def _jaccard(a_set, b_set):\n",
    "    if not a_set and not b_set: return 1.0\n",
    "    i = len(a_set & b_set)\n",
    "    u = len(a_set | b_set)\n",
    "    return i / u if u else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3976a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zip(zip_path, crs=3857):\n",
    "    tmp = tempfile.TemporaryDirectory()\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        zf.extractall(tmp.name)\n",
    "    shp = next(pathlib.Path(tmp.name).rglob(\"*.shp\"))\n",
    "    gdf = gpd.read_file(shp).set_crs(epsg=crs)\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "    return gdf, tmp\n",
    "\n",
    "def district_cycle(year):\n",
    "    if year <= 2012: return \"2001\"\n",
    "    if year <= 2022: return \"2011\"\n",
    "    return \"current\"\n",
    "\n",
    "counties_gdf, _ = read_zip('dashboard/backend/data/ca_counties.zip')\n",
    "counties_gdf = counties_gdf[['COUNTYFP', 'NAMELSAD', 'geometry']]\n",
    "counties_gdf['county_area'] = counties_gdf.geometry.area\n",
    "counties_gdf['county_id'] = counties_gdf['COUNTYFP'].astype(int)\n",
    "\n",
    "cgdf = counties_gdf.to_json(na='drop', to_wgs84=True)\n",
    "data_dir = pathlib.Path('dashboard/backend/data')\n",
    "\n",
    "asm11_zip = data_dir / '2011_assembly_state_shp.zip'\n",
    "sen11_zip = data_dir / '2011_senate_state_shp.zip'\n",
    "asmcur_zip = data_dir / '2021_AD_Final_shp.zip'\n",
    "sencur_zip = data_dir / '2021_SD_Final_shp.zip'\n",
    "\n",
    "dist_info = [\n",
    "    (asm11_zip, \"assembly\", \"2011\", 4019),\n",
    "    (sen11_zip, \"senate\",   \"2011\", 4019),\n",
    "    (asmcur_zip, \"assembly\",\"current\", 4269),\n",
    "    (sencur_zip, \"senate\",  \"current\", 4269)\n",
    "]\n",
    "\n",
    "weight_records = []\n",
    "tmps = []\n",
    "for zp, house, cycle, crs in dist_info:\n",
    "    gdf, tmp = read_zip(zp, crs)\n",
    "    tmps.append(tmp)\n",
    "    gdf = gdf.rename(columns={gdf.columns[0]: \"district_id\"})[[\"district_id\", \"geometry\"]]\n",
    "    gdf[\"house\"] = house\n",
    "    gdf[\"cycle\"] = cycle\n",
    "    gdf[\"dist_area\"] = gdf.geometry.area\n",
    "\n",
    "    inter = gpd.overlay(gdf, counties_gdf, how=\"intersection\")\n",
    "    inter[\"fragment_area\"] = inter.geometry.area\n",
    "\n",
    "    weight_records.append(\n",
    "        inter[[\"house\", \"cycle\", \"district_id\", \"county_id\", \"fragment_area\", 'county_area', 'dist_area']].reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "weights = pd.concat(weight_records, ignore_index=True)\n",
    "weights['weight'] = weights['fragment_area'] / weights['county_area']\n",
    "weights['district_share_in_county'] = weights['fragment_area']/weights['dist_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ce22460",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_map = {'AYE':1,'YES':1,'NOE':-1,'NO':-1}\n",
    "voting['vote_num'] = voting['vote_code'].str.upper().map(vote_map).fillna(0).astype(int)\n",
    "motion_dict = bill_motions.set_index('motion_id')['motion_text'].to_dict()\n",
    "\n",
    "# Define a roll call by bill + time + location_code + motion_id\n",
    "roll_cols = ['bill_ID','bill_version','Date','motion_id','chamber','voting_place']\n",
    "roll = (voting\n",
    "        .groupby(roll_cols, dropna=False)\n",
    "        .agg(yes=('vote_num', lambda x: int((np.array(x)>0).sum())),\n",
    "             no =('vote_num', lambda x: int((np.array(x)<0).sum())),\n",
    "             total=('vote_num','count'))\n",
    "        .reset_index())\n",
    "roll['pass'] = (roll['yes'] > roll['no'])\n",
    "\n",
    "bill_votes['vote_num'] = bill_votes['vote_code'].str.upper().map(vote_map).fillna(0).astype(int)\n",
    "bill_votes['Date'] = pd.to_datetime(bill_votes['vote_date_time']).dt.date\n",
    "roll_cols = ['bill_id','Date','motion_id','chamber','location_code']\n",
    "summary_roll = (bill_votes\n",
    "        .groupby(roll_cols, dropna=False)\n",
    "        .agg(yes=('vote_num', lambda x: int((np.array(x)>0).sum())),\n",
    "             no =('vote_num', lambda x: int((np.array(x)<0).sum())),\n",
    "             total=('vote_num','count'))\n",
    "        .reset_index())\n",
    "summary_roll['pass'] = (summary_roll['yes'] > summary_roll['no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03ad8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_55858/3143886450.py:19: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  first_read = g['Date'].min() if g['Date'].any() else pd.NaT\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_55858/3143886450.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stages_df = roll.groupby('bill_ID').apply(_stage_timing).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# ========= Stage timing from votes (committee/floor sequence only) =========\n",
    "# Definitions (fully based on votes):\n",
    "# - intro: earliest vote recorded\n",
    "# - comm_ref: earliest committee vote (any committee)\n",
    "# - first_read: earliest floor vote (any floor)\n",
    "# - second_read: earliest committee vote strictly after first_read\n",
    "# - third_read: earliest floor vote strictly after second_read\n",
    "def _stage_timing(group):\n",
    "    g = group.sort_values('Date')\n",
    "    is_committee = ~(g['voting_place'].isin(['Assembly Floor','Senate Floor']))\n",
    "\n",
    "    # Get the bill_ID from the group (should be consistent within group)\n",
    "    bill_id = g['bill_ID'].iloc[0]\n",
    "    is_floor = summary_roll.loc[(summary_roll['bill_id'] == bill_id) & (summary_roll['location_code'].isin(['AFLOOR','SFLOOR']))]\n",
    "\n",
    "    intro = g['Date'].min()\n",
    "\n",
    "    comm_ref = g.loc[is_committee, 'Date'].min() if is_committee.any() else pd.NaT\n",
    "    first_read = g['Date'].min() if g['Date'].any() else pd.NaT\n",
    "\n",
    "    second_read = pd.NaT\n",
    "    if pd.notna(first_read):\n",
    "        _after1 = g[(g['Date'] > first_read) & (is_committee)]\n",
    "        if not _after1.empty:\n",
    "            second_read = _after1['Date'].min()\n",
    "\n",
    "    third_read = pd.NaT\n",
    "    if pd.notna(second_read):\n",
    "        _after2 = g[(g['Date'] > second_read)]\n",
    "        if not _after2.empty:\n",
    "            third_read = _after2['Date'].min()\n",
    "\n",
    "    # mark which floors passed (simple yes>no rule)\n",
    "    asm_floor_pass = pd.NaT\n",
    "    sen_floor_pass = pd.NaT\n",
    "\n",
    "    if not is_floor.empty:\n",
    "        asm_floor_data = is_floor[(is_floor['location_code'] == 'AFLOOR') & (is_floor['pass'])]\n",
    "        if not asm_floor_data.empty:\n",
    "            asm_floor_pass = asm_floor_data['Date'].min()\n",
    "\n",
    "        sen_floor_data = is_floor[(is_floor['location_code'] == 'SFLOOR') & (is_floor['pass'])]\n",
    "        if not sen_floor_data.empty:\n",
    "            sen_floor_pass = sen_floor_data['Date'].min()\n",
    "\n",
    "    return pd.Series({\n",
    "        'intro': intro, 'comm_ref': comm_ref, 'first_read': first_read,\n",
    "        'second_read': second_read, 'third_read': third_read,\n",
    "        'asm_floor_pass': asm_floor_pass, 'sen_floor_pass': sen_floor_pass\n",
    "    })\n",
    "\n",
    "stages_df = roll.groupby('bill_ID').apply(_stage_timing).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ad5ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = (history.dropna(subset=['bill_ID'])\n",
    "            .sort_values('Date', ascending=False)\n",
    "            .groupby('bill_ID').first().reset_index()[['bill_ID','Action']])\n",
    "outcomes.loc[outcomes['Action'].isin(['CHAPTERED','ENROLLED','FILED','APPROVED']),'Outcome'] = 1\n",
    "outcomes.loc[outcomes['Action'].isin(['VETOED']),'Outcome'] = -1\n",
    "outcomes['Outcome'] = outcomes['Outcome'].fillna(0).astype(int)\n",
    "y_df = outcomes[['bill_ID','Outcome']].rename(columns={'Outcome':'outcome'})\n",
    "\n",
    "# ========= Topic / subject from versions/digests (non-modeled) =========\n",
    "if 'GeneralSubject' in versions.columns:\n",
    "    topic_map_df = (versions[['bill_id','GeneralSubject']]\n",
    "                    .dropna()\n",
    "                    .drop_duplicates()\n",
    "                    .assign(bill_ID=lambda d: d['bill_id'].map(bv2b))\n",
    "                    .dropna(subset=['bill_ID'])\n",
    "                    .drop_duplicates('bill_ID')\n",
    "                    .rename(columns={'GeneralSubject':'topic'})[['bill_ID','topic']])\n",
    "else:\n",
    "    topic_map_df = pd.DataFrame(columns=['bill_ID','topic'])\n",
    "\n",
    "# ========= First/Last action windows for longevity =========\n",
    "first_last = (history.dropna(subset=['bill_ID'])\n",
    "              .groupby('bill_ID')['Date']\n",
    "              .agg(First_action='min', Last_action='max')\n",
    "              .reset_index())\n",
    "\n",
    "# ========= Pipeline base =========\n",
    "pipe_base = (stages_df\n",
    "             .merge(first_last, on='bill_ID', how='left')\n",
    "             .merge(topic_map_df, on='bill_ID', how='left')\n",
    "             .merge(y_df, on='bill_ID', how='left'))\n",
    "\n",
    "# ========= Funnel metrics (consecutive stage pairs) =========\n",
    "stage_order = [c for c in ['intro','comm_ref','first_read','second_read','third_read', 'asm_floor_pass','sen_floor_pass'] if c in pipe_base.columns]\n",
    "pairs = [(stage_order[i], stage_order[i+1]) for i in range(len(stage_order)-1)]\n",
    "\n",
    "rows = []\n",
    "for a,b in pairs:\n",
    "    aa = _safe_dt(pipe_base[a]); bb = _safe_dt(pipe_base[b])\n",
    "    entered  = int(aa.notna().sum())\n",
    "    advanced = int(((aa.notna()) & (bb.notna())).sum())\n",
    "    rate     = float(advanced / entered) if entered else np.nan\n",
    "    mdays    = float(np.median((bb - aa).dt.days.dropna().values)) if advanced else np.nan\n",
    "    rows.append({'from':a,'to':b,'entered':entered,'advanced':advanced,'pass_rate':rate,'median_days':mdays})\n",
    "pipeline_stage_funnel = pd.DataFrame(rows)\n",
    "\n",
    "pipeline_timestamps_wide = pipe_base[['bill_ID','topic'] + stage_order].copy()\n",
    "for s in stage_order:\n",
    "    pipeline_timestamps_wide[f'{s}_ts'] = _safe_dt(pipeline_timestamps_wide[s]).astype('int64', errors='ignore')//10**9\n",
    "\n",
    "# ========= Stuck candidates (present at a, missing b; with 90th pct ref time) =========\n",
    "stuck_rows=[]\n",
    "for a,b in pairs:\n",
    "    aa = _safe_dt(pipe_base[a]); bb = _safe_dt(pipe_base[b])\n",
    "    dd = (bb - aa).dt.days\n",
    "    q90 = np.nanpercentile(dd.dropna().values, 90) if dd.notna().any() else np.nan\n",
    "    sub = pipe_base[(aa.notna()) & (bb.isna())][['bill_ID','topic']].copy()\n",
    "    if not sub.empty:\n",
    "        sub['stage']=a; sub['q90']=q90\n",
    "        stuck_rows.append(sub)\n",
    "pipeline_stuck_candidates = pd.concat(stuck_rows, ignore_index=True) if stuck_rows else pd.DataFrame(columns=['bill_ID','topic','stage','q90'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8f78303",
   "metadata": {},
   "outputs": [],
   "source": [
    "hear_seq = (hearings[['bill_id','location_code']]\n",
    "            .merge(locations[['committee_code','committee_clean']],\n",
    "                   left_on='location_code', right_on='committee_code', how='left')\n",
    "            .rename(columns={'committee_clean':'committee'})\n",
    "           )\n",
    "\n",
    "route_df = (hear_seq.groupby('bill_id')['committee']\n",
    "            .apply(lambda s: tuple([x for x in s.dropna().tolist() if x]))\n",
    "            .reset_index()\n",
    "            .rename(columns={'committee':'route'}))\n",
    "route_df['route_key'] = route_df['route'].apply(lambda r: ' > '.join(list(r)[:5]) if isinstance(r, tuple) and r else None)\n",
    "route_df.rename(columns={'bill_id':'bill_ID'}, inplace=True)\n",
    "\n",
    "route_perf = (route_df\n",
    "              .merge(y_df, on='bill_ID', how='left')\n",
    "              .merge(topic_map_df, on='bill_ID', how='left'))\n",
    "route_archetypes = (route_perf.groupby(['topic','route_key'])\n",
    "                    .agg(n=('bill_ID','nunique'),\n",
    "                         pass_rate=('outcome', lambda x: float(np.mean(np.array(x)==1)) if len(x)>0 else np.nan))\n",
    "                    .reset_index()\n",
    "                    .sort_values(['topic','n'], ascending=[True,False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fc799af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_55858/683124007.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  amendment_churn = (dv.groupby('bill_ID').apply(_digest_stats).reset_index()\n"
     ]
    }
   ],
   "source": [
    "dig = digests[['bill_id','DigestText']].copy()\n",
    "dig['bill_ID'] = dig['bill_id'].map(bv2b)\n",
    "ver = versions[['bill_id','VersionNum']].copy()\n",
    "ver['bill_ID'] = ver['bill_id'].map(bv2b)\n",
    "dv = (ver.merge(dig, on=['bill_id','bill_ID'], how='inner')\n",
    "         .dropna(subset=['DigestText']))\n",
    "\n",
    "def _digest_stats(df):\n",
    "    df = df.sort_values('VersionNum')\n",
    "    toks = [set(_tokenize(t)) for t in df['DigestText']]\n",
    "    sims=[]\n",
    "    for i in range(1,len(toks)):\n",
    "        sims.append(_jaccard(toks[i-1], toks[i]))\n",
    "    return pd.Series({'n_versions': len(df), 'median_sim': float(np.median(sims)) if sims else np.nan})\n",
    "\n",
    "amendment_churn = (dv.groupby('bill_ID').apply(_digest_stats).reset_index()\n",
    "                   .merge(topic_map_df, on='bill_ID', how='left'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a305de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = pd.Timestamp.now().date()\n",
    "pb = pipe_base[['bill_ID','topic'] + stage_order].copy()\n",
    "for c in stage_order: pb[c] = _safe_dt(pb[c])\n",
    "pb['last_date'] = pb[stage_order].max(axis=1)\n",
    "\n",
    "by_stage_q80 = {}\n",
    "for c in stage_order:\n",
    "    dd = pb[c].apply(lambda x: (now - x.date()).days if pd.notna(x) else np.nan)\n",
    "    by_stage_q80[c] = np.nanpercentile(dd.dropna().values, 80) if dd.notna().any() else np.nan\n",
    "\n",
    "last_stage_col = None\n",
    "for c in reversed(stage_order):\n",
    "    if pb[c].notna().any(): last_stage_col = c; break\n",
    "\n",
    "pb = pb.merge(route_archetypes[['route_key','topic','pass_rate']].drop_duplicates(subset=['route_key','topic']),\n",
    "              on='topic', how='left')\n",
    "pb = pb.merge(route_df[['bill_ID','route_key']], on='bill_ID', how='left')\n",
    "pb = pb.merge(amendment_churn[['bill_ID','n_versions']], on='bill_ID', how='left')\n",
    "\n",
    "def _risk_row(r):\n",
    "    q80 = by_stage_q80.get(last_stage_col, np.inf)\n",
    "    churn  = (r.get('n_versions',0) or 0) >= 5\n",
    "    low_route = (r.get('pass_rate',1.0) or 1.0) < 0.3\n",
    "    return int(sum([churn, low_route]))\n",
    "\n",
    "pb['risk'] = pb.apply(_risk_row, axis=1)\n",
    "risk_list = pb[['bill_ID','topic','route_key_x','n_versions','risk']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3476b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = stages_df[['bill_ID','comm_ref']].dropna()\n",
    "exits = stages_df[['bill_ID'] + [c for c in stage_order if c!='comm_ref']].copy()\n",
    "exits['has_exit'] = exits.drop(columns=['bill_ID']).notna().any(axis=1)\n",
    "gate = entries.merge(exits[['bill_ID','has_exit']], on='bill_ID', how='left')\n",
    "\n",
    "heard = hear_seq[['bill_id','committee']].dropna().drop_duplicates().rename(columns={'bill_id':'bill_ID'})\n",
    "gk = heard.merge(gate[['bill_ID','has_exit']], on='bill_ID', how='left')\n",
    "committee_gatekeeping = (gk.groupby('committee')\n",
    "                         .agg(entries=('bill_ID','nunique'),\n",
    "                              exits=('has_exit', lambda x: int(np.nansum(np.array(x)==True))))\n",
    "                         .reset_index())\n",
    "committee_gatekeeping['gatekeeping'] = 1 - (committee_gatekeeping['exits'] /\n",
    "                                            committee_gatekeeping['entries'].replace(0, np.nan))\n",
    "\n",
    "hear_dates = history[['bill_ID','Date','Action']].copy()\n",
    "hear_dates = hear_dates[hear_dates['Action'].str.upper()\n",
    "                        .str.contains('HEARING|REFERRED|RE-REFERRED|COMMITTEE', na=False)]\n",
    "hear_seq.rename(columns={'bill_id':'bill_ID'}, inplace=True)\n",
    "hear_dates['week'] = _safe_dt(hear_dates['Date']).dt.to_period('W').astype(str)\n",
    "committee_workload_median = (hear_seq.merge(hear_dates[['bill_ID','week']], on='bill_ID', how='left')\n",
    "                             .groupby(['committee','week'])\n",
    "                             .agg(bills=('bill_ID','nunique'))\n",
    "                             .groupby('committee')\n",
    "                             .agg(median_weekly_bills=('bills','median'))\n",
    "                             .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d3861b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = stages_df[['bill_ID']].copy()\n",
    "origin['origin'] = origin['bill_ID'].apply(_infer_origin_chamber_from_bill_id)\n",
    "cc = stages_df[['bill_ID','asm_floor_pass','sen_floor_pass']].copy()\n",
    "cc['A_then_S'] = cc['asm_floor_pass'].notna() & cc['sen_floor_pass'].notna()\n",
    "cc['S_then_A'] = cc['sen_floor_pass'].notna() & cc['asm_floor_pass'].notna()\n",
    "cross_chamber_friction = (cc.merge(topic_map_df, on='bill_ID', how='left')\n",
    "                          .groupby('topic')\n",
    "                          .agg(pass_Asm_then_Sen=('A_then_S', lambda x: int(np.nansum(x))),\n",
    "                               pass_Sen_then_Asm=('S_then_A', lambda x: int(np.nansum(x))))\n",
    "                          .reset_index())\n",
    "\n",
    "sv = stages_df[['bill_ID','intro']].merge(y_df, on='bill_ID', how='left')\n",
    "sv['start'] = _safe_dt(sv['intro'])\n",
    "ends = first_last[['bill_ID','Last_action']].rename(columns={'Last_action':'end'})\n",
    "sv = sv.merge(ends, on='bill_ID', how='left')\n",
    "sv['end'] = _safe_dt(sv['end'])\n",
    "sv = sv.merge(topic_map_df, on='bill_ID', how='left')\n",
    "\n",
    "def _survival_topic(df):\n",
    "    df = df.dropna(subset=['start'])\n",
    "    if df.empty: return pd.DataFrame(columns=['t','survival'])\n",
    "    t0 = df['start'].min()\n",
    "    t1 = df['end'].max() if df['end'].notna().any() else t0 + pd.Timedelta(days=1)\n",
    "    grid = pd.date_range(t0, t1, freq='7D')\n",
    "    rows=[]\n",
    "    for g in grid:\n",
    "        alive = ((df['end'].isna()) | (df['end'] > g)).sum()\n",
    "        total = len(df)\n",
    "        rows.append({'t': g, 'survival': alive/total if total else np.nan})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "_surv = []\n",
    "for topic, g in sv.groupby('topic'):\n",
    "    sdf = _survival_topic(g)\n",
    "    sdf['topic'] = topic\n",
    "    _surv.append(sdf)\n",
    "survival_curves = pd.concat(_surv, ignore_index=True) if _surv else pd.DataFrame(columns=['t','survival','topic'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "80d2bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = voting[['bill_ID','legislator_name','vote_code','location_code']].copy()\n",
    "vote_num = {'AYE':1,'YES':1,'NOE':-1,'NO':-1}\n",
    "v['vote'] = v['vote_code'].str.upper().map(vote_num).fillna(0).astype(int)\n",
    "mat = v.pivot_table(index='legislator_name', columns='bill_ID', values='vote', aggfunc='first').fillna(0).astype(int)\n",
    "l = mat.index.to_list()\n",
    "sim_edges=[]\n",
    "if mat.shape[0] >= 2:\n",
    "    X = mat.to_numpy(dtype=np.float32)\n",
    "    Xc = X - X.mean(axis=1, keepdims=True)\n",
    "    denom = np.sqrt((Xc**2).sum(axis=1, keepdims=True)); denom[denom==0]=1.0\n",
    "    Xn = Xc/denom\n",
    "    for i in range(Xn.shape[0]):\n",
    "        dots = Xn[i] @ Xn.T\n",
    "        dots[i] = -1\n",
    "        idx = np.where(dots>=0.6)[0]\n",
    "        for j in idx:\n",
    "            if i<j:\n",
    "                sim_edges.append((l[i], l[j], float(dots[j])))\n",
    "vote_similarity_edges = pd.DataFrame(sim_edges, columns=['u','v','sim'])\n",
    "\n",
    "adj=defaultdict(list)\n",
    "for _,r in vote_similarity_edges.iterrows():\n",
    "    adj[r['u']].append(r['v']); adj[r['v']].append(r['u'])\n",
    "visited=set(); comps=[]\n",
    "for node in l:\n",
    "    if node in visited: continue\n",
    "    q=deque([node]); comp=[]\n",
    "    while q:\n",
    "        x=q.popleft()\n",
    "        if x in visited: continue\n",
    "        visited.add(x); comp.append(x)\n",
    "        for nb in adj.get(x, []):\n",
    "            if nb not in visited: q.append(nb)\n",
    "    comps.append(comp)\n",
    "vote_communities = pd.DataFrame([(n,i) for i,comp in enumerate(comps) for n in comp],\n",
    "                                columns=['legislator_name','community'])\n",
    "\n",
    "vc = voting[['legislator_name','vote_code','location_code']].copy()\n",
    "vc['is_floor'] = vc['location_code'].isin(['AFLOOR','SFLOOR'])\n",
    "vc['yes'] = vc['vote_code'].str.upper().isin(['AYE','YES']).astype(int)\n",
    "leg_comm = vc[~vc['is_floor']].groupby('legislator_name')['yes'].mean().rename('comm_yes')\n",
    "leg_floor = vc[vc['is_floor']].groupby('legislator_name')['yes'].mean().rename('floor_yes')\n",
    "committee_floor_drift = (pd.concat([leg_comm, leg_floor], axis=1)\n",
    "                         .reset_index())\n",
    "committee_floor_drift['drift'] = committee_floor_drift['floor_yes'] - committee_floor_drift['comm_yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ceccbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dig_m = digests[['bill_id','DigestText']].copy()\n",
    "dig_m['bill_ID'] = dig_m['bill_id'].map(bv2b)\n",
    "dig_m = dig_m.dropna(subset=['bill_ID','DigestText'])\n",
    "dig_m = dig_m.merge(y_df, on='bill_ID', how='left')\n",
    "\n",
    "def _lift(df):\n",
    "    toks_pos=defaultdict(int); toks_neg=defaultdict(int)\n",
    "    for _,r in df.iterrows():\n",
    "        toks=set(_tokenize(r['DigestText']))\n",
    "        if int(r.get('outcome',0))==1:\n",
    "            for t in toks: toks_pos[t]+=1\n",
    "        else:\n",
    "            for t in toks: toks_neg[t]+=1\n",
    "    rows=[]\n",
    "    all_t = set(list(toks_pos.keys())+list(toks_neg.keys()))\n",
    "    for t in all_t:\n",
    "        pos = toks_pos.get(t,0)+1\n",
    "        neg = toks_neg.get(t,0)+1\n",
    "        rows.append((t, float(np.log(pos/neg)), toks_pos.get(t,0), toks_neg.get(t,0)))\n",
    "    return (pd.DataFrame(rows, columns=['token','log_lift_pass_vs_other','pos','neg'])\n",
    "              .sort_values('log_lift_pass_vs_other', ascending=False))\n",
    "\n",
    "text_lift_top_tokens = _lift(dig_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "711c17d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_55858/1481952033.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  donor_portfolios_hhi = (port.groupby('ExpenderName').apply(_hhi)\n"
     ]
    }
   ],
   "source": [
    "ca = pd.concat([\n",
    "    expend_assembly[['ExpenderName','Amount','matched_target_name','Term','DateEnd']].dropna(subset=['ExpenderName','Amount','matched_target_name']),\n",
    "    expend_senate  [['ExpenderName','Amount','matched_target_name','Term','DateEnd']].dropna(subset=['ExpenderName','Amount','matched_target_name'])\n",
    "], ignore_index=True)\n",
    "\n",
    "port = ca.groupby(['ExpenderName','matched_target_name'])['Amount'].sum().reset_index()\n",
    "def _hhi(g):\n",
    "    s = g['Amount'].sum()\n",
    "    if s<=0: return np.nan\n",
    "    p = (g['Amount']/s).values\n",
    "    return float(np.sum(p*p))\n",
    "donor_portfolios_hhi = (port.groupby('ExpenderName').apply(_hhi)\n",
    "                        .reset_index().rename(columns={0:'hhi'}))\n",
    "\n",
    "# Lobbying firm × committee heatmap (beneficiary string mapped to known committees where possible)\n",
    "known_committees = locations[['committee_code','committee_name']].dropna()\n",
    "kc = {c.lower():n for c,n in zip(known_committees['committee_code'], known_committees['committee_name'])}\n",
    "lob = lobbying[['FIRM_NAME','clean_beneficiary','BENE_AMT']].dropna().copy()\n",
    "lob['benef_code'] = lob['clean_beneficiary'].str.upper()\n",
    "lob['committee'] = np.where(lob['benef_code'].str.startswith(('CX','CS')),\n",
    "                            lob['benef_code'].str.lower().map({k.lower():v for k,v in kc.items()}),\n",
    "                            np.nan)\n",
    "lobbying_firm_committee_heatmap = (lob.dropna(subset=['committee'])\n",
    "                                   .groupby(['FIRM_NAME','committee'])\n",
    "                                   .agg(contacts=('committee','count'), spend=('BENE_AMT','sum'))\n",
    "                                   .reset_index())\n",
    "\n",
    "# money-vote alignment (by term, crude quartile split)\n",
    "vt = voting[['legislator_name','vote_code','vote_date_time']].copy()\n",
    "vt['canon'] = vt['legislator_name'].apply(_canon_name)\n",
    "vt['term'] = vt['vote_date_time'].apply(_term_from_date)\n",
    "vt['yes'] = vt['vote_code'].str.upper().isin(['AYE','YES']).astype(int)\n",
    "leg_term_rate = vt.groupby(['canon','term'])['yes'].mean().reset_index().rename(columns={'yes':'yes_rate'})\n",
    "\n",
    "don = ca.copy()\n",
    "don['canon'] = don['matched_target_name'].apply(_canon_name)\n",
    "fund = (don.groupby(['canon','Term'])['Amount'].sum()\n",
    "        .reset_index().rename(columns={'Term':'term','Amount':'funding'}))\n",
    "ft = fund.merge(leg_term_rate, on=['canon','term'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f3fd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _quartiles(g):\n",
    "    if g.empty: return pd.Series({'yes_rate_top':np.nan,'yes_rate_bottom':np.nan,'delta':np.nan,'n_top':0,'n_bottom':0})\n",
    "    q = g['funding'].quantile([0.25,0.75]).values\n",
    "    low = g[g['funding']<=q[0]]; high = g[g['funding']>=q[1]]\n",
    "    return pd.Series({'yes_rate_top': float(high['yes_rate'].mean()) if not high.empty else np.nan,\n",
    "                      'yes_rate_bottom': float(low['yes_rate'].mean()) if not low.empty else np.nan,\n",
    "                      'delta': float((high['yes_rate'].mean() - low['yes_rate'].mean())) if (not high.empty and not low.empty) else np.nan,\n",
    "                      'n_top': int(high.shape[0]), 'n_bottom': int(low.shape[0])})\n",
    "money_vote_alignment = ft.groupby('term').apply(_quartiles, include_groups=False).reset_index()\n",
    "\n",
    "# Event-time spending around legislative window\n",
    "cc = ca[['DateEnd','Amount']].dropna().copy()\n",
    "cc['DateEnd'] = pd.to_datetime(_safe_dt(cc['DateEnd'])).dt.date\n",
    "start_min = pd.to_datetime(_safe_dt(first_last['First_action']).min() if first_last['First_action'].notna().any() else cc['DateEnd'].min()).date()\n",
    "cc['t'] = cc['DateEnd'].apply(lambda d: (d - start_min).days if pd.notna(d) else np.nan)\n",
    "money_event_time_curve = (cc.groupby('t')['Amount'].mean().reset_index().sort_values('t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7efbac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_roster = politicians[['District No.','Term','full_name','chamber']].drop_duplicates().copy()\n",
    "pol_roster['district_id'] = pd.to_numeric(pol_roster['District No.'], errors='coerce')\n",
    "\n",
    "# donations (assembly/senate files)\n",
    "ea = expend_assembly.copy()\n",
    "ea = ea[ea['Term'].apply(lambda x: isinstance(x,str))]\n",
    "ea['term_year'] = ea['Term'].str.extract(r'^(\\d{4})').astype(int)\n",
    "ea['term_year'] = np.where((ea['term_year']%2==0), ea['term_year']-1, ea['term_year'])\n",
    "ea_agg = ea.groupby(['matched_target_name','term_year'])['Amount'].sum().reset_index()\n",
    "\n",
    "es = expend_senate.copy()\n",
    "es = es[es['Term'].apply(lambda x: isinstance(x,str))]\n",
    "es['term_year'] = es['Term'].str.extract(r'^(\\d{4})').astype(int)\n",
    "es['term_year'] = np.where((es['term_year']%2==0), es['term_year']-1, es['term_year'])\n",
    "es_agg = es.groupby(['matched_target_name','term_year'])['Amount'].sum().reset_index()\n",
    "\n",
    "# lobbying by legislator-term\n",
    "lb = lobbying[['clean_beneficiary','EXPN_DATE','BENE_AMT']].dropna().copy()\n",
    "lb['EXPN_DATE'] = _safe_dt(lb['EXPN_DATE'])\n",
    "lb['term_year'] = lb['EXPN_DATE'].apply(lambda x: np.nan if pd.isna(x) else (x.year-1 if (x.year%2==0 and x.month<11) else (x.year+1 if x.year%2==0 else x.year)))\n",
    "lob_ag = lb.groupby(['clean_beneficiary','term_year'])['BENE_AMT'].sum().reset_index().rename(columns={'BENE_AMT':'total_lobbying'})\n",
    "\n",
    "def _nkey(s): return _canon_name(re.sub(r',','', str(s)))\n",
    "pol_roster['name_key'] = pol_roster['full_name'].apply(_nkey)\n",
    "ea_agg['name_key'] = ea_agg['matched_target_name'].apply(_nkey)\n",
    "es_agg['name_key'] = es_agg['matched_target_name'].apply(_nkey)\n",
    "lob_ag['name_key'] = lob_ag['clean_beneficiary'].apply(_nkey)\n",
    "pol_roster['term_year'] = pol_roster['Term'].str.extract(r'^(\\d{4})').astype(float)\n",
    "\n",
    "lfund = (pol_roster\n",
    "         .merge(lob_ag[['name_key','term_year','total_lobbying']], on=['name_key','term_year'], how='left')\n",
    "         .merge(ea_agg[['name_key','term_year','Amount']].rename(columns={'Amount':'don_a'}), on=['name_key','term_year'], how='left')\n",
    "         .merge(es_agg[['name_key','term_year','Amount']].rename(columns={'Amount':'don_s'}), on=['name_key','term_year'], how='left'))\n",
    "\n",
    "lfund['total_donations'] = lfund[['don_a','don_s']].sum(axis=1, skipna=True)\n",
    "lfund['total_lobbying']  = lfund['total_lobbying'].fillna(0)\n",
    "lfund['total_received']  = lfund[['total_donations','total_lobbying']].sum(axis=1, skipna=True)\n",
    "\n",
    "# map roster cycle to weights cycle\n",
    "lfund['cycle'] = np.where(lfund['term_year']<=2012, '2011', 'current')\n",
    "\n",
    "# allocate district totals to counties\n",
    "reg = (lfund[['cycle','district_id','chamber','total_donations','total_lobbying','total_received']]\n",
    "       .merge(weights, on=['cycle','district_id'], how='left'))\n",
    "for c in ['total_donations','total_lobbying','total_received']:\n",
    "    reg[c] = reg[c] * reg['district_share_in_county']\n",
    "reg_funds = (reg.groupby(['county_id','chamber'])\n",
    "             .agg(total_donations=('total_donations','sum'),\n",
    "                  total_lobbying=('total_lobbying','sum'),\n",
    "                  total_received=('total_received','sum'))\n",
    "             .reset_index())\n",
    "\n",
    "co_cal = reg_funds.merge(counties_gdf[['county_id','NAMELSAD','geometry']], on='county_id', how='left')\n",
    "ca_legislator_funding_geo = gpd.GeoDataFrame(co_cal, geometry='geometry', crs=counties_gdf.crs).to_crs(epsg=4326)\n",
    "ca_legislator_funding = reg_funds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b6262b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_55858/4039378295.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: float(np.mean((g['yes']/(g['total'].replace(0, np.nan))) >= 0.5)))\n"
     ]
    }
   ],
   "source": [
    "bill_dates_df = first_last.copy()\n",
    "bill_dates_df['longevity_days'] = (bill_dates_df['Last_action'] - bill_dates_df['First_action']).dt.days\n",
    "signals = (roll.groupby('bill_ID')\n",
    "           .apply(lambda g: float(np.mean((g['yes']/(g['total'].replace(0, np.nan))) >= 0.5)))\n",
    "           .reset_index().rename(columns={0:'vote_signal'}))\n",
    "n_versions = (versions.assign(bill_ID=lambda d: d['bill_id'].map(bv2b))\n",
    "              .dropna(subset=['bill_ID'])\n",
    "              .groupby('bill_ID')['VersionNum'].nunique()\n",
    "              .reset_index().rename(columns={'VersionNum':'bill_version_count'}))\n",
    "\n",
    "bills_table = (topic_map_df\n",
    "               .merge(y_df, on='bill_ID', how='left')\n",
    "               .merge(bill_dates_df[['bill_ID','First_action','longevity_days']], on='bill_ID', how='left')\n",
    "               .merge(signals, on='bill_ID', how='left')\n",
    "               .merge(amendment_churn[['bill_ID','n_versions','median_sim']], on='bill_ID', how='left')\n",
    "               .merge(n_versions, on='bill_ID', how='left'))\n",
    "bills_table['First_action'] = pd.to_datetime(bills_table['First_action']).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ea6f2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "precomp_outputs = {\n",
    "    'pipeline_stage_funnel': pipeline_stage_funnel,\n",
    "    'pipeline_timestamps_wide': pipeline_timestamps_wide,\n",
    "    'pipeline_stuck_candidates': pipeline_stuck_candidates,\n",
    "    'route_archetypes': route_archetypes,\n",
    "    'amendment_churn': amendment_churn,\n",
    "    'risk_list': risk_list,\n",
    "    'committee_gatekeeping': committee_gatekeeping,\n",
    "    'committee_workload_median': committee_workload_median,\n",
    "    'cross_chamber_friction': cross_chamber_friction,\n",
    "    'survival_curves': survival_curves,\n",
    "    'vote_similarity_edges': vote_similarity_edges,\n",
    "    'vote_communities': vote_communities,\n",
    "    'committee_floor_drift': committee_floor_drift,\n",
    "    'text_lift_top_tokens': text_lift_top_tokens,\n",
    "    'donor_portfolios_hhi': donor_portfolios_hhi,\n",
    "    'lobbying_firm_committee_heatmap': lobbying_firm_committee_heatmap,\n",
    "    'money_vote_alignment': money_vote_alignment,\n",
    "    'money_event_time_curve': money_event_time_curve,\n",
    "    'ca_legislator_funding_geo': ca_legislator_funding_geo,  # GeoDataFrame\n",
    "    'ca_legislator_funding': ca_legislator_funding,          # tabular\n",
    "    'bills_table': bills_table\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc0822e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
