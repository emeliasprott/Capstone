{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Up the Data\n",
    "In this notebook, I will identify the relationships between all of the data. Many of the relationships depend on string matches, but the text data is inconsistent and has many typos. For example, *'Assemblymember'* could be written as *'Assemblyman'*, *'Assemblywoman'*, *'A semblmember'*, and more. At the same time, with so many repeated words and phrases, many strings appear to match when they should not. A simple string-distance algorithm might find 'Assemblymember David Chiu' to match with 'Assemblymember Dave Chu', which is not correct. Therefore I use an approach that combines fuzzy string matching and regex  with spacy token similarity and entity linking to match the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_15784/176948873.py:1: DtypeWarning: Columns (4,6,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  bill_analysis = pd.read_csv('ca_leg/legislation_data/bill_analysis_tbl.csv')\n"
     ]
    }
   ],
   "source": [
    "bill_analysis = pd.read_csv('ca_leg/legislation_data/bill_analysis_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_codes = bill_analysis.loc[bill_analysis['committee_code'].notna(), ['committee_code', 'committee_name']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_codes.to_csv('ca_leg/legislation_data/committee_codes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills = pd.read_csv('ca_leg/legislation_data/bill_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_history = pd.read_csv('ca_leg/legislation_data/bill_history_tbl.csv', dtype={'action_status': str, 'primary_location': str, 'secondary_location': str, 'end_status': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_versions = pd.read_csv('ca_leg/legislation_data/bill_version_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ca_leg/legislation_data/bill_version_text.json\", \"r\") as f:\n",
    "    bill_text = json.load(f)\n",
    "    authors_data = {bill_id: bill_info['Authors']\n",
    "                   for bill_id, bill_info in bill_text.items()\n",
    "                   if 'Authors' in bill_info}\n",
    "    bill_text_data = {}\n",
    "    for bill_id, bill_info in bill_text.items():\n",
    "        record = {}\n",
    "        if 'Title' in bill_info.keys():\n",
    "            title = bill_info.get('Title')\n",
    "            record.update({'title': title})\n",
    "        if 'GeneralSubject' in bill_info.keys():\n",
    "            general_subject = bill_info.get('GeneralSubject')\n",
    "            record.update({'general_subject': general_subject})\n",
    "        if 'DigestText' in bill_info.keys():\n",
    "            digest_text = bill_info.get('DigestText')\n",
    "            record.update({'digest_text': digest_text})\n",
    "        if 'BillContent' in bill_info.keys():\n",
    "            content = bill_info.get('BillContent')\n",
    "            record.update({'content': content})\n",
    "        bill_text_data[bill_id] = record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRINCIPAL_COAUTHOR', 'COAUTHOR', 'null', 'LEAD_AUTHOR']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([a for b in [v.keys() for v in authors_data.values()] for a in b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for bill_id, authors in authors_data.items():\n",
    "    for author_type, house in authors.items():\n",
    "        for house_name, author_name in house.items():\n",
    "            records.append([bill_id, author_type, \"COMMITTEE\" if house_name == 'UNKNOWN' else house_name, author_name])\n",
    "\n",
    "df = pd.DataFrame(records, columns=['bill_id', 'author_type', 'house', 'author_name'])\n",
    "df['bill_id'] = df['bill_id'].apply(lambda x: re.sub(r'__', '', x))\n",
    "combined = df.merge(bill_versions, left_on='bill_id', right_on='bill_version_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_text_records = []\n",
    "for bill_id, text_info in bill_text_data.items():\n",
    "    record = {'bill_id': bill_id}\n",
    "    record.update(text_info)\n",
    "    bill_text_records.append(record)\n",
    "bill_text_df = pd.DataFrame(bill_text_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_text_df.to_csv('ca_leg/legislation_data/bill_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = combined.loc[combined['bill_version_action'].notna()].merge(bills, left_on='bill_id_y', right_on='bill_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.to_csv('ca_leg/legislation_data/combined_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_53776/2120954024.py:1: DtypeWarning: Columns (30,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full = pd.read_csv('ca_leg/legislation_data/combined_table.csv')\n"
     ]
    }
   ],
   "source": [
    "full = pd.read_csv('ca_leg/legislation_data/combined_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bill_id_x', 'author_type', 'house', 'author_name', 'bill_version_id',\n",
       "       'bill_id_y', 'version_num', 'bill_version_action_date',\n",
       "       'bill_version_action', 'request_num', 'subject', 'vote_required',\n",
       "       'appropriation', 'fiscal_committee', 'local_program',\n",
       "       'substantive_changes', 'urgency', 'taxlevy', 'bill_xml', 'year_x',\n",
       "       'bill_id', 'session_year', 'session_num', 'measure_num',\n",
       "       'measure_state', 'chapter_year', 'chapter_type', 'chapter_session_num',\n",
       "       'chapter_num', 'latest_bill_version_id', 'current_location',\n",
       "       'current_status', 'year_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "disclosure = pd.read_csv('calaccess/CVR_LOBBY_DISCLOSURE_CD.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure = pd.read_csv('calaccess/LEXP_CD.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobbying = disclosure[['FILING_ID', 'FIRM_NAME']].merge(expenditure, on='FILING_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobbying['EXPN_DATE'] = pd.to_datetime(lobbying['EXPN_DATE'], format='%m/%d/%Y %H:%M:%S %p', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = lobbying.loc[(lobbying['EXPN_DATE'].notna()) & (lobbying['EXPN_DATE'] > pd.to_datetime('2001-01-01', format='%Y-%m-%d')) & ((lobbying['BENE_NAME'].notna()) | (lobbying['BENE_POSIT'].notna()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_committees = pd.read_csv('pdf_parsing/assembly_committees_clean.csv')\n",
    "assembly_roster = pd.read_csv('pdf_parsing/assembly_roster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "senate_committees = pd.read_csv('pdf_parsing/senate_committees_cleaned.csv')\n",
    "senate_roster = pd.read_csv('pdf_parsing/senate_roster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "senate_roster['Last'] = senate_roster['Name'].str.split(',').str[0].apply(lambda x: x.strip())\n",
    "senate_roster['Term'] = senate_roster['pages'].apply(lambda x: f\"{2000 + int(x.split(',')[0].strip())}-{2000 + int(x.split(',')[1].strip())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def politician_table(committees, roster):\n",
    "    doubles = committees.loc[committees['politician'].str.contains(',')]\n",
    "    hyphens = committees.loc[committees['politician'].str.contains('-')]\n",
    "    neither = committees.loc[(~committees['politician'].str.contains(',')) & (~committees['politician'].str.contains('-'))]\n",
    "    hyphens['Last'] = hyphens['politician'].apply(lambda x: re.sub(r'-', ' ', x))\n",
    "    hyp = hyphens.merge(roster, left_on=['Last', 'term'], right_on=['Last', 'Term'], how='inner')\n",
    "    if len(doubles) > 0:\n",
    "        doubles[['Last', 'First']] = doubles['politician'].str.split(',', expand=True)\n",
    "        doubles['Last'] = doubles['Last'].str.strip()\n",
    "        doubles['First'] = doubles['First'].str.strip()\n",
    "        doubles.rename(columns={'term': 'Term'}, inplace=True)\n",
    "        dbs = doubles.merge(roster, on=['Last', 'First', 'Term'], how='left')\n",
    "        politicians = pd.concat([neither.merge(roster, left_on=['politician', 'term'], right_on=['Last', 'Term'], how='inner'), hyp, dbs])\n",
    "    else:\n",
    "        politicians = pd.concat([neither.merge(roster, left_on=['politician', 'term'], right_on=['Last', 'Term'], how='inner'), hyp])\n",
    "    return politicians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "senate_roster.loc[senate_roster['Name'] == 'Valladares, Suzette Martinez', ['Party', 'District No.', 'Seat No.']] = ['R', 23, 7140]\n",
    "senate_roster.loc[senate_roster['Name'] == 'Weber Pierson, Dr Akilah', ['Party', 'District No.', 'Seat No.']] = ['D', 39, 7310]\n",
    "senate_roster.loc[senate_roster['Name'] == 'Eggman, Susan Talamantes', ['Party', 'District No.', 'Seat No.']] = ['D', 5, 8530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly = politician_table(assembly_committees, assembly_roster)\n",
    "senate = politician_table(senate_committees, senate_roster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly['full_name'] = assembly['First'] + ' ' + assembly['Last']\n",
    "senate['full_name'] = senate['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.strip()) + ' ' + senate['Name'].apply(lambda x: x.split(',')[0]).apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly['chamber'] = 'assembly'\n",
    "senate['chamber'] = 'senate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians = pd.concat([assembly[['committee_clean', 'position', 'Occupation', 'Party', 'District No.', 'Seat No.', 'Term', 'Last', 'full_name', 'chamber']], senate[['committee_clean', 'position', 'Occupation', 'Party', 'District No.', 'Seat No.', 'Term', 'Last', 'full_name', 'chamber']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians.to_csv('ca_leg/legislation_data/politicians.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "po = pd.read_csv('ca_leg/legislation_data/politicians.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from rapidfuzz import fuzz, process\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = unidecode(text.lower().strip())\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "df_lob = lb.copy()[['BENE_NAME', 'BENE_POSIT']].drop_duplicates()\n",
    "df_legislators = politicians['full_name'].drop_duplicates().apply(clean_text).tolist()\n",
    "df_committees = politicians['committee_clean'].drop_duplicates().apply(clean_text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ids = {name: f\"LEG_{i}\" for i, name in enumerate(df_legislators)}\n",
    "entity_ids.update({name: f\"COM_{i}\" for i, name in enumerate(df_committees)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unidecode(text).upper()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    prefixes = r'^(HON|HONORABLE|REP|REPRESENTATIVE|SEN|SENATOR|ASSEMBLY|ASSEMBLYMAN|ASSEMBLYMEMBER|COMMITTEE\\s+ON|THE|STAFF\\s+OF|OFFICE\\s+OF)\\s+'\n",
    "    text = re.sub(prefixes, '', text, flags=re.IGNORECASE)\n",
    "    suffixes = r'(\\s+JR|\\s+SR|\\s+III|\\s+II|\\s+IV|\\s+MD|\\s+PHD|\\s+ESQ)$'\n",
    "    text = re.sub(suffixes, '', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def preprocess_entity_ids(entity_ids):\n",
    "    name_mapping = {}\n",
    "    processed_entities = {}\n",
    "    ngram_index = {}\n",
    "\n",
    "    for original_name, entity_id in entity_ids.items():\n",
    "        clean_name = clean_text(original_name)\n",
    "        entity_type = 'legislator' if entity_id.startswith(\"LEG_\") else 'committee'\n",
    "\n",
    "        name_mapping[clean_name] = {\n",
    "            'original': original_name,\n",
    "            'id': entity_id,\n",
    "            'type': entity_type\n",
    "        }\n",
    "\n",
    "        processed_entities[original_name] = {\n",
    "            'clean_name': clean_name,\n",
    "            'tokens': set(clean_name.split()),\n",
    "            'entity_type': entity_type,\n",
    "            'id': entity_id\n",
    "        }\n",
    "        tokens = clean_name.split()\n",
    "        for token in tokens:\n",
    "            if len(token) >= 3:\n",
    "                if token not in ngram_index:\n",
    "                    ngram_index[token] = []\n",
    "                ngram_index[token].append(original_name)\n",
    "        if len(tokens) >= 2:\n",
    "            for i in range(len(tokens) - 1):\n",
    "                bigram = f\"{tokens[i]} {tokens[i+1]}\"\n",
    "                if bigram not in ngram_index:\n",
    "                    ngram_index[bigram] = []\n",
    "                ngram_index[bigram].append(original_name)\n",
    "\n",
    "    return {\n",
    "        'name_mapping': name_mapping,\n",
    "        'processed_entities': processed_entities,\n",
    "        'ngram_index': ngram_index,\n",
    "        'legislator_names': [name for name, eid in entity_ids.items() if eid.startswith(\"LEG_\")],\n",
    "        'committee_names': [name for name, eid in entity_ids.items() if eid.startswith(\"COM_\")]\n",
    "    }\n",
    "\n",
    "def get_candidates_by_ngrams(text, ngram_index):\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = cleaned.split()\n",
    "\n",
    "    candidates = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        if len(token) >= 3 and token in ngram_index:\n",
    "            for candidate in ngram_index[token]:\n",
    "                candidates[candidate] = candidates.get(candidate, 0) + 1\n",
    "\n",
    "    if len(tokens) >= 2:\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigram = f\"{tokens[i]} {tokens[i+1]}\"\n",
    "            if bigram in ngram_index:\n",
    "                for candidate in ngram_index[bigram]:\n",
    "                    candidates[candidate] = candidates.get(candidate, 0) + 3\n",
    "\n",
    "    return sorted(candidates.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_score(text1, text2):\n",
    "    clean1 = clean_text(text1)\n",
    "    clean2 = clean_text(text2)\n",
    "\n",
    "    if not clean1 or not clean2:\n",
    "        return 0\n",
    "\n",
    "    if clean1 == clean2:\n",
    "        return 100\n",
    "\n",
    "    tokens1 = set(clean1.split())\n",
    "    tokens2 = set(clean2.split())\n",
    "\n",
    "    intersection = tokens1.intersection(tokens2)\n",
    "    jaccard = len(intersection) / (len(tokens1) + len(tokens2) - len(intersection)) if (len(tokens1) + len(tokens2) - len(intersection)) > 0 else 0\n",
    "    subsequence_score = 0\n",
    "    if clean1 in clean2 or clean2 in clean1:\n",
    "        subsequence_score = 30\n",
    "\n",
    "    fuzzy_score = fuzz.token_set_ratio(clean1, clean2) * 0.65  # FUZZY SCORE\n",
    "\n",
    "    final_score = (jaccard * 25) + subsequence_score + fuzzy_score # final score\n",
    "\n",
    "    return min(final_score, 100)\n",
    "\n",
    "def extract_referenced_names(position_text):\n",
    "    if not position_text or not isinstance(position_text, str):\n",
    "        return []\n",
    "    referenced_names = []\n",
    "    position_lower = position_text.lower()\n",
    "    # bulk regex search\n",
    "    patterns = [\n",
    "        r'(?:staff|aide|assist\\w*|chief|counsel|direct\\w*)(?:\\s+\\w+)?\\s+(?:to|for|of|with)\\s+(?:sen\\w*|rep\\w*|assembl\\w*|congress\\w*)?\\s+([A-Za-z\\s\\.\\-]+?)(?:$|,|\\s+\\(|\\s+[A-Z]{2})',\n",
    "        r'(?:sen\\w*|rep\\w*|assembl\\w*|congress\\w*)\\s+([A-Za-z\\s\\.\\-]+?)(?:\\'s?)?\\s+(?:staff|office|aide|assist\\w*|chief)',\n",
    "        r'(?:office|staff)\\s+(?:of|for)\\s+(?:sen\\w*|rep\\w*|assembl\\w*|congress\\w*)?\\s+([A-Za-z\\s\\.\\-]+?)(?:$|,|\\s+\\(|\\s+[A-Z]{2})',\n",
    "        r'(?:sen\\w*|rep\\w*|assembl\\w*|congress\\w*)\\s+([A-Za-z\\s\\.\\-]{2,30})(?:$|,|\\s+\\(|\\s+[A-Z]{2})',\n",
    "        r'\\b([A-Za-z\\s\\.\\-]{2,30})\\s+\\([A-Z]{2}\\)',\n",
    "        r'\\b(?:senator|representative|congressman|chairperson|chairman|assembl\\w*)\\s+([A-Za-z\\s\\.\\-]{2,30})\\b'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        matches = re.finditer(pattern, position_lower, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            name = match.group(1).strip()\n",
    "            if name and len(name) > 2:\n",
    "                if any(term not in name.lower() for term in ['staff', 'office', 'committee']):\n",
    "                    start, end = match.span(1)\n",
    "                    original_case = position_text[start:end].strip()\n",
    "                    if original_case and len(original_case) > 2 and original_case not in referenced_names:\n",
    "                        referenced_names.append(original_case)\n",
    "\n",
    "    if not referenced_names: # NER if no matches found\n",
    "        try:\n",
    "            doc = nlp(position_text)\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == \"PERSON\" and len(ent.text) > 2:\n",
    "                    if ent.text not in referenced_names:\n",
    "                        referenced_names.append(ent.text)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return referenced_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46309/46309 [04:19<00:00, 178.66it/s]\n"
     ]
    }
   ],
   "source": [
    "result_df = df_lob.copy()\n",
    "entity_data = preprocess_entity_ids(entity_ids)\n",
    "name_mapping = entity_data['name_mapping']\n",
    "ngram_index = entity_data['ngram_index']\n",
    "legislator_names = entity_data['legislator_names']\n",
    "committee_names = entity_data['committee_names']\n",
    "\n",
    "GENERIC_LEGISLATOR_TITLES = ['assemblymember', 'senator', 'assemblyman', 'assemblywoman']\n",
    "GOV_DEPT_INDICATORS = ['ca department', 'ca dept', 'california department', 'california dept','department of', 'dept. of', 'dept of', 'agency', 'bureau', 'division of', 'state of california', 'state board', 'state commission']\n",
    "result_df['MATCHED_NAME'] = None\n",
    "result_df['ENTITY_ID'] = None\n",
    "result_df['ENTITY_TYPE'] = None\n",
    "result_df['MATCH_METHOD'] = None\n",
    "result_df['CONFIDENCE'] = None\n",
    "\n",
    "for idx, row in tqdm(result_df.iterrows(), total=len(result_df)):\n",
    "    bene_name = str(row['BENE_NAME']) if pd.notna(row['BENE_NAME']) else \"\"\n",
    "    bene_position = str(row['BENE_POSIT']) if pd.notna(row['BENE_POSIT']) else \"\"\n",
    "    if not bene_name.strip() and not bene_position.strip():\n",
    "        continue\n",
    "    combined_text = f\"{bene_name} {bene_position}\".lower()\n",
    "    if any(indicator in combined_text for indicator in GOV_DEPT_INDICATORS):\n",
    "        continue\n",
    "\n",
    "\n",
    "    position_is_generic = bene_position.strip().lower() in GENERIC_LEGISLATOR_TITLES\n",
    "    for field_name, field_value in [('BENE_NAME', bene_name),\n",
    "                                   ('BENE_POSIT', bene_position if not position_is_generic else \"\")]:\n",
    "        if not field_value.strip():\n",
    "            continue\n",
    "\n",
    "        clean_value = clean_text(field_value)\n",
    "        if clean_value in name_mapping:\n",
    "            entity_info = name_mapping[clean_value]\n",
    "            result_df.at[idx, 'MATCHED_NAME'] = entity_info['original']\n",
    "            result_df.at[idx, 'ENTITY_ID'] = entity_info['id']\n",
    "            result_df.at[idx, 'ENTITY_TYPE'] = entity_info['type']\n",
    "            result_df.at[idx, 'MATCH_METHOD'] = f'exact_{field_name.lower()}'\n",
    "            result_df.at[idx, 'CONFIDENCE'] = 'high'\n",
    "            break\n",
    "    if pd.notna(result_df.at[idx, 'MATCHED_NAME']):\n",
    "        continue\n",
    "    all_referenced_names = []\n",
    "    if not position_is_generic:\n",
    "        all_referenced_names.extend(extract_referenced_names(bene_position))\n",
    "    all_referenced_names.extend(extract_referenced_names(bene_name))\n",
    "\n",
    "    for ref_name in all_referenced_names:\n",
    "        clean_ref = clean_text(ref_name)\n",
    "        if clean_ref in name_mapping:\n",
    "            entity_info = name_mapping[clean_ref]\n",
    "            result_df.at[idx, 'MATCHED_NAME'] = entity_info['original']\n",
    "            result_df.at[idx, 'ENTITY_ID'] = entity_info['id']\n",
    "            result_df.at[idx, 'ENTITY_TYPE'] = entity_info['type']\n",
    "            result_df.at[idx, 'MATCH_METHOD'] = 'reference_exact'\n",
    "            result_df.at[idx, 'CONFIDENCE'] = 'high'\n",
    "            break\n",
    "\n",
    "    if pd.notna(result_df.at[idx, 'MATCHED_NAME']):\n",
    "        continue\n",
    "\n",
    "    position_has_legislator = any(term in combined_text for term in ['senator', 'representative', 'rep ', 'sen ', 'assemblymember', 'assemblyman', 'assemblywoman', 'assembly member'])\n",
    "    position_has_committee = any(term in combined_text for term in ['committee', 'commission', 'board', 'task force', 'caucus'])\n",
    "    search_pool = None\n",
    "\n",
    "    if position_has_legislator:\n",
    "        search_pool = legislator_names\n",
    "    elif position_has_committee:\n",
    "        search_pool = committee_names\n",
    "\n",
    "    candidates = []\n",
    "    if bene_name.strip():\n",
    "        candidates.extend(get_candidates_by_ngrams(bene_name, ngram_index))\n",
    "\n",
    "    if bene_position.strip() and not position_is_generic:\n",
    "        candidates.extend(get_candidates_by_ngrams(bene_position, ngram_index))\n",
    "\n",
    "    seen = set()\n",
    "    unique_candidates = [(name, score) for name, score in candidates\n",
    "                         if not (name in seen or seen.add(name))]\n",
    "\n",
    "    if search_pool:\n",
    "        unique_candidates = [(name, score) for name, score in unique_candidates if name in search_pool]\n",
    "\n",
    "    top_candidates = unique_candidates[:10] if unique_candidates else []\n",
    "\n",
    "    if top_candidates:\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        best_method = None\n",
    "\n",
    "        for candidate_name, _ in top_candidates:\n",
    "            if bene_name.strip():\n",
    "                name_score = calculate_similarity_score(bene_name, candidate_name)\n",
    "\n",
    "                if name_score > best_score and name_score >= 75:\n",
    "                    best_score = name_score\n",
    "                    best_match = candidate_name\n",
    "                    best_method = \"fuzzy_name\"\n",
    "            if bene_position.strip() and not position_is_generic:\n",
    "                position_score = calculate_similarity_score(bene_position, candidate_name)\n",
    "                if position_score > best_score and position_score >= 85:\n",
    "                    best_score = position_score\n",
    "                    best_match = candidate_name\n",
    "                    best_method = \"fuzzy_position\"\n",
    "            if bene_name.strip() and bene_position.strip():\n",
    "                combined_text = f\"{bene_name} {bene_position}\"\n",
    "                combined_score = calculate_similarity_score(combined_text, candidate_name)\n",
    "\n",
    "                if combined_score > best_score and combined_score >= 70:\n",
    "                    best_score = combined_score\n",
    "                    best_match = candidate_name\n",
    "                    best_method = \"fuzzy_combined\"\n",
    "\n",
    "        if best_match:\n",
    "            confidence = \"high\" if best_score >= 90 else \"medium\" if best_score >= 75 else \"low\"\n",
    "\n",
    "            result_df.at[idx, 'MATCHED_NAME'] = best_match\n",
    "            result_df.at[idx, 'ENTITY_ID'] = entity_ids[best_match]\n",
    "            result_df.at[idx, 'ENTITY_TYPE'] = 'legislator' if entity_ids[best_match].startswith('LEG_') else 'committee'\n",
    "            result_df.at[idx, 'MATCH_METHOD'] = best_method\n",
    "            result_df.at[idx, 'CONFIDENCE'] = confidence\n",
    "            continue\n",
    "\n",
    "    for field_name, field_value in [('BENE_NAME', bene_name),\n",
    "                                   ('BENE_POSIT', bene_position if not position_is_generic else \"\")]:\n",
    "        if not field_value.strip():\n",
    "            continue\n",
    "\n",
    "        clean_value = clean_text(field_value)\n",
    "        search_list = search_pool if search_pool else list(entity_ids.keys())\n",
    "\n",
    "        name_match = process.extractOne(\n",
    "            clean_value,\n",
    "            search_list,\n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            score_cutoff=85\n",
    "        )\n",
    "\n",
    "        if name_match:\n",
    "            match, score = name_match\n",
    "            result_df.at[idx, 'MATCHED_NAME'] = match\n",
    "            result_df.at[idx, 'ENTITY_ID'] = entity_ids[match]\n",
    "            result_df.at[idx, 'ENTITY_TYPE'] = 'legislator' if entity_ids[match].startswith('LEG_') else 'committee'\n",
    "            result_df.at[idx, 'MATCH_METHOD'] = f'direct_fuzzy_{field_name.lower()}'\n",
    "            result_df.at[idx, 'CONFIDENCE'] = 'high' if score > 90 else 'medium'\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_name_positions_dict = result_df.loc[result_df['CONFIDENCE'].notna(), ['BENE_NAME', 'BENE_POSIT', 'MATCHED_NAME']].set_index(['BENE_NAME', 'BENE_POSIT']).to_dict()['MATCHED_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb['clean_beneficiary'] = lb[['BENE_NAME', 'BENE_POSIT']].apply(lambda x: ben_name_positions_dict.get(tuple(x), None), axis=1)\n",
    "lb.to_csv('calaccess/lobbying_clean2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure_assembly = pd.read_csv('calaccess/expenditure_assembly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly['target_name'] = assembly['Last'] + ', ' + assembly['First']\n",
    "expenditure_assembly['year'] = expenditure_assembly['DateRange'].apply(lambda x: int(x.split('-')[0]))\n",
    "expenditure_assembly['term'] = expenditure_assembly['year'].apply(lambda x: f\"{x}-{x+1}\" if x % 2 != 0 else f\"{x-1}-{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_names(_names, expenditure_df):\n",
    "    expenditure_names = expenditure_df['TargetCandidateName'].unique()\n",
    "    name_mapping = {}\n",
    "    for exp_name in expenditure_names:\n",
    "        best_match = process.extractOne(\n",
    "            exp_name,\n",
    "            _names,\n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            score_cutoff=90\n",
    "        )\n",
    "\n",
    "        if best_match:\n",
    "            name_mapping[exp_name] = best_match[0]\n",
    "        else:\n",
    "            name_mapping[exp_name] = None\n",
    "\n",
    "    return name_mapping\n",
    "\n",
    "name_mapping = match_names(assembly['target_name'].unique(), expenditure_assembly)\n",
    "expenditure_assembly['matched_target_name'] = expenditure_assembly['TargetCandidateName'].map(name_mapping)\n",
    "merged_df = pd.merge(\n",
    "    expenditure_assembly,\n",
    "    assembly,\n",
    "    left_on='matched_target_name',\n",
    "    right_on='target_name',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('calaccess/expend_assembly_matched.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure_senate = pd.read_csv('calaccess/expenditure_senator.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "senate_name_mapping = match_names(senate['Name'].unique(), expenditure_senate)\n",
    "expenditure_senate['matched_target_name'] = expenditure_senate['TargetCandidateName'].map(senate_name_mapping)\n",
    "merged_senate_df = pd.merge(\n",
    "    expenditure_senate,\n",
    "    senate,\n",
    "    left_on='matched_target_name',\n",
    "    right_on='Name',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_senate_df.to_csv('calaccess/expend_senate_matched.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_99337/2005777650.py:1: DtypeWarning: Columns (30,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined = pd.read_csv('ca_leg/legislation_data/combined_table.csv')\n"
     ]
    }
   ],
   "source": [
    "combined = pd.read_csv('ca_leg/legislation_data/combined_table.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca_leg_etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
