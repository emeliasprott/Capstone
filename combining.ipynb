{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Up the Data\n",
    "In this notebook, I will identify the relationships between all of the data. Many of the relationships depend on string matches, but the text data is inconsistent and has many typos. For example, *'Assemblymember'* could be written as *'Assemblyman'*, *'Assemblywoman'*, *'A semblmember'*, and more. At the same time, with so many repeated words and phrases, many strings appear to match when they should not. A simple string-distance algorithm might find 'Assemblymember David Chiu' to match with 'Assemblymember Dave Chu', which is not correct. Therefore I use an approach that combines fuzzy string matching and regex  with spacy token similarity and entity linking to match the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "from rapidfuzz import fuzz, process\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_82551/176948873.py:1: DtypeWarning: Columns (4,6,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  bill_analysis = pd.read_csv('ca_leg/legislation_data/bill_analysis_tbl.csv')\n"
     ]
    }
   ],
   "source": [
    "bill_analysis = pd.read_csv('ca_leg/legislation_data/bill_analysis_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_codes = bill_analysis.loc[bill_analysis['committee_code'].notna(), ['committee_code', 'committee_name']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_codes.to_csv('ca_leg/legislation_data/committee_codes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills = pd.read_csv('ca_leg/legislation_data/bill_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_history = pd.read_csv('ca_leg/legislation_data/bill_history_tbl.csv', dtype={'action_status': str, 'primary_location': str, 'secondary_location': str, 'end_status': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_versions = pd.read_csv('ca_leg/legislation_data/bill_version_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ca_leg/legislation_data/bill_version_text.json\", \"r\") as f:\n",
    "    bill_text = json.load(f)\n",
    "    authors_data = {bill_id: bill_info['Authors']\n",
    "                   for bill_id, bill_info in bill_text.items()\n",
    "                   if 'Authors' in bill_info}\n",
    "    bill_text_data = {}\n",
    "    for bill_id, bill_info in bill_text.items():\n",
    "        record = {}\n",
    "        if 'Title' in bill_info.keys():\n",
    "            title = bill_info.get('Title')\n",
    "            record.update({'title': title})\n",
    "        if 'GeneralSubject' in bill_info.keys():\n",
    "            general_subject = bill_info.get('GeneralSubject')\n",
    "            record.update({'general_subject': general_subject})\n",
    "        if 'DigestText' in bill_info.keys():\n",
    "            digest_text = bill_info.get('DigestText')\n",
    "            record.update({'digest_text': digest_text})\n",
    "        if 'BillContent' in bill_info.keys():\n",
    "            content = bill_info.get('BillContent')\n",
    "            record.update({'content': content})\n",
    "        bill_text_data[bill_id] = record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COAUTHOR', 'PRINCIPAL_COAUTHOR', 'LEAD_AUTHOR', 'null']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([a for b in [v.keys() for v in authors_data.values()] for a in b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for bill_id, authors in authors_data.items():\n",
    "    for author_type, house in authors.items():\n",
    "        for house_name, author_name in house.items():\n",
    "            records.append([bill_id, author_type, \"COMMITTEE\" if house_name == 'UNKNOWN' else house_name, author_name])\n",
    "\n",
    "df = pd.DataFrame(records, columns=['bill_id', 'author_type', 'house', 'author_name'])\n",
    "df['bill_id'] = df['bill_id'].apply(lambda x: re.sub(r'__', '', x))\n",
    "combined = df.merge(bill_versions, left_on='bill_id', right_on='bill_version_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_text_records = []\n",
    "for bill_id, text_info in bill_text_data.items():\n",
    "    record = {'bill_id': bill_id}\n",
    "    record.update(text_info)\n",
    "    bill_text_records.append(record)\n",
    "bill_text_df = pd.DataFrame(bill_text_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_text_df.to_csv('ca_leg/legislation_data/bill_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = combined.loc[combined['bill_version_action'].notna()].merge(bills, left_on='bill_id_y', right_on='bill_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.to_csv('ca_leg/legislation_data/combined_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_82551/2120954024.py:1: DtypeWarning: Columns (30,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full = pd.read_csv('ca_leg/legislation_data/combined_table.csv')\n"
     ]
    }
   ],
   "source": [
    "full = pd.read_csv('ca_leg/legislation_data/combined_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bill_id_x', 'author_type', 'house', 'author_name', 'bill_version_id',\n",
       "       'bill_id_y', 'version_num', 'bill_version_action_date',\n",
       "       'bill_version_action', 'request_num', 'subject', 'vote_required',\n",
       "       'appropriation', 'fiscal_committee', 'local_program',\n",
       "       'substantive_changes', 'urgency', 'taxlevy', 'bill_xml', 'year_x',\n",
       "       'bill_id', 'session_year', 'session_num', 'measure_num',\n",
       "       'measure_state', 'chapter_year', 'chapter_type', 'chapter_session_num',\n",
       "       'chapter_num', 'latest_bill_version_id', 'current_location',\n",
       "       'current_status', 'year_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "disclosure = pd.read_csv('calaccess/CVR_LOBBY_DISCLOSURE_CD2.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure = pd.read_csv('calaccess/LEXP_CD2.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobbying = disclosure[['FILING_ID', 'FIRM_NAME']].merge(expenditure, on='FILING_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobbying['EXPN_DATE'] = pd.to_datetime(lobbying['EXPN_DATE'], format='%m/%d/%Y %H:%M:%S %p', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = lobbying.loc[(lobbying['EXPN_DATE'].notna()) & (lobbying['EXPN_DATE'] > pd.to_datetime('2001-01-01', format='%Y-%m-%d')) & ((lobbying['BENE_NAME'].notna()) | (lobbying['BENE_POSIT'].notna()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_committees = pd.read_csv('pdf_parsing/assembly_committees_clean.csv')\n",
    "assembly_roster = pd.read_csv('pdf_parsing/assembly_roster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "senate_committees = pd.read_csv('pdf_parsing/senate_committees_cleaned.csv')\n",
    "senate_roster = pd.read_csv('pdf_parsing/senate_roster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "senate_roster['Last'] = senate_roster['Name'].str.split(',').str[0].apply(lambda x: x.strip())\n",
    "senate_roster['Term'] = senate_roster['pages'].apply(lambda x: f\"{2000 + int(x.split(',')[0].strip())}-{2000 + int(x.split(',')[1].strip())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def politician_table(committees, roster):\n",
    "    doubles = committees.loc[committees['politician'].str.contains(',')]\n",
    "    hyphens = committees.loc[committees['politician'].str.contains('-')]\n",
    "    neither = committees.loc[(~committees['politician'].str.contains(',')) & (~committees['politician'].str.contains('-'))]\n",
    "    hyphens['Last'] = hyphens['politician'].apply(lambda x: re.sub(r'-', ' ', x))\n",
    "    hyp = hyphens.merge(roster, left_on=['Last', 'term'], right_on=['Last', 'Term'], how='inner')\n",
    "    if len(doubles) > 0:\n",
    "        doubles[['Last', 'First']] = doubles['politician'].str.split(',', expand=True)\n",
    "        doubles['Last'] = doubles['Last'].str.strip()\n",
    "        doubles['First'] = doubles['First'].str.strip()\n",
    "        doubles.rename(columns={'term': 'Term'}, inplace=True)\n",
    "        dbs = doubles.merge(roster, on=['Last', 'First', 'Term'], how='left')\n",
    "        politicians = pd.concat([neither.merge(roster, left_on=['politician', 'term'], right_on=['Last', 'Term'], how='inner'), hyp, dbs])\n",
    "    else:\n",
    "        politicians = pd.concat([neither.merge(roster, left_on=['politician', 'term'], right_on=['Last', 'Term'], how='inner'), hyp])\n",
    "    return politicians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "senate_roster.loc[senate_roster['Name'] == 'Valladares, Suzette Martinez', ['Party', 'District No.', 'Seat No.']] = ['R', 23, 7140]\n",
    "senate_roster.loc[senate_roster['Name'] == 'Weber Pierson, Dr Akilah', ['Party', 'District No.', 'Seat No.']] = ['D', 39, 7310]\n",
    "senate_roster.loc[senate_roster['Name'] == 'Eggman, Susan Talamantes', ['Party', 'District No.', 'Seat No.']] = ['D', 5, 8530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly = politician_table(assembly_committees, assembly_roster).rename(columns={'name': 'full_name'})\n",
    "senate = politician_table(senate_committees, senate_roster).rename(columns={'Name': 'full_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly['chamber'] = 'assembly'\n",
    "senate['chamber'] = 'senate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians = pd.concat([assembly[['committee_clean', 'position', 'Occupation', 'Party', 'District No.', 'Seat No.', 'Term', 'Last', 'full_name', 'chamber']], senate[['committee_clean', 'position', 'Occupation', 'Party', 'District No.', 'Seat No.', 'Term', 'Last', 'full_name', 'chamber']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = politicians.loc[politicians['full_name'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss = assembly[['Term', 'Last', 'chamber', 'First']].merge(missing[['Term', 'Last', 'chamber']], on=['Last', 'Term', 'chamber']).drop_duplicates()\n",
    "miss['full_name'] = miss['Last'] + ', ' + miss['First']\n",
    "mi = miss.merge(assembly[['Term', 'Last', 'chamber', 'First', 'Seat No.']], on=['Term', 'Last', 'chamber', 'First'], how='left')\n",
    "mis = {(row['Term'], row['Last'], row['chamber'], row['Seat No.']): row['full_name'] for _, row in mi.iterrows()}\n",
    "politicians.loc[politicians['full_name'].isna(), 'full_name'] = politicians.loc[politicians['full_name'].isna()].apply(lambda x: mis.get((x['Term'], x['Last'], x['chamber'], x['Seat No.'])), axis=1)\n",
    "assembly.loc[assembly['full_name'].isna(), 'full_name'] = assembly.loc[assembly['full_name'].isna()].apply(\n",
    "\tlambda x: mis.get((x['Term'], x['Last'], x['chamber'], x['Seat No.']), None), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians.to_csv('ca_leg/legislation_data/politicians.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "po = politicians.copy()\n",
    "# po = pd.read_csv('ca_leg/legislation_data/politicians.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from rapidfuzz import fuzz, process\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \" \".join(text.split(',')[::-1])\n",
    "    text = unidecode(text.lower().strip())\n",
    "    return re.sub(r'[^\\w\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in po.loc[po['full_name'].apply(lambda x: isinstance(x, float)), ['Term', 'Last', 'chamber']].drop_duplicates().iterrows():\n",
    "    term, last = row['Term'], row['Last']\n",
    "    a = assembly.loc[(assembly['politician'] == last) & (assembly['term'] == term)]\n",
    "    if len(a) > 0:\n",
    "        po.loc[(po['Term'] == term) & (po['Last'] == last) & (po['chamber'] == row['chamber']), 'full_name'] = a['full_name'].values[0]\n",
    "        continue\n",
    "    else:\n",
    "        a = assembly.loc[assembly['politician'] == last]\n",
    "    if len(a) > 0:\n",
    "        po.loc[(po['Term'] == term) & (po['Last'] == last) & (po['chamber'] == row['chamber']), 'full_name'] = a['full_name'].values[0]\n",
    "    else:\n",
    "        print(last, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual = {'Steinberg': 'Steinberg, Darrell', 'Calderon': 'Calderon, Ron', 'Stone': 'Stone, Mark', 'Rubio': 'Rubio, Susan', 'Nguyen': 'Nguyen, Janet', 'Berryhill': 'Berryhill, Tom', 'Horton': 'Horton, Shirley', 'Rivas': 'Rivas, Robert'}\n",
    "\n",
    "def name_fix_col(row):\n",
    "    if pd.isna(row['full_name']):\n",
    "        return manual.get(row['Last'])\n",
    "    else:\n",
    "        return row['full_name']\n",
    "\n",
    "po['full_name'] = po.apply(name_fix_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lob = lb.copy()[['BENE_NAME', 'BENE_POSIT']].drop_duplicates()\n",
    "df_legislators = po['full_name'].drop_duplicates().apply(clean_text).tolist()\n",
    "df_committees = po['committee_clean'].drop_duplicates().apply(clean_text).tolist()\n",
    "entity_ids = {name: f\"LEG_{i}\" for i, name in enumerate(df_legislators)}\n",
    "entity_ids.update({name: f\"COM_{i}\" for i, name in enumerate(df_committees)})\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_last_name_aliases(full_names):\n",
    "    last_name_map = {}\n",
    "\n",
    "    for full in full_names:\n",
    "        tokens = full.split()\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "        last = tokens[-1]\n",
    "        last_name_map.setdefault(last, []).append(full)\n",
    "\n",
    "    return {\n",
    "        last: fulls[0]\n",
    "        for last, fulls in last_name_map.items()\n",
    "        if len(fulls) == 1\n",
    "    }\n",
    "\n",
    "last_name_aliases = build_last_name_aliases(df_legislators)\n",
    "\n",
    "for last_name, full_name in last_name_aliases.items():\n",
    "    entity_ids[last_name] = entity_ids[full_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unidecode(text).upper()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    prefixes = r'^(HON|HONORABLE|REP|REPRESENTATIVE|SEN|SENATOR|ASSEMBLY|ASSEMBLYMAN|ASSEMBLYMEMBER|COMMITTEE\\s+ON|THE|STAFF\\s+OF|OFFICE\\s+OF)\\s+'\n",
    "    text = re.sub(prefixes, '', text, flags=re.IGNORECASE)\n",
    "    suffixes = r'(\\s+JR|\\s+SR|\\s+III|\\s+II|\\s+IV|\\s+MD|\\s+PHD|\\s+ESQ)$'\n",
    "    text = re.sub(suffixes, '', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def preprocess_entity_ids(entity_ids):\n",
    "    name_mapping = {}\n",
    "    processed_entities = {}\n",
    "    ngram_index = {}\n",
    "\n",
    "    for original_name, entity_id in entity_ids.items():\n",
    "        clean_name = clean_text(original_name)\n",
    "        is_alias = original_name in last_name_aliases\n",
    "        entity_type = 'legislator' if entity_id.startswith(\"LEG_\") else 'committee'\n",
    "\n",
    "        name_mapping[clean_name] = {\n",
    "            'original': original_name,\n",
    "            'id': entity_id,\n",
    "            'type': entity_type,\n",
    "            'alias': is_alias\n",
    "        }\n",
    "\n",
    "        processed_entities[original_name] = {\n",
    "            'clean_name': clean_name,\n",
    "            'tokens': set(clean_name.split()),\n",
    "            'entity_type': entity_type,\n",
    "            'id': entity_id\n",
    "        }\n",
    "        tokens = clean_name.split()\n",
    "        for token in tokens:\n",
    "            if len(token) >= 3:\n",
    "                if token not in ngram_index:\n",
    "                    ngram_index[token] = []\n",
    "                ngram_index[token].append(original_name)\n",
    "        if len(tokens) >= 2:\n",
    "            for i in range(len(tokens) - 1):\n",
    "                bigram = f\"{tokens[i]} {tokens[i+1]}\"\n",
    "                if bigram not in ngram_index:\n",
    "                    ngram_index[bigram] = []\n",
    "                ngram_index[bigram].append(original_name)\n",
    "\n",
    "    for last, full_names in last_name_aliases.items():\n",
    "        if len(full_names) == 1:\n",
    "            name_mapping[last] = {\n",
    "                'original': full_names[0],\n",
    "                'id': entity_ids[full_names[0]],\n",
    "                'type': 'legislator',\n",
    "                'alias': True\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        'name_mapping': name_mapping,\n",
    "        'processed_entities': processed_entities,\n",
    "        'ngram_index': ngram_index,\n",
    "        'legislator_names': [name for name, eid in entity_ids.items() if eid.startswith(\"LEG_\")],\n",
    "        'committee_names': [name for name, eid in entity_ids.items() if eid.startswith(\"COM_\")]\n",
    "    }\n",
    "\n",
    "def get_candidates_by_ngrams(text, ngram_index):\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = cleaned.split()\n",
    "\n",
    "    candidates = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        if len(token) >= 3 and token in ngram_index:\n",
    "            for candidate in ngram_index[token]:\n",
    "                candidates[candidate] = candidates.get(candidate, 0) + 1\n",
    "\n",
    "    if len(tokens) >= 2:\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigram = f\"{tokens[i]} {tokens[i+1]}\"\n",
    "            if bigram in ngram_index:\n",
    "                for candidate in ngram_index[bigram]:\n",
    "                    candidates[candidate] = candidates.get(candidate, 0) + 3\n",
    "\n",
    "    return sorted(candidates.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_score(text1, text2):\n",
    "    clean1 = clean_text(text1)\n",
    "    clean2 = clean_text(text2)\n",
    "\n",
    "    if not clean1 or not clean2:\n",
    "        return 0\n",
    "\n",
    "    if clean1 == clean2:\n",
    "        return 100\n",
    "\n",
    "    tokens1 = set(clean1.split())\n",
    "    tokens2 = set(clean2.split())\n",
    "\n",
    "    intersection = tokens1.intersection(tokens2)\n",
    "    jaccard = len(intersection) / (len(tokens1) + len(tokens2) - len(intersection)) if (len(tokens1) + len(tokens2) - len(intersection)) > 0 else 0\n",
    "    subsequence_score = 0\n",
    "    if clean1 in clean2 or clean2 in clean1:\n",
    "        subsequence_score = 30\n",
    "\n",
    "    fuzzy_score = fuzz.token_set_ratio(clean1, clean2) * 0.65  # FUZZY SCORE\n",
    "\n",
    "    final_score = (jaccard * 25) + subsequence_score + fuzzy_score # final score\n",
    "\n",
    "    return min(final_score, 100)\n",
    "\n",
    "def extract_referenced_names(position_text):\n",
    "    if not position_text or not isinstance(position_text, str):\n",
    "        return []\n",
    "    referenced_names = []\n",
    "    position_lower = position_text.lower()\n",
    "    # bulk regex search\n",
    "    patterns = [\n",
    "        r'(?:staff|aide|assist\\w*|chief|counsel|direct\\w*)(?:\\s+\\w+)?\\s+(?:to|for|of|with)\\s+(?:sen\\w*|rep\\w*|assembl\\w*|congress\\w*)?\\s+([A-Za-z\\s\\.\\-]+?)(?:$|,|\\s+\\(|\\s+[A-Z]{2})',\n",
    "        r'(?:sen\\w*|rep\\w*|assembl\\w*|congress\\w*)\\s+([A-Za-z\\s\\.\\-]+?)(?:\\'s?)?\\s+(?:staff|office|aide|assist\\w*|chief)',\n",
    "        r'(?:office|staff)\\s+(?:of|for)\\s+(?:sen\\w*|rep\\w*|assembl\\w*|congress\\w*)?\\s+([A-Za-z\\s\\.\\-]+?)(?:$|,|\\s+\\(|\\s+[A-Z]{2})',\n",
    "        r'(?:sen\\w*|rep\\w*|assembl\\w*|congress\\w*)\\s+([A-Za-z\\s\\.\\-]{2,30})(?:$|,|\\s+\\(|\\s+[A-Z]{2})',\n",
    "        r'\\b([A-Za-z\\s\\.\\-]{2,30})\\s+\\([A-Z]{2}\\)',\n",
    "        r'\\b(?:senator|representative|congressman|chairperson|chairman|assembl\\w*)\\s+([A-Za-z\\s\\.\\-]{2,30})\\b'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        matches = re.finditer(pattern, position_lower, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            name = match.group(1).strip()\n",
    "            if name and len(name) > 2:\n",
    "                if any(term not in name.lower() for term in ['staff', 'office', 'committee']):\n",
    "                    start, end = match.span(1)\n",
    "                    original_case = position_text[start:end].strip()\n",
    "                    if original_case and len(original_case) > 2 and original_case not in referenced_names:\n",
    "                        referenced_names.append(original_case)\n",
    "\n",
    "    if not referenced_names: # NER if no matches found\n",
    "        try:\n",
    "            doc = nlp(position_text)\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == \"PERSON\" and len(ent.text) > 2:\n",
    "                    if ent.text not in referenced_names:\n",
    "                        referenced_names.append(ent.text)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return referenced_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_names_from_politicians(politicians_df):\n",
    "    last_names_mapping = {}\n",
    "\n",
    "    for _, row in politicians_df.iterrows():\n",
    "        full_name = row['full_name']\n",
    "        chamber = row['chamber']\n",
    "\n",
    "        if ',' in full_name:\n",
    "            last_name = full_name.split(',')[0].strip()\n",
    "        else:\n",
    "            last_name = full_name.split()[-1].strip()\n",
    "\n",
    "        clean_last = clean_text(last_name)\n",
    "\n",
    "        if clean_last not in last_names_mapping:\n",
    "            last_names_mapping[clean_last] = []\n",
    "\n",
    "        last_names_mapping[clean_last].append({\n",
    "            'full_name': full_name,\n",
    "            'chamber': chamber,\n",
    "            'last_name': last_name\n",
    "        })\n",
    "\n",
    "    return last_names_mapping\n",
    "\n",
    "def search_by_last_name(position_text, last_names_mapping):\n",
    "    if not position_text or not isinstance(position_text, str):\n",
    "        return []\n",
    "\n",
    "    position_lower = position_text.lower()\n",
    "    matches = []\n",
    "\n",
    "    # Check for senate/assembly indicators with last names\n",
    "    senate_patterns = [\n",
    "        r'senate?\\s+(\\w+)',\n",
    "        r'sen\\.\\s+(\\w+)',\n",
    "        r'senator\\s+(\\w+)'\n",
    "    ]\n",
    "\n",
    "    assembly_patterns = [\n",
    "        r'assembly\\s*member?\\s+(\\w+)',\n",
    "        r'assemblym[ae]n\\s+(\\w+)',\n",
    "        r'assemblywoman\\s+(\\w+)'\n",
    "    ]\n",
    "\n",
    "    for pattern in senate_patterns:\n",
    "        for match in re.finditer(pattern, position_lower, re.IGNORECASE):\n",
    "            last_name = clean_text(match.group(1))\n",
    "            if last_name in last_names_mapping:\n",
    "                for politician in last_names_mapping[last_name]:\n",
    "                    if politician['chamber'] == 'senate':\n",
    "                        matches.append({\n",
    "                            'full_name': politician['full_name'],\n",
    "                            'match_type': 'senate_last_name',\n",
    "                            'extracted_name': match.group(1)\n",
    "                        })\n",
    "\n",
    "    for pattern in assembly_patterns:\n",
    "        for match in re.finditer(pattern, position_lower, re.IGNORECASE):\n",
    "            last_name = clean_text(match.group(1))\n",
    "            if last_name in last_names_mapping:\n",
    "                for politician in last_names_mapping[last_name]:\n",
    "                    if politician['chamber'] == 'assembly':\n",
    "                        matches.append({\n",
    "                            'full_name': politician['full_name'],\n",
    "                            'match_type': 'assembly_last_name',\n",
    "                            'extracted_name': match.group(1)\n",
    "                        })\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(94728) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "100%|██████████| 47766/47766 [04:17<00:00, 185.33it/s]\n"
     ]
    }
   ],
   "source": [
    "result_df = df_lob.copy()\n",
    "entity_data = preprocess_entity_ids(entity_ids)\n",
    "name_mapping = entity_data['name_mapping']\n",
    "ngram_index = entity_data['ngram_index']\n",
    "legislator_names = entity_data['legislator_names']\n",
    "committee_names = entity_data['committee_names']\n",
    "\n",
    "last_names_mapping = extract_last_names_from_politicians(po)\n",
    "\n",
    "GENERIC_LEGISLATOR_TITLES = ['assemblymember', 'senator', 'assemblyman', 'assemblywoman', 'representative', 'rep', 'sen']\n",
    "GOV_DEPT_INDICATORS = ['ca department', 'ca dept', 'california department', 'california dept','department of', 'dept. of', 'dept of', 'agency', 'bureau', 'division of', 'state of california', 'state board', 'state commission']\n",
    "result_df['MATCHED_NAME'] = None\n",
    "result_df['ENTITY_ID'] = None\n",
    "result_df['ENTITY_TYPE'] = None\n",
    "result_df['MATCH_METHOD'] = None\n",
    "result_df['CONFIDENCE'] = None\n",
    "\n",
    "for idx, row in tqdm(result_df.iterrows(), total=len(result_df)):\n",
    "    bene_name = str(row['BENE_NAME']) if pd.notna(row['BENE_NAME']) else \"\"\n",
    "    bene_position = str(row['BENE_POSIT']) if pd.notna(row['BENE_POSIT']) else \"\"\n",
    "    if not bene_name.strip() and not bene_position.strip():\n",
    "        continue\n",
    "    combined_text = f\"{bene_name} {bene_position}\".lower()\n",
    "    if any(indicator in combined_text for indicator in GOV_DEPT_INDICATORS):\n",
    "        continue\n",
    "\n",
    "    position_is_generic = bene_position.strip().lower() in GENERIC_LEGISLATOR_TITLES\n",
    "\n",
    "    for field_name, field_value in [('BENE_NAME', bene_name),\n",
    "                                   ('BENE_POSIT', bene_position if not position_is_generic else \"\")]:\n",
    "        if not field_value.strip():\n",
    "            continue\n",
    "\n",
    "        clean_value = clean_text(field_value)\n",
    "        if clean_value in name_mapping:\n",
    "            entity_info = name_mapping[clean_value]\n",
    "            result_df.at[idx, 'MATCHED_NAME'] = entity_info['original']\n",
    "            result_df.at[idx, 'ENTITY_ID'] = entity_info['id']\n",
    "            result_df.at[idx, 'ENTITY_TYPE'] = entity_info['type']\n",
    "            result_df.at[idx, 'MATCH_METHOD'] = f'exact_{field_name.lower()}'\n",
    "            result_df.at[idx, 'CONFIDENCE'] = (\n",
    "                'medium' if entity_info.get('alias') else 'high'\n",
    "            )\n",
    "            break\n",
    "\n",
    "    if pd.notna(result_df.at[idx, 'MATCHED_NAME']):\n",
    "        continue\n",
    "\n",
    "    all_referenced_names = []\n",
    "    if not position_is_generic:\n",
    "        all_referenced_names.extend(extract_referenced_names(bene_position))\n",
    "    all_referenced_names.extend(extract_referenced_names(bene_name))\n",
    "\n",
    "    for ref_name in all_referenced_names:\n",
    "        clean_ref = clean_text(ref_name)\n",
    "        if clean_ref in name_mapping:\n",
    "            entity_info = name_mapping[clean_ref]\n",
    "            result_df.at[idx, 'MATCHED_NAME'] = entity_info['original']\n",
    "            result_df.at[idx, 'ENTITY_ID'] = entity_info['id']\n",
    "            result_df.at[idx, 'ENTITY_TYPE'] = entity_info['type']\n",
    "            result_df.at[idx, 'MATCH_METHOD'] = 'reference_exact'\n",
    "            result_df.at[idx, 'CONFIDENCE'] = 'high'\n",
    "            break\n",
    "\n",
    "    if pd.notna(result_df.at[idx, 'MATCHED_NAME']):\n",
    "        continue\n",
    "\n",
    "    last_name_matches = (\n",
    "            search_by_last_name(bene_position, last_names_mapping) +\n",
    "            search_by_last_name(bene_name, last_names_mapping)\n",
    "        )\n",
    "    if last_name_matches:\n",
    "        best_last_name_match = last_name_matches[0]\n",
    "        matched_name = best_last_name_match['full_name']\n",
    "\n",
    "        clean_matched = clean_text(matched_name)\n",
    "        if clean_matched in name_mapping:\n",
    "            entity_info = name_mapping[clean_matched]\n",
    "            result_df.at[idx, 'MATCHED_NAME'] = entity_info['original']\n",
    "            result_df.at[idx, 'ENTITY_ID'] = entity_info['id']\n",
    "            result_df.at[idx, 'ENTITY_TYPE'] = entity_info['type']\n",
    "            result_df.at[idx, 'MATCH_METHOD'] = best_last_name_match['match_type']\n",
    "            result_df.at[idx, 'CONFIDENCE'] = 'medium'\n",
    "            continue\n",
    "\n",
    "    if pd.notna(result_df.at[idx, 'MATCHED_NAME']):\n",
    "        continue\n",
    "\n",
    "    position_has_legislator = any(term in combined_text for term in ['senator', 'representative', 'rep ', 'sen ', 'assemblymember', 'assemblyman', 'assemblywoman', 'assembly member'])\n",
    "    position_has_committee = any(term in combined_text for term in ['committee', 'commission', 'board', 'task force', 'caucus'])\n",
    "    search_pool = None\n",
    "\n",
    "    if position_has_legislator:\n",
    "        search_pool = legislator_names\n",
    "    elif position_has_committee:\n",
    "        search_pool = committee_names\n",
    "\n",
    "    candidates = []\n",
    "    if bene_name.strip():\n",
    "        candidates.extend(get_candidates_by_ngrams(bene_name, ngram_index))\n",
    "\n",
    "    if bene_position.strip() and not position_is_generic:\n",
    "        candidates.extend(get_candidates_by_ngrams(bene_position, ngram_index))\n",
    "\n",
    "    seen = set()\n",
    "    unique_candidates = [(name, score) for name, score in candidates\n",
    "                         if not (name in seen or seen.add(name))]\n",
    "\n",
    "    if search_pool:\n",
    "        unique_candidates = [(name, score) for name, score in unique_candidates if name in search_pool]\n",
    "\n",
    "    top_candidates = unique_candidates[:10] if unique_candidates else []\n",
    "\n",
    "    if top_candidates:\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        best_method = None\n",
    "\n",
    "        for candidate_name, _ in top_candidates:\n",
    "            if candidate_name in last_name_aliases:\n",
    "                continue\n",
    "            if bene_name.strip():\n",
    "                name_score = calculate_similarity_score(bene_name, candidate_name)\n",
    "\n",
    "                if name_score > best_score and name_score >= 75:\n",
    "                    best_score = name_score\n",
    "                    best_match = candidate_name\n",
    "                    best_method = \"fuzzy_name\"\n",
    "            if bene_position.strip() and not position_is_generic:\n",
    "                position_score = calculate_similarity_score(bene_position, candidate_name)\n",
    "                if position_score > best_score and position_score >= 85:\n",
    "                    best_score = position_score\n",
    "                    best_match = candidate_name\n",
    "                    best_method = \"fuzzy_position\"\n",
    "            if bene_name.strip() and bene_position.strip():\n",
    "                combined_text = f\"{bene_name} {bene_position}\"\n",
    "                combined_score = calculate_similarity_score(combined_text, candidate_name)\n",
    "\n",
    "                if combined_score > best_score and combined_score >= 70:\n",
    "                    best_score = combined_score\n",
    "                    best_match = candidate_name\n",
    "                    best_method = \"fuzzy_combined\"\n",
    "\n",
    "        if best_match:\n",
    "            confidence = \"high\" if best_score >= 90 else \"medium\" if best_score >= 75 else \"low\"\n",
    "\n",
    "            result_df.at[idx, 'MATCHED_NAME'] = best_match\n",
    "            result_df.at[idx, 'ENTITY_ID'] = entity_ids[best_match]\n",
    "            result_df.at[idx, 'ENTITY_TYPE'] = 'legislator' if entity_ids[best_match].startswith('LEG_') else 'committee'\n",
    "            result_df.at[idx, 'MATCH_METHOD'] = best_method\n",
    "            result_df.at[idx, 'CONFIDENCE'] = confidence\n",
    "            continue\n",
    "\n",
    "    for field_name, field_value in [('BENE_NAME', bene_name),\n",
    "                                   ('BENE_POSIT', bene_position if not position_is_generic else \"\")]:\n",
    "        if not field_value.strip():\n",
    "            continue\n",
    "\n",
    "        clean_value = clean_text(field_value)\n",
    "        search_list = search_pool if search_pool else list(entity_ids.keys())\n",
    "\n",
    "        name_match = process.extractOne(\n",
    "            clean_value,\n",
    "            search_list,\n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            score_cutoff=85\n",
    "        )\n",
    "\n",
    "        if name_match:\n",
    "            result_df.at[idx, 'MATCHED_NAME'] = name_match[0]\n",
    "            result_df.at[idx, 'ENTITY_ID'] = entity_ids[name_match[0]]\n",
    "            result_df.at[idx, 'ENTITY_TYPE'] = 'legislator' if entity_ids[name_match[0]].startswith('LEG_') else 'committee'\n",
    "            result_df.at[idx, 'MATCH_METHOD'] = f'direct_fuzzy_{field_name.lower()}'\n",
    "            result_df.at[idx, 'CONFIDENCE'] = 'high' if name_match[1] > 90 else 'medium'\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.loc[result_df['MATCHED_NAME'].isin(last_name_aliases.keys()), 'MATCHED_NAME'] = result_df.loc[result_df['MATCHED_NAME'].isin(last_name_aliases.keys()), 'MATCHED_NAME'].map(last_name_aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_name_positions_dict = result_df.loc[result_df['CONFIDENCE'].notna(), ['BENE_NAME', 'BENE_POSIT', 'MATCHED_NAME']].set_index(['BENE_NAME', 'BENE_POSIT']).to_dict()['MATCHED_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb['clean_beneficiary'] = lb[['BENE_NAME', 'BENE_POSIT']].apply(lambda x: ben_name_positions_dict.get(tuple(x), None), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.loc[lb['clean_beneficiary'].notna()].to_csv('calaccess/lobbying_clean2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure_senate = pd.read_csv('calaccess/expenditure_assembly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure_senate['year'] = expenditure_senate['DateRange'].apply(lambda x: int(x.split('-')[0]))\n",
    "expenditure_senate['term'] = expenditure_senate['year'].apply(lambda x: f\"{x}-{x+1}\" if x % 2 != 0 else f\"{x-1}-{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure_assembly = pd.read_csv('calaccess/expenditure_senator.csv')\n",
    "expenditure_assembly['year'] = expenditure_assembly['DateRange'].apply(lambda x: int(x.split('-')[0]))\n",
    "expenditure_assembly['term'] = expenditure_assembly['year'].apply(lambda x: f\"{x}-{x+1}\" if x % 2 != 0 else f\"{x-1}-{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_PREFIXES = [\n",
    "    'SENATOR', 'SEN', 'REPRESENTATIVE', 'REP', 'ASSEMBLYMEMBER',\n",
    "    'ASSEMBLYMAN', 'ASSEMBLYWOMAN', 'ASM'\n",
    "]\n",
    "SUFFIXES = ['JR', 'SR', 'III', 'II', 'IV']\n",
    "\n",
    "def normalize_person_name(name):\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    n = unidecode(name).upper().strip()\n",
    "\n",
    "    # strip stray quotes / punctuation at ends\n",
    "    n = n.replace('.', ' ')\n",
    "    n = n.replace('&', ' AND ')\n",
    "    n = re.sub(r'[^A-Z0-9,\\s]', ' ', n)\n",
    "    n = re.sub(r'\\s+', ' ', n).strip()\n",
    "\n",
    "    # remove leading titles\n",
    "    tokens = n.split()\n",
    "    while tokens and tokens[0] in TITLE_PREFIXES:\n",
    "        tokens = tokens[1:]\n",
    "    n = \" \".join(tokens)\n",
    "\n",
    "    # handle LAST, FIRST M format\n",
    "    if ',' in n:\n",
    "        parts = [p.strip() for p in n.split(',', 1)]\n",
    "        last = parts[0]\n",
    "        first_part = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "        fp_tokens = first_part.split()\n",
    "        # drop suffixes from end\n",
    "        while fp_tokens and fp_tokens[-1] in SUFFIXES:\n",
    "            fp_tokens = fp_tokens[:-1]\n",
    "        # drop middle initials (single letters)\n",
    "        fp_tokens = [t for t in fp_tokens if len(t) > 1]\n",
    "\n",
    "        if fp_tokens:\n",
    "            first = fp_tokens[0]\n",
    "            rest = \" \".join(fp_tokens[1:])\n",
    "            n = \" \".join([first, rest, last]).strip()\n",
    "        else:\n",
    "            n = last\n",
    "\n",
    "    # handle FIRST M LAST without comma (if it shows up)\n",
    "    tokens = n.split()\n",
    "    if len(tokens) >= 2:\n",
    "        # strip suffixes at end\n",
    "        while tokens and tokens[-1] in SUFFIXES:\n",
    "            tokens = tokens[:-1]\n",
    "        # drop one-letter middle initials\n",
    "        core = [t for t in tokens if len(t) > 1]\n",
    "        n = \" \".join(core)\n",
    "\n",
    "    # final cleanup\n",
    "    n = re.sub(r'\\s+', ' ', n).strip()\n",
    "    return n.lower()\n",
    "\n",
    "def build_last_name_aliases_from_index(key_to_full):\n",
    "    last_name_map = {}\n",
    "\n",
    "    for key, full_names in key_to_full.items():\n",
    "        tokens = key.split()\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "        last = tokens[-1]\n",
    "        last_name_map.setdefault(last, set()).update(full_names)\n",
    "\n",
    "    return {\n",
    "        last: sorted(fulls)[0]\n",
    "        for last, fulls in last_name_map.items()\n",
    "        if len(fulls) == 1\n",
    "    }\n",
    "\n",
    "\n",
    "def build_legislator_index(df):\n",
    "    keys = []\n",
    "    key_to_full = {}\n",
    "\n",
    "    for full in df['full_name'].dropna().unique():\n",
    "        key = normalize_person_name(full)\n",
    "        if not key:\n",
    "            continue\n",
    "        if key not in key_to_full:\n",
    "            key_to_full[key] = set()\n",
    "            keys.append(key)\n",
    "        key_to_full[key].add(full)\n",
    "\n",
    "    last_name_aliases = build_last_name_aliases_from_index(key_to_full)\n",
    "    for last, full in last_name_aliases.items():\n",
    "        if last not in key_to_full:\n",
    "            key_to_full[last] = {full}\n",
    "            keys.append(last)\n",
    "\n",
    "    return keys, key_to_full, last_name_aliases\n",
    "\n",
    "\n",
    "asm_keys, asm_key_to_full, asm_last_aliases = build_legislator_index(assembly)\n",
    "sen_keys, sen_key_to_full, sen_last_aliases = build_legislator_index(senate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_targets_with_index(df, target_col, leg_keys, key_to_full,\n",
    "                           min_score_medium=84, min_score_high=92):\n",
    "    out = df.copy()\n",
    "    out['MATCHED_NAME'] = None\n",
    "    out['MATCH_CONFIDENCE'] = None\n",
    "    out['MATCH_SCORE'] = None\n",
    "\n",
    "    for idx, row in out.iterrows():\n",
    "        raw = row.get(target_col, \"\")\n",
    "        if not isinstance(raw, str) or not raw.strip():\n",
    "            continue\n",
    "\n",
    "        key = normalize_person_name(raw)\n",
    "        if not key:\n",
    "            continue\n",
    "\n",
    "        # 1. exact key match\n",
    "        if key in key_to_full:\n",
    "            full = sorted(key_to_full[key])[0]\n",
    "            out.at[idx, 'MATCHED_NAME'] = full\n",
    "            out.at[idx, 'MATCH_SCORE'] = 100\n",
    "            out.at[idx, 'MATCH_CONFIDENCE'] = (\n",
    "                'medium' if key in last_name_aliases else 'high'\n",
    "            )\n",
    "            continue\n",
    "\n",
    "\n",
    "        # 2. fuzzy match over canonical keys (your old match_names, but on normalized space)\n",
    "        m = process.extractOne(\n",
    "            key,\n",
    "            leg_keys,\n",
    "            scorer=fuzz.token_sort_ratio\n",
    "        )\n",
    "        if m:\n",
    "            best_key, score, _ = m\n",
    "            if best_key in last_name_aliases and score < 95:\n",
    "                continue\n",
    "            if score >= min_score_high:\n",
    "                full = sorted(key_to_full[best_key])[0]\n",
    "                out.at[idx, 'MATCHED_NAME'] = full\n",
    "                out.at[idx, 'MATCH_SCORE'] = float(score)\n",
    "                out.at[idx, 'MATCH_CONFIDENCE'] = (\n",
    "                    'medium' if best_key in last_name_aliases else 'high'\n",
    "                )\n",
    "\n",
    "            elif score >= min_score_medium:\n",
    "                full = sorted(key_to_full[best_key])[0]\n",
    "                out.at[idx, 'MATCHED_NAME'] = full\n",
    "                out.at[idx, 'MATCH_CONFIDENCE'] = 'medium'\n",
    "                out.at[idx, 'MATCH_SCORE'] = float(score)\n",
    "            # else leave unmatched\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "exp_as_matched = map_targets_with_index(\n",
    "    expenditure_assembly,\n",
    "    target_col='TargetCandidateName',\n",
    "    leg_keys=asm_keys,\n",
    "    key_to_full=asm_key_to_full,\n",
    "    min_score_medium=78,\n",
    "    min_score_high=91\n",
    ")\n",
    "\n",
    "\n",
    "exp_se_matched = map_targets_with_index(\n",
    "    expenditure_senate,\n",
    "    target_col='TargetCandidateName',\n",
    "    leg_keys=sen_keys,\n",
    "    key_to_full=sen_key_to_full,\n",
    "    min_score_medium=84,\n",
    "    min_score_high=92\n",
    ")\n",
    "\n",
    "exp_se = exp_se_matched[exp_se_matched['MATCHED_NAME'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_names(_names, expenditure_df, cutoff=84):\n",
    "    expenditure_names = {n: n.lower() for n in expenditure_df['TargetCandidateName'].unique()}\n",
    "    name_mapping = {}\n",
    "    __names = [name.lower() for name in _names if isinstance(name, str)]\n",
    "    for title, exp_name in expenditure_names.items():\n",
    "        best_match = process.extractOne(\n",
    "            exp_name,\n",
    "            __names,\n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            score_cutoff=cutoff\n",
    "        )\n",
    "\n",
    "        if best_match:\n",
    "            name_mapping[title] = _names[__names.index(best_match[0])]\n",
    "        else:\n",
    "            name_mapping[title] = None\n",
    "\n",
    "    return name_mapping\n",
    "\n",
    "name_mapping = match_names(assembly['full_name'].unique(), expenditure_assembly, cutoff=72)\n",
    "expenditure_assembly['matched_target_name'] = expenditure_assembly['TargetCandidateName'].map(name_mapping)\n",
    "merged_df = pd.merge(\n",
    "    expenditure_assembly,\n",
    "    assembly,\n",
    "    left_on='matched_target_name',\n",
    "    right_on='full_name',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_as = expenditure_assembly.loc[(expenditure_assembly['matched_target_name'].notna())]\n",
    "exp_as.loc[exp_as['year'] < 2000, ['year', 'term']] = [2000, '2000-2001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_as.to_csv('calaccess/expend_assembly_matched.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "senate_name_mapping = match_names(senate['full_name'].unique(), expenditure_senate, cutoff=80)\n",
    "expenditure_senate['matched_target_name'] = expenditure_senate['TargetCandidateName'].map(senate_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure_senate.loc[expenditure_senate['matched_target_name'].notna()].to_csv('calaccess/expend_senate_matched.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca_leg_etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
