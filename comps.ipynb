{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56de9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import re, json, ast, pathlib, zipfile, tempfile, datetime as _dt, warnings, torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, deque\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d001b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_votes = pd.read_csv('ca_leg/legislation_data/bill_summary_vote_tbl.csv')\n",
    "bill_history = pd.read_csv('ca_leg/legislation_data/bill_history_tbl.csv', dtype={'action_status': str, 'primary_location': str, 'secondary_location': str, 'end_status': str})\n",
    "authors = pd.read_csv('ca_leg/legislation_data/authors.csv')\n",
    "history = pd.read_csv('ca_leg/legislation_data/history.csv')\n",
    "versions = pd.read_csv('ca_leg/legislation_data/bill_versions.csv')\n",
    "bill_votes = pd.read_csv('ca_leg/legislation_data/bill_detail_vote_tbl.csv', parse_dates=['session_date'])\n",
    "bill_summary = pd.read_csv('ca_leg/legislation_data/bill_summary_vote_tbl.csv')\n",
    "bill_motions = pd.read_csv('ca_leg/legislation_data/bill_motion_tbl.csv')\n",
    "locations = pd.read_csv('ca_leg/legislation_data/committee_codes.csv')\n",
    "politicians = pd.read_csv('ca_leg/legislation_data/politicians.csv')\n",
    "lobbying = pd.read_csv('calaccess/lobbying_clean2.csv', dtype={'PAYEE_NAMS': str, 'BAKREF_TID': str})\n",
    "expend_assembly = pd.read_csv('calaccess/expend_assembly_matched.csv', dtype={'TargetPropositionName': str})\n",
    "expend_senate = pd.read_csv('calaccess/expend_senate_matched.csv', dtype={'TargetPropositionName': str})\n",
    "digests = pd.read_csv('ca_leg/legislation_data/digest.csv')\n",
    "hearings = pd.read_csv('ca_leg/legislation_data/committee_hearing_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b534fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes['vote_date_time'] = pd.to_datetime(bill_votes['vote_date_time']).apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "bill_votes['legislator_name'] = bill_votes['legislator_name'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b423fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_KEYWORDS = [\"Assembly Third Reading\",\"Assembly 3rd reading\",\"senate 3rd reading\",\"Senate Third Reading\",\"Concurrence - Urgency Added\",\"Concurrence in Senate Amendments\",\"Do pass as amended, and re-refer\",\"Do pass as amended, but re-refer\",\"Do pass as amended\",\"Do pass and be re-referred\",\"Concurrence\",\"Consent Calendar\",\"Urgency Clause\",\"Special Consent\",\"Motion to Reconsider\",\"Do pass\",\"Reconsideration\",\"Committee amendments\",\"W/O REF. TO FILE\",\"Be re-referred to the Committee\",\"Lay on the Table\",\"Amend by\",\"Unfinished Business\",\"Placed on Appropriations Suspense File\"]\n",
    "\n",
    "def extract_action(motion_text):\n",
    "    if not isinstance(motion_text, str) or motion_text is None:\n",
    "        return None\n",
    "    motion = motion_text.upper()\n",
    "    action = next((act for act in ACTION_KEYWORDS if act.upper() in motion), None)\n",
    "    if action != 'Reconsideration' and 'RECONSIDER' in motion:\n",
    "        if action is not None:\n",
    "            action += ' Reconsideration'\n",
    "        else:\n",
    "            action = 'Reconsideration'\n",
    "    return action if action else None\n",
    "\n",
    "bill_motions['simplified_motion'] = bill_motions['motion_text'].apply(extract_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68367bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_coms = {}\n",
    "for i, row in locations.iterrows():\n",
    "    if row['committee_code'].startswith('CZ'):\n",
    "        continue\n",
    "    name = row['committee_name']\n",
    "    if row['committee_code'].startswith('CS'):\n",
    "        if name.startswith('Sen.'):\n",
    "            cname = re.sub(r'Sen. ', 'senate ', name).lower()\n",
    "        elif name.startswith('Senate '):\n",
    "            cname = name.lower()\n",
    "        else:\n",
    "            cname = 'senate ' + name.lower()\n",
    "    elif row['committee_code'].startswith('CX'):\n",
    "        if name.lower().startswith('assembly'):\n",
    "            cname = name.lower()\n",
    "        else:\n",
    "            cname = 'assembly ' + name.lower()\n",
    "    if re.search(r'x\\d$', cname) is not None:\n",
    "        cname = re.sub(r'x(?=\\d$)', 'no. ', cname)\n",
    "    clean_coms[row['committee_code']] = cname\n",
    "\n",
    "leg_committees = [f\"{row['chamber']} {row['committee_clean']}\".lower() for _, row in politicians[['committee_clean', 'chamber']].drop_duplicates().iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42a3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_committees(_names, clean_coms, threshold=92):\n",
    "    clean_c = list(clean_coms.values())\n",
    "    clean_codes = list(clean_coms.keys())\n",
    "    name_mapping = {}\n",
    "    for i, clean in enumerate(clean_c):\n",
    "        code = clean_codes[i]\n",
    "        matches = []\n",
    "        matches.append(process.extractOne(clean, _names, scorer=fuzz.token_sort_ratio, score_cutoff=threshold))\n",
    "        matches.append(process.extractOne(clean, _names, scorer=fuzz.partial_ratio, score_cutoff=threshold))\n",
    "        valid_matches = [m for m in matches if m is not None]\n",
    "        if len(valid_matches) > 0:\n",
    "            best_match = max(valid_matches, key=lambda x: x[1])\n",
    "            name_mapping[code] = best_match[0]\n",
    "        else:\n",
    "            fall_back = process.extractOne(clean, _names, scorer=fuzz.token_sort_ratio, score_cutoff=threshold - 8)\n",
    "            if fall_back is not None:\n",
    "                name_mapping[code] = fall_back[0]\n",
    "            else:\n",
    "                name_mapping[code] = None\n",
    "    return name_mapping\n",
    "\n",
    "committee_matches = match_committees(leg_committees, clean_coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e35bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations['committee_clean'] = locations['committee_code'].map(committee_matches)\n",
    "locations.loc[locations['committee_name'] == 'EDUCATION X5', 'committee_clean'] = 'Budget and Fiscal Review: Education'\n",
    "locations.loc[locations['committee_code'] == 'CX12', 'committee_clean'] = 'Budget No. 1 on Health and Human Services'\n",
    "locations.loc[locations['committee_code'] == 'CS68', 'committee_clean'] = 'Budget No. 3 - Health and Human Services'\n",
    "locations.loc[locations['committee_code'] == 'CS66', 'committee_clean'] = 'Senate Veterans Affairs'\n",
    "locations.loc[locations['committee_code'] == 'CS56', 'committee_clean'] = 'Senate Public Employment and Retirement'\n",
    "locations.loc[locations['committee_code'] == 'CS62', 'committee_clean'] = 'Senate Budget and Fiscal Review'\n",
    "locations.loc[locations['committee_code'] == 'CX23', 'committee_clean'] = 'Assembly Utilities and Commerce'\n",
    "\n",
    "motion_codes = {row['motion_id']: row['simplified_motion'] for _, row in bill_motions.iterrows()}\n",
    "summary_votes['motion_text'] = summary_votes['motion_id'].map(motion_codes)\n",
    "\n",
    "def repair_bill_id(id):\n",
    "    front, end = id[:4], id[4:]\n",
    "    if re.search(r'\\d{4}$', front):\n",
    "        return f\"{front}{int(front) + 1}{end}\"\n",
    "    else:\n",
    "        return id\n",
    "\n",
    "versions['ID'] = versions['bill_id'].apply(lambda x: repair_bill_id(x))\n",
    "bill_vers = versions.loc[versions['bill_id'].str.startswith('2')].copy()\n",
    "for i, row in bill_vers.iterrows():\n",
    "    tail = f\"{row['VersionNum']}{row['MeasureState']}\"\n",
    "    repaired = repair_bill_id(re.sub(tail, '', row['bill_id']))\n",
    "    end = int(repaired[-4:])\n",
    "    bill_vers.loc[i, 'bill_ID'] = f\"{repaired[:-4]}{end}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8776b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_parties = {row['full_name']: row['Party'] for _, row in politicians[['full_name', 'Party']].drop_duplicates().iterrows()}\n",
    "bill_ids = list(set(bill_votes.loc[bill_votes['bill_id'].str.startswith('2'), 'bill_id'].unique().tolist() + summary_votes.loc[summary_votes['bill_id'].str.startswith('2'), 'bill_id'].unique().tolist()))\n",
    "bill_id_codes = {row['bill_id']: row['bill_ID'] for _, row in bill_vers.drop_duplicates(subset=['bill_id', 'bill_ID']).iterrows()}\n",
    "history['bill_ID'] = history['bill_id'].map(bill_id_codes)\n",
    "history['Date'] = pd.to_datetime(history['Date'])\n",
    "\n",
    "introduction_dates = {}\n",
    "for v, group in history.loc[history['bill_ID'].isin(bill_ids)].groupby('bill_ID'):\n",
    "    introduction_dates[v] = {'Dates': group['Date'].unique().tolist(), 'Actions': group.sort_values('Date', ascending=True).drop_duplicates(subset=['Action', 'Date'])['Action'].tolist()}\n",
    "\n",
    "version_id_mapping = {i: list(group.values) for i, group in bill_vers.groupby('bill_ID')['ID']}\n",
    "version_id_mapping2 = {i: list(group.values) for i, group in bill_vers.groupby('bill_ID')['bill_id']}\n",
    "bv2b = {v: k for k, val in version_id_mapping2.items() for v in val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "787897a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ranges = {}\n",
    "for k, v in introduction_dates.items():\n",
    "    first, last = min(v['Dates']), max(v['Dates'])\n",
    "    date_ranges[k] = {'First_action': first, 'Last_action': last}\n",
    "\n",
    "outcomes = history.loc[history['bill_ID'].notna()].sort_values('Date', ascending=False).groupby('bill_ID').first().reset_index()[['bill_ID', 'Action']]\n",
    "outcomes.loc[outcomes['Action'].isin(['CHAPTERED', 'ENROLLED', 'FILED', 'APPROVED']), 'Outcome'] = 1\n",
    "outcomes.loc[outcomes['Action'] == 'VETOED', 'Outcome'] = -1\n",
    "outcomes.loc[outcomes['Outcome'].isna(), 'Outcome'] = 0\n",
    "outcome = outcomes.set_index('bill_ID')['Outcome'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d7d28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_bill_ids = {}\n",
    "for i in summary_votes.loc[summary_votes['bill_id'].isin(bill_ids)].groupby(['year', 'motion_id'])['bill_id'].value_counts().index:\n",
    "    year, motion_id, bill_id = i\n",
    "    if (year, motion_id) not in vote_bill_ids.keys():\n",
    "        vote_bill_ids[(year, motion_id)] = [bill_id]\n",
    "    else:\n",
    "        vote_bill_ids[(year, motion_id)].append(bill_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5757f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_vers_dig = bill_vers.merge(digests, on='bill_id', how='inner')\n",
    "features = {row['ID']: {'digest': row['DigestText'], 'MeasureState': row['MeasureState'], 'VoteRequired': row['VoteRequired'] if row['VoteRequired'] is not None else 'No', 'VersionNum': row['VersionNum'] if row['VersionNum'] is not None else 'No', 'LocalProgram': row['LocalProgram'] if row['LocalProgram'] is not None else 'No', 'FiscalCommittee': row['FiscalCommittee'] if row['FiscalCommittee'] is not None else 'No', 'TaxLevy': row['TaxLevy'] if row['TaxLevy'] is not None else 'No', 'Urgency': row['Urgency'] if row['Urgency'] is not None else 'No'} for _, row in bill_vers_dig.iterrows()}\n",
    "\n",
    "committee_codes = {v.lower(): k for k, v in enumerate(politicians['committee_clean'].unique().tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb7f4ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes['chamber'] = bill_votes['location_code'].apply(lambda x: 'assembly' if x == 'AFLOOR' or str(x).startswith('CX') else 'senate' if x == 'SFLOOR' or str(x).startswith('CS') else 'full')\n",
    "bill_votes['vote_date_time'] = pd.to_datetime(bill_votes['vote_date_time'])\n",
    "bill_votes['term'] = bill_votes['vote_date_time'].apply(lambda x: f\"{x.year}-{x.year + 1}\" if x.year % 2 == 1 else f\"{x.year - 1}-{x.year}\" if x.year % 2 == 0 and x < pd.Timestamp(year=x.year, month=11, day=2) else f\"{x.year + 1}-{x.year + 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4dda9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_locations = authors.loc[(authors['House'] == 'UNKNOWN') & (authors['bill_id'].map(bill_id_codes).isin(bill_ids)), ['bill_id', 'Name']].drop_duplicates()\n",
    "for i, row in author_locations.iterrows():\n",
    "    if 'AB' in row['bill_id']:\n",
    "        author_locations.loc[i, 'name'] = 'Assembly ' + row['Name']\n",
    "    elif 'SB' in row['bill_id']:\n",
    "        author_locations.loc[i, 'name'] = 'Senate ' + row['Name']\n",
    "    else:\n",
    "        author_locations.loc[i, 'name'] = 'Joint ' + row['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0efa0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_strings(source_list, target_list):\n",
    "    def preprocess_name(name):\n",
    "        if not isinstance(name, str):\n",
    "            return \"\"\n",
    "        name = name.lower()\n",
    "        name = re.sub(r'\\(.*?\\)', '', name)\n",
    "        name = re.sub(r'committee on', '', name)\n",
    "        name = re.sub(r'[^a-z\\s]', ' ', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        return name\n",
    "    clean_source = [preprocess_name(c) for c in source_list]\n",
    "    clean_target = [preprocess_name(c) for c in target_list]\n",
    "    keywords = [\"education\",\"health\",\"finance\",\"budget\",\"transportation\",\"judiciary\",\"environment\",\"agriculture\",\"energy\",\"labor\",\"housing\",\"veterans affairs\",\"public safety\",\"insurance\",\"banking\",\"public health\",\"small business\",\"redistricting\",\"public utilities\",\"natural resources\",\"water\",\"technology\",\"communications\",\"elections\",\"government\",\"appropriations\",\"rules\",\"ethics\",\"criminal justice\",\"environmental protection\",\"college and university\",\"human services\",\"reproductive health\",\"mental health\",\"technology\",\"aggriculture\",\"urban development\",\"renewable energy\",\"gun violence\",\"commerce\",\"privacy\",\"cybersecurity\",\"infrastructure\",\"disaster preparedness\",\"prisons\",\"aging\"]\n",
    "    def get_committee_keywords(name):\n",
    "        return set(kw for kw in keywords if kw in name)\n",
    "    target_keywords = [get_committee_keywords(name) for name in clean_target]\n",
    "    def calculate_similarity(source_idx, target_idx):\n",
    "        source = clean_source[source_idx]\n",
    "        target = clean_target[target_idx]\n",
    "        if not source or not target:\n",
    "            return 0\n",
    "        if source == target:\n",
    "            return 100\n",
    "        token_sort = fuzz.token_sort_ratio(source, target)\n",
    "        token_set = fuzz.token_set_ratio(source, target)\n",
    "        partial = fuzz.partial_ratio(source, target)\n",
    "        source_kw = get_committee_keywords(source)\n",
    "        keyword_overlap = len(source_kw.intersection(target_keywords[target_idx]))\n",
    "        keyword_bonus = min(20, keyword_overlap * 10)\n",
    "        weighted_score = (token_sort * 0.3) + (token_set * 0.5) + (partial * 0.2) + keyword_bonus\n",
    "        return weighted_score\n",
    "    matches = {}\n",
    "    for i, source in enumerate(source_list):\n",
    "        scores = [calculate_similarity(i, j) for j in range(len(target_list))]\n",
    "        if not scores or max(scores) < 60:\n",
    "            matches[source] = None\n",
    "        else:\n",
    "            best_idx = np.argmax(scores)\n",
    "            confidence = scores[best_idx]\n",
    "            if confidence >= 60:\n",
    "                matches[source] = target_list[best_idx]\n",
    "            else:\n",
    "                matches[source] = None\n",
    "    return matches\n",
    "\n",
    "author_com_matches = fuzzy_strings(author_locations['name'].unique().tolist(), leg_committees)\n",
    "author_locations['name'] = author_locations['name'].map(author_com_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7148acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sponsors = authors.loc[authors['bill_id'].map(bill_id_codes).isin(bill_ids)]\n",
    "sponsors['term'] = sponsors['bill_id'].apply(lambda x: f\"{x[:4]}-{int(x[:4]) + 1}\" if int(x[:4]) % 2 == 1 else f\"{int(x[:4]) - 1}-{x[:4]}\" if int(x[:4]) % 2 == 0 and int(x[:4]) < 2009 else f\"{x[:4]}-{int(x[:4]) + 1}\")\n",
    "\n",
    "lob = lobbying.loc[lobbying['clean_beneficiary'].notna(), ['FIRM_NAME', 'EXPN_DSCR', 'clean_beneficiary', 'EXPN_DATE', 'BENE_AMT']]\n",
    "lob['EXPN_DATE'] = pd.to_datetime(lob['EXPN_DATE'])\n",
    "lob['term'] = lob['EXPN_DATE'].apply(lambda x: f\"{x.year}-{x.year + 1}\" if x.year % 2 == 1 else f\"{x.year - 1}-{x.year}\" if x.year % 2 == 0 and x < pd.Timestamp(year=x.year, month=11, day=2) else f\"{x.year}-{x.year + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fd6877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in politicians.loc[politicians['full_name'].apply(lambda x: isinstance(x, float)), ['Term', 'Last', 'chamber']].drop_duplicates().iterrows():\n",
    "    term, last = row['Term'], row['Last']\n",
    "    a = politicians.loc[(politicians['Last'] == last) & (politicians['Term'] == term) & (politicians['full_name'].apply(lambda x: isinstance(x, str)))]\n",
    "    if len(a) > 0:\n",
    "        politicians.loc[(politicians['Term'] == term) & (politicians['Last'] == last) & (politicians['chamber'] == row['chamber']), 'full_name'] = a['full_name'].values[0]\n",
    "        continue\n",
    "    else:\n",
    "        a = politicians.loc[(politicians['Last'] == last) & (politicians['full_name'].apply(lambda x: isinstance(x, str)))]\n",
    "    if len(a) > 0:\n",
    "        politicians.loc[(politicians['Term'] == term) & (politicians['Last'] == last) & (politicians['chamber'] == row['chamber']), 'full_name'] = a['full_name'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c50d9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_names_terms = {}\n",
    "for _, row in politicians[['full_name', 'Term', 'chamber']].drop_duplicates().iterrows():\n",
    "    if ',' in row['full_name']:\n",
    "        name = row['full_name'].split(',')[1].strip() + ' ' + row['full_name'].split(',')[0].strip()\n",
    "    else:\n",
    "        name = row['full_name']\n",
    "    pol_names_terms[(row['full_name'].lower(), row['Term'])] = {'chamber': row['chamber'], 'name': name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce1c79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "expend_assembly = expend_assembly.rename(columns={'term': 'Term'})\n",
    "expend_assembly['chamber'] = 'assembly'\n",
    "expend_senate = expend_senate.rename(columns={'term': 'Term'})\n",
    "expend_senate['chamber'] = 'senate'\n",
    "\n",
    "campaign_contributions = pd.concat([expend_assembly.loc[expend_assembly['matched_target_name'].notna(), ['ExpenderName', 'Amount', 'matched_target_name', 'Term', 'chamber', 'DateEnd']].drop_duplicates(subset=['ExpenderName', 'Amount', 'matched_target_name', 'DateEnd']), expend_senate.loc[expend_senate['matched_target_name'].notna(), ['ExpenderName', 'Amount', 'matched_target_name', 'Term', 'chamber', 'DateEnd']].drop_duplicates(subset=['ExpenderName', 'Amount', 'matched_target_name', 'DateEnd'])])\n",
    "campaign_contributions['DateEnd'] = pd.to_datetime(campaign_contributions['DateEnd'])\n",
    "sponsors['bill_ID'] = sponsors['bill_id'].apply(repair_bill_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f786305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = history.merge(bill_votes, left_on=['bill_ID', 'Date'], right_on=['bill_id', 'vote_date_time'], how='inner').rename(columns={'bill_id_x': 'bill_version'}).drop('bill_id_y', axis=1)\n",
    "voting['bv_id'] = voting['bill_version'].apply(repair_bill_id)\n",
    "\n",
    "voting_places = {}\n",
    "for i, row in voting.groupby(['motion_id', 'term', 'chamber', 'Date']).agg({'legislator_name': lambda x: list(x)}).iterrows():\n",
    "    motion_id, term, chamber, date = i\n",
    "    g = politicians.loc[(politicians['chamber'] == chamber) & (politicians['Term'] == term) & (politicians['Last'].isin(row['legislator_name']))]\n",
    "    voting_places[(motion_id, term, chamber, date)] = {'most_common_committee': g.groupby('committee_clean').size().sort_values(ascending=False).head(1).index[0] if len(g) > 0 else None}\n",
    "voting['voting_place'] = voting.apply(lambda row: voting_places.get((row['motion_id'], row['term'], row['chamber'], row['Date']), {}).get('most_common_committee', None), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cebf2422",
   "metadata": {},
   "outputs": [],
   "source": [
    "hear = hearings[['bill_id', 'location_code']].merge(locations[['committee_code', 'committee_clean']], left_on='location_code', right_on='committee_code', how='left')[['bill_id', 'committee_clean']].drop_duplicates()\n",
    "hear['year'] = hear['bill_id'].apply(lambda x: int(x[:4]))\n",
    "\n",
    "positions = {p: 'Democratic Alternate' if re.search(r'Democratic\\s*Alternate', p) is not None else 'Vice Chair' if re.search(r'V\\s*i\\s*c\\s*e\\s*-*\\s*C\\s*h\\s*a\\s*i\\s*r\\s*', p) is not None else 'Co-Chair' if re.search(r'Co\\s*-\\s*Chair', p) is not None else 'Chair' if re.search(r'Cha\\s*i\\s*r', p) is not None else 'Republican Alternate' if re.search(r'\\s*Republican\\s*Alternate', p) is not None else p for p in politicians['position'].unique()}\n",
    "vnums = bill_vers.set_index('ID')['VersionNum'].to_dict()\n",
    "vid_map = {v: k for k, val in version_id_mapping.items() for v in val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69f26aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_dt(s):\n",
    "    return pd.to_datetime(s, errors='coerce')\n",
    "\n",
    "def _canon_name(n):\n",
    "    n = re.sub(r'[^\\w\\s]', ' ', str(n)).lower()\n",
    "    n = re.sub(r'\\s+', ' ', n).strip()\n",
    "    return n\n",
    "\n",
    "def _infer_origin_chamber_from_bill_id(bill_id):\n",
    "    s = str(bill_id)\n",
    "    if 'AB' in s: return 'assembly'\n",
    "    if 'SB' in s: return 'senate'\n",
    "    return None\n",
    "\n",
    "def _term_from_date(ts):\n",
    "    if pd.isna(ts): return np.nan\n",
    "    y = ts.year\n",
    "    if y % 2 == 1:\n",
    "        return f\"{y}-{y+1}\"\n",
    "    else:\n",
    "        if ts.month < 11:\n",
    "            return f\"{y-1}-{y}\"\n",
    "        return f\"{y+1}-{y+2}\"\n",
    "\n",
    "def _tokenize(s):\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r'[^a-z0-9\\s]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return [t for t in s.split(' ') if t]\n",
    "\n",
    "def _jaccard(a_set, b_set):\n",
    "    if not a_set and not b_set: return 1.0\n",
    "    i = len(a_set & b_set)\n",
    "    u = len(a_set | b_set)\n",
    "    return i / u if u else 0.0\n",
    "\n",
    "def read_zip(zip_path, crs=3857):\n",
    "    tmp = tempfile.TemporaryDirectory()\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        zf.extractall(tmp.name)\n",
    "    shp = next(pathlib.Path(tmp.name).rglob(\"*.shp\"))\n",
    "    gdf = gpd.read_file(shp).set_crs(epsg=crs)\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "    return gdf, tmp\n",
    "\n",
    "def district_cycle(year):\n",
    "    if year <= 2012: return \"2001\"\n",
    "    if year <= 2022: return \"2011\"\n",
    "    return \"current\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75f6d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_gdf, _ = read_zip('dashboard/backend/data/ca_counties.zip')\n",
    "counties_gdf = counties_gdf[['COUNTYFP', 'NAMELSAD', 'geometry']]\n",
    "counties_gdf['county_area'] = counties_gdf.geometry.area\n",
    "counties_gdf['county_id'] = counties_gdf['COUNTYFP'].astype(int)\n",
    "data_dir = pathlib.Path('dashboard/backend/data')\n",
    "asm11_zip = data_dir / '2011_assembly_state_shp.zip'\n",
    "sen11_zip = data_dir / '2011_senate_state_shp.zip'\n",
    "asmcur_zip = data_dir / '2021_AD_Final_shp.zip'\n",
    "sencur_zip = data_dir / '2021_SD_Final_shp.zip'\n",
    "dist_info = [(asm11_zip, \"assembly\", \"2011\", 4019),(sen11_zip, \"senate\", \"2011\", 4019),(asmcur_zip, \"assembly\",\"current\", 4269),(sencur_zip, \"senate\",  \"current\", 4269)]\n",
    "weight_records = []\n",
    "tmps = []\n",
    "for zp, house, cycle, crs in dist_info:\n",
    "    gdf, tmp = read_zip(zp, crs)\n",
    "    tmps.append(tmp)\n",
    "    gdf = gdf.rename(columns={gdf.columns[0]: \"district_id\"})[[\"district_id\", \"geometry\"]]\n",
    "    gdf[\"house\"] = house\n",
    "    gdf[\"cycle\"] = cycle\n",
    "    gdf[\"dist_area\"] = gdf.geometry.area\n",
    "    inter = gpd.overlay(gdf, counties_gdf, how=\"intersection\")\n",
    "    inter[\"fragment_area\"] = inter.geometry.area\n",
    "    weight_records.append(inter[[\"house\", \"cycle\", \"district_id\", \"county_id\", \"fragment_area\", 'county_area', 'dist_area']].reset_index(drop=True))\n",
    "weights = pd.concat(weight_records, ignore_index=True)\n",
    "weights['weight'] = weights['fragment_area'] / weights['county_area']\n",
    "weights['district_share_in_county'] = weights['fragment_area']/weights['dist_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1817d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = politicians.loc[politicians['District No.'].isna(), ['full_name', 'Term']].drop_duplicates()\n",
    "fix['District No.'] = [78, 30, 26, 30, 30, 29, 29, 22, 29, 22, 36, 29, 22, 22, 6]\n",
    "for i, row in fix.iterrows():\n",
    "    politicians.loc[(politicians['full_name'] == row['full_name']) & (politicians['Term'] == row['Term']), 'District No.'] = row['District No.']\n",
    "\n",
    "lob['clean_beneficiary'] = lob['clean_beneficiary'].apply(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "lobb = lob.groupby(['clean_beneficiary', 'term']).agg({'BENE_AMT': 'sum'}).reset_index().rename(columns={'BENE_AMT': 'AMOUNT'})\n",
    "exp_as = expend_assembly[['Amount', 'year', 'matched_target_name']].drop_duplicates().groupby(['matched_target_name', 'year']).agg({'Amount': 'sum'}).reset_index().rename(columns={'year': 'term'})\n",
    "exp_sen = expend_senate.groupby(['matched_target_name', 'year']).agg({'Amount': 'sum'}).reset_index().rename(columns={\"year\": 'term'})\n",
    "politicians['lower'] = politicians['full_name'].str.lower()\n",
    "def name_swap(name):\n",
    "    return re.sub(r'\\,', '', name.lower()).strip()\n",
    "politicians['name2'] = politicians['full_name'].apply(name_swap)\n",
    "politicians.loc[politicians['name2'].isin([p for p in politicians['lower'].unique() if p in lobb['clean_beneficiary'].unique()]), 'name2'] = politicians.loc[politicians['name2'].isin([p for p in politicians['lower'].unique() if p in lobb['clean_beneficiary'].unique()]), 'lower']\n",
    "pl = politicians[['Party', 'District No.', 'Seat No.', 'Term', 'full_name', 'chamber', 'name2']].drop_duplicates().merge(lobb, left_on=['Term', 'name2'], right_on=['term', 'clean_beneficiary'], how='left').rename(columns={'AMOUNT': 'total_lobbying'})\n",
    "exp_as['name2'] = exp_as['matched_target_name'].apply(lambda x: re.sub(r'\\,', '', x.lower()))\n",
    "pl['term'] = pl['Term'].apply(lambda x: int(x.split('-')[-1]))\n",
    "exp_as.loc[exp_as['term'] % 2 == 1, 'term'] = exp_as.loc[exp_as['term'] % 2 == 1, 'term'] - 1\n",
    "exp_sen.loc[exp_sen['term'] % 2 == 1, 'term'] = exp_sen.loc[exp_sen['term'] % 2 == 1, 'term'] - 1\n",
    "pld = pl.merge(exp_as, on=['term', 'name2'], how='left').rename(columns={'Amount': 'total_donations_'})\n",
    "exp_sen['name2'] = exp_sen['matched_target_name'].apply(lambda x: re.sub(r'\\,', '', x.lower()))\n",
    "pldd = pld.merge(exp_sen, on=['term', 'name2'], how='left')\n",
    "pldd['total_donations_'] = pldd[['total_donations_', 'Amount']].sum(skipna=True, axis=1)\n",
    "pldd = pldd.rename(columns={'total_donations_': 'total_donations'})\n",
    "pldd['total_received'] = pldd['total_donations'] + pldd['total_lobbying']\n",
    "for c in ['total_donations', 'total_lobbying', 'total_received']:\n",
    "    pldd[c] = pldd[c].fillna(0).astype(float)\n",
    "lfund = pldd.copy()\n",
    "lfund['District No.'] = lfund['District No.'].astype(str).apply(lambda x: re.sub(r'\\s', '', x)).astype(float).astype(int)\n",
    "lfund_ = lfund.groupby(['Term', 'District No.', 'chamber']).agg({'total_donations': 'sum','total_lobbying': 'sum','total_received': 'sum'}).reset_index()\n",
    "lfund_['cycle'] = lfund_['Term'].apply(lambda x: '2011' if int(x.split('-')[0]) <= 2012 else 'current')\n",
    "reg_funds = lfund_.merge(weights, left_on=['cycle', 'District No.', 'chamber'], right_on=['cycle', 'district_id', 'house'], how='left')\n",
    "reg_funds['total_donations'] *= reg_funds['district_share_in_county']\n",
    "reg_funds['total_lobbying'] *= reg_funds['district_share_in_county']\n",
    "reg_funds['total_received'] *= reg_funds['district_share_in_county']\n",
    "reg_funds_ = reg_funds.groupby(['county_id', 'house']).agg({'total_donations': 'sum','total_lobbying': 'sum','total_received': 'sum'}).reset_index()\n",
    "co_cal = reg_funds_.merge(counties_gdf, on='county_id', how='left')\n",
    "ca_legislator_funding = gpd.GeoDataFrame(co_cal, geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ab0eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_map = {'AYE':1,'YES':1,'NOE':-1,'NO':-1}\n",
    "voting['vote_num'] = voting['vote_code'].str.upper().map(vote_map).fillna(0).astype(int)\n",
    "motion_dict = bill_motions.set_index('motion_id')['motion_text'].to_dict()\n",
    "roll_cols = ['bill_ID','bill_version','Date','motion_id','chamber','voting_place']\n",
    "roll = (voting.groupby(roll_cols, dropna=False).agg(yes=('vote_num', lambda x: int((np.array(x)>0).sum())), no=('vote_num', lambda x: int((np.array(x)<0).sum())), total=('vote_num','count')).reset_index())\n",
    "roll['pass'] = (roll['yes'] > roll['no'])\n",
    "bill_votes['vote_num'] = bill_votes['vote_code'].str.upper().map(vote_map).fillna(0).astype(int)\n",
    "bill_votes['Date'] = pd.to_datetime(bill_votes['vote_date_time']).dt.date\n",
    "roll_cols2 = ['bill_id','Date','motion_id','chamber','location_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad9e2545",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_roll = (bill_votes.groupby(roll_cols2, dropna=False).agg(yes=('vote_num', lambda x: int((np.array(x)>0).sum())), no=('vote_num', lambda x: int((np.array(x)<0).sum())), total=('vote_num','count')).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdc258f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_31820/3102085590.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stages_df = roll.groupby('bill_ID', group_keys=False).apply(_stage_timing).reset_index()\n"
     ]
    }
   ],
   "source": [
    "summary_roll = (bill_votes.groupby(roll_cols2, dropna=False).agg(yes=('vote_num', lambda x: int((np.array(x)>0).sum())), no=('vote_num', lambda x: int((np.array(x)<0).sum())), total=('vote_num','count')).reset_index())\n",
    "summary_roll['pass'] = (summary_roll['yes'] > summary_roll['no'])\n",
    "\n",
    "def _stage_timing(group):\n",
    "    g = group.sort_values('Date')\n",
    "    intro = g['Date'].min()\n",
    "    is_committee = ~(g['voting_place'].isin(['Assembly Floor','Senate Floor']))\n",
    "    comm_ref = g.loc[is_committee, 'Date'].min() if is_committee.any() else pd.NaT\n",
    "    first_read = g['Date'].min() if not g.empty else pd.NaT\n",
    "    second_read = pd.NaT\n",
    "    if pd.notna(first_read):\n",
    "        _after1 = g[(g['Date'] > first_read) & (is_committee)]\n",
    "        if not _after1.empty:\n",
    "            second_read = _after1['Date'].min()\n",
    "    third_read = pd.NaT\n",
    "    if pd.notna(second_read):\n",
    "        _after2 = g[(g['Date'] > second_read)]\n",
    "        if not _after2.empty:\n",
    "            third_read = _after2['Date'].min()\n",
    "    is_floor = summary_roll.loc[(summary_roll['bill_id'] == g['bill_ID'].iloc[0]) & (summary_roll['location_code'].isin(['AFLOOR','SFLOOR']))]\n",
    "    asm_floor_pass = pd.NaT\n",
    "    sen_floor_pass = pd.NaT\n",
    "    if not is_floor.empty:\n",
    "        asm_floor_data = is_floor[(is_floor['location_code'] == 'AFLOOR') & (is_floor['pass'])]\n",
    "        if not asm_floor_data.empty:\n",
    "            asm_floor_pass = asm_floor_data['Date'].min()\n",
    "        sen_floor_data = is_floor[(is_floor['location_code'] == 'SFLOOR') & (is_floor['pass'])]\n",
    "        if not sen_floor_data.empty:\n",
    "            sen_floor_pass = sen_floor_data['Date'].min()\n",
    "    return pd.Series({'intro': intro, 'comm_ref': comm_ref, 'first_read': first_read, 'second_read': second_read, 'third_read': third_read, 'asm_floor_pass': asm_floor_pass, 'sen_floor_pass': sen_floor_pass})\n",
    "\n",
    "stages_df = roll.groupby('bill_ID', group_keys=False).apply(_stage_timing).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "740b0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bill_labels_updated.json', 'r') as f:\n",
    "    bill_labels = json.load(f)\n",
    "stages_df['topic'] = stages_df['bill_ID'].map(bill_labels)\n",
    "stages_df = stages_df.loc[stages_df['topic'].notna()]\n",
    "\n",
    "outcomes = (history.dropna(subset=['bill_ID']).sort_values('Date', ascending=False).groupby('bill_ID').first().reset_index()[['bill_ID','Action']])\n",
    "outcomes.loc[outcomes['Action'].isin(['CHAPTERED','ENROLLED','FILED','APPROVED']),'Outcome'] = 1\n",
    "outcomes.loc[outcomes['Action'].isin(['VETOED']),'Outcome'] = -1\n",
    "outcomes['Outcome'] = outcomes['Outcome'].fillna(0).astype(int)\n",
    "y_df = outcomes[['bill_ID','Outcome']].rename(columns={'Outcome':'outcome'})\n",
    "\n",
    "first_last = (history.dropna(subset=['bill_ID']).groupby('bill_ID')['Date'].agg(First_action='min', Last_action='max').reset_index())\n",
    "pipe_base = (stages_df.merge(first_last, on='bill_ID', how='left').merge(y_df, on='bill_ID', how='left'))\n",
    "stage_order = [c for c in ['intro','comm_ref','first_read','second_read','third_read','asm_floor_pass','sen_floor_pass'] if c in pipe_base.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a104124",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(stage_order[i], stage_order[i+1]) for i in range(len(stage_order)-1)]\n",
    "rows = []\n",
    "for a,b in pairs:\n",
    "    aa = _safe_dt(pipe_base[a]); bb = _safe_dt(pipe_base[b])\n",
    "    entered = int(aa.notna().sum())\n",
    "    advanced = int(((aa.notna()) & (bb.notna())).sum())\n",
    "    rate = float(advanced / entered) if entered else np.nan\n",
    "    mdays = float(np.median((bb - aa).dt.days.dropna().values)) if advanced else np.nan\n",
    "    rows.append({'from':a,'to':b,'entered':entered,'advanced':advanced,'pass_rate':rate,'median_days':mdays})\n",
    "pipeline_stage_funnel = pd.DataFrame(rows)\n",
    "pipeline_timestamps_wide = pipe_base[['bill_ID','topic'] + stage_order].copy()\n",
    "for s in stage_order:\n",
    "    pipeline_timestamps_wide[f'{s}_ts'] = _safe_dt(pipeline_timestamps_wide[s]).astype('int64', errors='ignore')//10**9\n",
    "stuck_rows=[]\n",
    "for a,b in pairs:\n",
    "    aa = _safe_dt(pipe_base[a]); bb = _safe_dt(pipe_base[b])\n",
    "    dd = (bb - aa).dt.days\n",
    "    q90 = np.nanpercentile(dd.dropna().values, 90) if dd.notna().any() else np.nan\n",
    "    sub = pipe_base[(aa.notna()) & (bb.isna())][['bill_ID','topic']].copy()\n",
    "    if not sub.empty:\n",
    "        sub['stage']=a; sub['q90']=q90\n",
    "        stuck_rows.append(sub)\n",
    "pipeline_stuck_candidates = pd.concat(stuck_rows, ignore_index=True) if stuck_rows else pd.DataFrame(columns=['bill_ID','topic','stage','q90'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01298a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hear_seq = (hearings[['bill_id','location_code']].merge(locations[['committee_code','committee_clean']], left_on='location_code', right_on='committee_code', how='left').rename(columns={'committee_clean':'committee'}))\n",
    "route_df = (hear_seq.groupby('bill_id')['committee'].apply(lambda s: tuple([x for x in s.dropna().tolist() if x])).reset_index().rename(columns={'committee':'route'}))\n",
    "route_df['route_key'] = route_df['route'].apply(lambda r: ' > '.join(list(r)[:5]) if isinstance(r, tuple) and r else None)\n",
    "route_df.rename(columns={'bill_id':'bill_ID'}, inplace=True)\n",
    "route_df['topic'] = route_df['bill_ID'].map(bill_labels)\n",
    "route_df = route_df.loc[route_df['topic'].notna()]\n",
    "route_perf = route_df.merge(y_df, on='bill_ID', how='left')\n",
    "route_archetypes = (route_perf.groupby(['topic','route_key']).agg(n=('bill_ID','nunique'), pass_rate=('outcome', lambda x: float(np.mean(np.array(x)==1)) if len(x)>0 else np.nan)).reset_index().sort_values(['topic','n'], ascending=[True,False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e832b6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_31820/2715825013.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_digest_stats)\n"
     ]
    }
   ],
   "source": [
    "dig = digests[['bill_id','DigestText']].copy()\n",
    "dig['bill_ID'] = dig['bill_id'].map(bv2b)\n",
    "ver = versions[['bill_id','VersionNum']].copy()\n",
    "ver['bill_ID'] = ver['bill_id'].map(bv2b)\n",
    "dv = (ver.merge(dig, on=['bill_id','bill_ID'], how='inner').dropna(subset=['DigestText']))\n",
    "def _digest_stats(df):\n",
    "    df = df.sort_values('VersionNum')\n",
    "    toks = [set(_tokenize(t)) for t in df['DigestText']]\n",
    "    sims=[]\n",
    "    for i in range(1,len(toks)):\n",
    "        sims.append(_jaccard(toks[i-1], toks[i]))\n",
    "    return pd.Series({'n_versions': len(df), 'median_sim': float(np.median(sims)) if sims else np.nan})\n",
    "amendment_churn = (\n",
    "    dv.groupby('bill_ID')\n",
    "      .apply(_digest_stats)\n",
    "      .reset_index()\n",
    ")\n",
    "amendment_churn['topic'] = amendment_churn['bill_ID'].map(bill_labels)\n",
    "amendment_churn = amendment_churn.loc[amendment_churn['topic'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fc241cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = pd.Timestamp.now().date()\n",
    "pb = pipe_base[['bill_ID','topic'] + stage_order].copy()\n",
    "for c in stage_order: pb[c] = _safe_dt(pb[c])\n",
    "pb['last_date'] = pb[stage_order].max(axis=1)\n",
    "by_stage_q80 = {}\n",
    "for c in stage_order:\n",
    "    dd = pb[c].apply(lambda x: (now - x.date()).days if pd.notna(x) else np.nan)\n",
    "    by_stage_q80[c] = np.nanpercentile(dd.dropna().values, 80) if dd.notna().any() else np.nan\n",
    "last_stage_col = None\n",
    "for c in reversed(stage_order):\n",
    "    if pb[c].notna().any(): last_stage_col = c; break\n",
    "pb = pb.merge(route_archetypes[['route_key','topic','pass_rate']].drop_duplicates(subset=['route_key','topic']), on='topic', how='left')\n",
    "pb = pb.merge(route_df[['bill_ID','route_key']], on='bill_ID', how='left')\n",
    "pb = pb.merge(amendment_churn[['bill_ID','n_versions']], on='bill_ID', how='left')\n",
    "def _risk_row(r):\n",
    "    churn  = (r.get('n_versions',0) or 0) >= 5\n",
    "    low_route = (r.get('pass_rate',1.0) or 1.0) < 0.3\n",
    "    return int(sum([churn, low_route]))\n",
    "pb['risk'] = pb.apply(_risk_row, axis=1)\n",
    "risk_list = pb[['bill_ID','topic','route_key_x','n_versions','risk']].copy().rename(columns={'route_key_x':'route_key'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91cc98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = stages_df[['bill_ID','comm_ref']].dropna()\n",
    "exits = stages_df[['bill_ID'] + [c for c in stage_order if c!='comm_ref']].copy()\n",
    "exits['has_exit'] = exits.drop(columns=['bill_ID']).notna().any(axis=1)\n",
    "gate = entries.merge(exits[['bill_ID','has_exit']], on='bill_ID', how='left')\n",
    "heard = hear_seq[['bill_id','committee']].dropna().drop_duplicates().rename(columns={'bill_id':'bill_ID'})\n",
    "gk = heard.merge(gate[['bill_ID','has_exit']], on='bill_ID', how='left')\n",
    "committee_gatekeeping = (gk.groupby('committee').agg(entries=('bill_ID','nunique'), exits=('has_exit', lambda x: int(np.nansum(np.array(x)==True)))).reset_index())\n",
    "committee_gatekeeping['gatekeeping'] = 1 - (committee_gatekeeping['exits'] / committee_gatekeeping['entries'].replace(0, np.nan))\n",
    "hear_dates = history[['bill_ID','Date','Action']].copy()\n",
    "hear_dates = hear_dates[hear_dates['Action'].str.upper().str.contains('HEARING|REFERRED|RE-REFERRED|COMMITTEE', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b546792",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = stages_df[['bill_ID']].copy()\n",
    "origin['origin'] = origin['bill_ID'].apply(_infer_origin_chamber_from_bill_id)\n",
    "ccf = stages_df[['bill_ID','asm_floor_pass','sen_floor_pass']].copy()\n",
    "ccf['A_then_S'] = ccf['asm_floor_pass'].notna() & ccf['sen_floor_pass'].notna()\n",
    "ccf['S_then_A'] = ccf['sen_floor_pass'].notna() & ccf['asm_floor_pass'].notna()\n",
    "ccf['topic'] = ccf['bill_ID'].map(bill_labels)\n",
    "ccf = ccf.loc[ccf['topic'].notna()]\n",
    "cross_chamber_friction = (ccf.groupby('topic').agg(pass_Asm_then_Sen=('A_then_S', lambda x: int(np.nansum(x))), pass_Sen_then_Asm=('S_then_A', lambda x: int(np.nansum(x)))).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "140b5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = stages_df[['bill_ID','intro']].merge(y_df, on='bill_ID', how='left')\n",
    "sv['start'] = _safe_dt(sv['intro'])\n",
    "ends = first_last[['bill_ID','Last_action']].rename(columns={'Last_action':'end'})\n",
    "sv = sv.merge(ends, on='bill_ID', how='left')\n",
    "sv['end'] = _safe_dt(sv['end'])\n",
    "sv['topic'] = sv['bill_ID'].map(bill_labels)\n",
    "sv = sv.loc[sv['topic'].notna()]\n",
    "def _survival_topic(df):\n",
    "    df = df.dropna(subset=['start'])\n",
    "    if df.empty: return pd.DataFrame(columns=['t','survival'])\n",
    "    t0 = df['start'].min()\n",
    "    t1 = df['end'].max() if df['end'].notna().any() else t0 + pd.Timedelta(days=1)\n",
    "    grid = pd.date_range(t0, t1, freq='7D')\n",
    "    rows=[]\n",
    "    for g in grid:\n",
    "        alive = ((df['end'].isna()) | (df['end'] > g)).sum()\n",
    "        total = len(df)\n",
    "        rows.append({'t': g, 'survival': alive/total if total else np.nan})\n",
    "    return pd.DataFrame(rows)\n",
    "_surv = []\n",
    "for topic, g in sv.groupby('topic'):\n",
    "    sdf = _survival_topic(g)\n",
    "    sdf['topic'] = topic\n",
    "    _surv.append(sdf)\n",
    "survival_curves = pd.concat(_surv, ignore_index=True) if _surv else pd.DataFrame(columns=['t','survival','topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7a8039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = voting[['bill_ID','legislator_name','vote_code','location_code']].copy()\n",
    "vote_num_map = {'AYE':1,'YES':1,'NOE':-1,'NO':-1}\n",
    "v['vote'] = v['vote_code'].str.upper().map(vote_num_map).fillna(0).astype(int)\n",
    "mat = v.pivot_table(index='legislator_name', columns='bill_ID', values='vote', aggfunc='first').fillna(0).astype(int)\n",
    "l = mat.index.to_list()\n",
    "sim_edges=[]\n",
    "if mat.shape[0] >= 2:\n",
    "    X = mat.to_numpy(dtype=np.float32)\n",
    "    Xc = X - X.mean(axis=1, keepdims=True)\n",
    "    denom = np.sqrt((Xc**2).sum(axis=1, keepdims=True)); denom[denom==0]=1.0\n",
    "    Xn = Xc/denom\n",
    "    for i in range(Xn.shape[0]):\n",
    "        dots = Xn[i] @ Xn.T\n",
    "        dots[i] = -1\n",
    "        idx = np.where(dots>=0.6)[0]\n",
    "        for j in idx:\n",
    "            if i<j:\n",
    "                sim_edges.append((l[i], l[j], float(dots[j])))\n",
    "vote_similarity_edges = pd.DataFrame(sim_edges, columns=['u','v','sim'])\n",
    "adj=defaultdict(list)\n",
    "for _,r in vote_similarity_edges.iterrows():\n",
    "    adj[r['u']].append(r['v']); adj[r['v']].append(r['u'])\n",
    "visited=set(); comps=[]\n",
    "for node in l:\n",
    "    if node in visited: continue\n",
    "    q=deque([node]); comp=[]\n",
    "    while q:\n",
    "        x=q.popleft()\n",
    "        if x in visited: continue\n",
    "        visited.add(x); comp.append(x)\n",
    "        for nb in adj.get(x, []):\n",
    "            if nb not in visited: q.append(nb)\n",
    "    comps.append(comp)\n",
    "vote_communities = pd.DataFrame([(n,i) for i,comp in enumerate(comps) for n in comp], columns=['legislator_name','community'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a129fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = voting[['legislator_name','vote_code','location_code']].copy()\n",
    "vc['is_floor'] = vc['location_code'].isin(['AFLOOR','SFLOOR'])\n",
    "vc['yes'] = vc['vote_code'].str.upper().isin(['AYE','YES']).astype(int)\n",
    "leg_comm = vc[~vc['is_floor']].groupby('legislator_name')['yes'].mean().rename('comm_yes')\n",
    "leg_floor = vc[vc['is_floor']].groupby('legislator_name')['yes'].mean().rename('floor_yes')\n",
    "committee_floor_drift = (pd.concat([leg_comm, leg_floor], axis=1).reset_index())\n",
    "committee_floor_drift['drift'] = committee_floor_drift['floor_yes'] - committee_floor_drift['comm_yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9325c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dig_m = digests[['bill_id','DigestText']].copy()\n",
    "dig_m['bill_ID'] = dig_m['bill_id'].map(bv2b)\n",
    "dig_m = dig_m.dropna(subset=['bill_ID','DigestText'])\n",
    "dig_m = dig_m.merge(y_df, on='bill_ID', how='left')\n",
    "def _lift(df):\n",
    "    toks_pos=defaultdict(int); toks_neg=defaultdict(int)\n",
    "    for _,r in df.iterrows():\n",
    "        toks=set(_tokenize(r['DigestText']))\n",
    "        if int(r.get('outcome',0))==1:\n",
    "            for t in toks: toks_pos[t]+=1\n",
    "        else:\n",
    "            for t in toks: toks_neg[t]+=1\n",
    "    rows=[]\n",
    "    all_t = set(list(toks_pos.keys())+list(toks_neg.keys()))\n",
    "    for t in all_t:\n",
    "        pos = toks_pos.get(t,0)+1\n",
    "        neg = toks_neg.get(t,0)+1\n",
    "        rows.append((t, float(np.log(pos/neg)), toks_pos.get(t,0), toks_neg.get(t,0)))\n",
    "    return (pd.DataFrame(rows, columns=['token','log_lift_pass_vs_other','pos','neg']).sort_values('log_lift_pass_vs_other', ascending=False))\n",
    "text_lift_top_tokens = _lift(dig_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1297e2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_31820/3676854094.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  donor_portfolios_hhi = (port.groupby('ExpenderName').apply(_hhi).reset_index().rename(columns={0:'hhi'}))\n"
     ]
    }
   ],
   "source": [
    "ca = pd.concat([\n",
    "    expend_assembly[['ExpenderName','Amount','matched_target_name','Term','DateEnd']].dropna(subset=['ExpenderName','Amount','matched_target_name']),\n",
    "    expend_senate  [['ExpenderName','Amount','matched_target_name','Term','DateEnd']].dropna(subset=['ExpenderName','Amount','matched_target_name'])\n",
    "], ignore_index=True)\n",
    "port = ca.groupby(['ExpenderName','matched_target_name'])['Amount'].sum().reset_index()\n",
    "def _hhi(g):\n",
    "    s = g['Amount'].sum()\n",
    "    if s<=0: return np.nan\n",
    "    p = (g['Amount']/s).values\n",
    "    return float(np.sum(p*p))\n",
    "donor_portfolios_hhi = (port.groupby('ExpenderName').apply(_hhi).reset_index().rename(columns={0:'hhi'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6afbb322",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = voting[['legislator_name','vote_code','vote_date_time']].copy()\n",
    "pol_last_names = politicians[['Last', 'full_name']].dropna().drop_duplicates()\n",
    "pol_last_names['canon'] = pol_last_names['full_name'].apply(_canon_name)\n",
    "pln_map = dict(zip(pol_last_names['Last'].str.lower(), pol_last_names['canon']))\n",
    "\n",
    "vt['canon'] = vt['legislator_name'].apply(_canon_name).map(pln_map)\n",
    "vt['term'] = vt['vote_date_time'].apply(_term_from_date)\n",
    "vt['yes'] = vt['vote_code'].str.upper().isin(['AYE','YES']).astype(int)\n",
    "leg_term_rate = vt.groupby(['canon','term'])['yes'].mean().reset_index().rename(columns={'yes':'yes_rate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f87b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "don = ca.copy()\n",
    "don['canon'] = don['matched_target_name'].apply(_canon_name)\n",
    "fund = (don.groupby(['canon','Term'])['Amount'].sum().reset_index().rename(columns={'Term':'term','Amount':'funding'}))\n",
    "ft = fund.merge(leg_term_rate, on=['canon','term'], how='inner')\n",
    "def _quartiles(g):\n",
    "    if g.empty: return pd.Series({'yes_rate_top':np.nan,'yes_rate_bottom':np.nan,'delta':np.nan,'n_top':0,'n_bottom':0})\n",
    "    q = g['funding'].quantile([0.25,0.75]).values\n",
    "    low = g[g['funding']<=q[0]]; high = g[g['funding']>=q[1]]\n",
    "    return pd.Series({'yes_rate_top': float(high['yes_rate'].mean()) if not high.empty else np.nan, 'yes_rate_bottom': float(low['yes_rate'].mean()) if not low.empty else np.nan, 'delta': float((high['yes_rate'].mean() - low['yes_rate'].mean())) if (not high.empty and not low.empty) else np.nan, 'n_top': int(high.shape[0]), 'n_bottom': int(low.shape[0])})\n",
    "money_vote_alignment = ft.groupby('term').apply(_quartiles, include_groups=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfe32471",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc2 = ca[['DateEnd','Amount']].dropna().copy()\n",
    "cc2['DateEnd'] = pd.to_datetime(_safe_dt(cc2['DateEnd'])).dt.date\n",
    "start_min = pd.to_datetime(_safe_dt(first_last['First_action']).min() if first_last['First_action'].notna().any() else cc2['DateEnd'].min()).date()\n",
    "cc2['t'] = cc2['DateEnd'].apply(lambda d: (d - start_min).days if pd.notna(d) else np.nan)\n",
    "money_event_time_curve = (cc2.groupby('t')['Amount'].mean().reset_index().sort_values('t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aaf9a98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_31820/3274793717.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  signals = (roll.groupby('bill_ID').apply(lambda g: float(np.mean((g['yes']/(g['total'].replace(0, np.nan))) >= 0.5))).reset_index().rename(columns={0:'vote_signal'}))\n"
     ]
    }
   ],
   "source": [
    "bill_dates_df = first_last.copy()\n",
    "bill_dates_df['longevity_days'] = (bill_dates_df['Last_action'] - bill_dates_df['First_action']).dt.days\n",
    "signals = (roll.groupby('bill_ID').apply(lambda g: float(np.mean((g['yes']/(g['total'].replace(0, np.nan))) >= 0.5))).reset_index().rename(columns={0:'vote_signal'}))\n",
    "n_versions = (versions.assign(bill_ID=lambda d: d['bill_id'].map(bv2b)).dropna(subset=['bill_ID']).groupby('bill_ID')['VersionNum'].nunique().reset_index().rename(columns={'VersionNum':'bill_version_count'}))\n",
    "y_df['topic'] = y_df['bill_ID'].map(bill_labels)\n",
    "y_df2 = y_df.loc[y_df['topic'].notna()].copy()\n",
    "bills_table = (y_df2.merge(bill_dates_df[['bill_ID','First_action','longevity_days', 'Last_action']], on='bill_ID', how='left').merge(signals, on='bill_ID', how='left').merge(amendment_churn[['bill_ID','n_versions','median_sim']], on='bill_ID', how='left').merge(n_versions, on='bill_ID', how='left'))\n",
    "bills_table['First_action'] = pd.to_datetime(bills_table['First_action']).dt.strftime('%Y-%m-%d')\n",
    "bills_table['Last_action'] = pd.to_datetime(bills_table['Last_action']).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a36037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = voting[['bill_ID','legislator_name','vote_code','chamber','term', 'location_code', 'Date']].copy()\n",
    "vv['last'] = vv['legislator_name'].str.lower().str.strip()\n",
    "vv['yes'] = vv['vote_code'].str.upper().isin(['AYE','YES']).astype(int)\n",
    "legislators_last_names = {}\n",
    "for _, row in politicians[['chamber', 'Last', 'Term', 'full_name']].drop_duplicates().iterrows():\n",
    "    legislators_last_names[(row['chamber'], row['Last'].lower(), row['Term'])] = row['full_name']\n",
    "def _resolve_full_name(row):\n",
    "    return legislators_last_names.get((row['chamber'], row['last'], row['term']), np.nan)\n",
    "vv['full_name'] = vv.apply(_resolve_full_name, axis=1)\n",
    "vv['party'] = vv['full_name'].map(leg_parties)\n",
    "vv['topic'] = vv['bill_ID'].map(bill_labels)\n",
    "vv = vv.loc[vv['topic'].notna()]\n",
    "vv_major = vv[vv['party'].isin(['D','R'])].copy()\n",
    "rc = (vv_major.groupby(['bill_ID','term','topic','party'])['yes'].mean().unstack('party').reset_index().rename(columns={'D':'yes_D','R':'yes_R'}))\n",
    "for c in ['yes_D','yes_R']:\n",
    "    if c not in rc.columns: rc[c] = np.nan\n",
    "rc['polarization'] = (rc['yes_D'] - rc['yes_R']).abs()\n",
    "rc['party_line_split'] = np.where(((rc['yes_D']>0.5) & (rc['yes_R']<0.5)) | ((rc['yes_D']<0.5) & (rc['yes_R']>0.5)), 1, 0)\n",
    "topic_controversy = (rc.groupby(['topic','term']).agg(n_rollcalls=('bill_ID','nunique'), mean_polarization=('polarization','mean'), median_polarization=('polarization','median'), party_line_share=('party_line_split','mean'), dem_yes_rate=('yes_D','mean'), rep_yes_rate=('yes_R','mean')).reset_index())\n",
    "rollcall_party_splits = rc[['bill_ID','term','topic','yes_D','yes_R','polarization','party_line_split']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68730d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vv2 = vv.copy()\n",
    "vv2['canon'] = vv2['legislator_name'].apply(_canon_name)\n",
    "vv2['any_vote'] = 1\n",
    "_weight_col = 'yes'\n",
    "topic_votes = (vv2.dropna(subset=['topic']).groupby(['canon','term','topic'])[_weight_col].sum().reset_index(name='topic_votes'))\n",
    "total_votes = (vv2.groupby(['canon','term'])[_weight_col].sum().reset_index(name='total_votes'))\n",
    "weights_topics = (topic_votes.merge(total_votes, on=['canon','term'], how='left'))\n",
    "weights_topics['topic_share'] = np.where(weights_topics['total_votes']>0, weights_topics['topic_votes']/weights_topics['total_votes'], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e63a9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (expend_assembly, expend_senate):\n",
    "    df['year'] = df['Term'].str.extract(r'^(\\d{4})').astype(int)\n",
    "    df['term'] = np.where((df['year']%2==0), df['year']-1, df['year'])\n",
    "    df['term'] = df['term'].astype(int).astype(str) + '-' + (df['term']+1).astype(int).astype(str)\n",
    "    df['canon'] = df['matched_target_name'].apply(_canon_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b76878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "don_by_leg_term = (pd.concat([expend_assembly, expend_senate], ignore_index=True).groupby(['canon','term'])['Amount'].sum().reset_index().rename(columns={'Amount':'donations'}))\n",
    "lb2 = lobbying[['clean_beneficiary','EXPN_DATE','BENE_AMT']].dropna().copy()\n",
    "lb2['EXPN_DATE'] = pd.to_datetime(lb2['EXPN_DATE'], errors='coerce')\n",
    "lb2['term'] = lb2['EXPN_DATE'].apply(lambda x: np.nan if pd.isna(x) else (f\"{x.year-1}-{x.year}\" if (x.year%2==0 and x.month<11) else f\"{x.year+1}-{x.year+2}\" if (x.year%2==0) else f\"{x.year}-{x.year+1}\"))\n",
    "lb2['canon'] = lb2['clean_beneficiary'].apply(_canon_name)\n",
    "lob_by_leg_term = (lb2.groupby(['canon','term'])['BENE_AMT'].sum().reset_index().rename(columns={'BENE_AMT':'lobbying'}))\n",
    "fund_leg_term = (don_by_leg_term.merge(lob_by_leg_term, on=['canon','term'], how='outer').fillna({'donations':0.0,'lobbying':0.0}))\n",
    "fund_leg_term['total_received'] = fund_leg_term['donations'] + fund_leg_term['lobbying']\n",
    "weights_topics['canon'] = weights_topics['canon'].map(pln_map)\n",
    "alloc = (weights_topics.merge(fund_leg_term, on=['canon','term'], how='left').fillna({'donations':0.0,'lobbying':0.0,'total_received':0.0}))\n",
    "alloc['donations_topic'] = alloc['donations'] * alloc['topic_share']\n",
    "alloc['lobbying_topic']  = alloc['lobbying']  * alloc['topic_share']\n",
    "alloc['total_topic'] = alloc['total_received'] * alloc['topic_share']\n",
    "topic_funding_by_term = (alloc.groupby(['topic','term']).agg(total_donations=('donations_topic','sum'), total_lobbying=('lobbying_topic','sum'), total_received=('total_topic','sum')).reset_index())\n",
    "topic_funding_by_leg = (alloc.groupby(['canon','term','topic']).agg(donations=('donations_topic','sum'), lobbying=('lobbying_topic','sum'), total=('total_topic','sum')).reset_index())\n",
    "don_leg_term = (pd.concat([expend_assembly, expend_senate], ignore_index=True).assign(canon=lambda d: d['matched_target_name'].apply(_canon_name)).rename(columns={'Amount':'donation'}))\n",
    "don_alloc = (don_leg_term.merge(weights_topics[['canon','term','topic','topic_share']], on=['canon','term'], how='left').fillna({'topic_share':0.0}))\n",
    "don_alloc['donation_topic'] = don_alloc['donation'] * don_alloc['topic_share']\n",
    "donor_topic_by_term = (don_alloc.groupby(['ExpenderName','topic','term'])['donation_topic'].sum().reset_index().rename(columns={'donation_topic':'donations_allocated'}))\n",
    "for _df in (topic_funding_by_term, topic_funding_by_leg, donor_topic_by_term):\n",
    "    _df['term'] = _df['term'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b938896",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_counts = (stages_df.dropna(subset=['intro']).assign(month=lambda d: _safe_dt(d['intro']).dt.to_period('M').astype(str)).groupby(['topic','month']).agg(introduced=('bill_ID','nunique')).reset_index())\n",
    "monthly_counts['month'] = pd.to_datetime(monthly_counts['month']+'-01')\n",
    "topic_momentum = []\n",
    "for t, g in monthly_counts.sort_values('month').groupby('topic'):\n",
    "    s = g.set_index('month')['introduced'].astype(float)\n",
    "    if len(s)>=2:\n",
    "        ema = s.ewm(span=12, adjust=False).mean()\n",
    "    else:\n",
    "        ema = s.copy()\n",
    "    topic_momentum.append(pd.DataFrame({'topic':t,'month':ema.index,'ema_introduced':ema.values}))\n",
    "topic_momentum = pd.concat(topic_momentum, ignore_index=True) if topic_momentum else pd.DataFrame(columns=['topic','month','ema_introduced'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08ecd2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_funnel_obs = (pipe_base.assign(any_comm=pipe_base['comm_ref'].notna().astype(int), any_floor=((pipe_base['asm_floor_pass'].notna()) | (pipe_base['sen_floor_pass'].notna())).astype(int)).groupby('topic').agg(introduced=('bill_ID','nunique'), reached_committee=('any_comm','sum'), reached_floor=('any_floor','sum'), passed_outcome=('outcome', lambda x: int(np.sum(np.array(x)==1)))).reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1c50935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_cpu(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu()\n",
    "    if isinstance(x, dict):\n",
    "        return {k: _to_cpu(v) for k, v in x.items()}\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [ _to_cpu(v) for v in x ]\n",
    "    return x\n",
    "\n",
    "def _to_list(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy().tolist()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.tolist()\n",
    "    return x\n",
    "\n",
    "def _scalarize(v):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        if v.numel() == 1:\n",
    "            return v.item()\n",
    "        return v.detach().cpu().numpy().tolist()\n",
    "    if isinstance(v, dict):\n",
    "        return {k: _scalarize(u) for k, u in v.items()}\n",
    "    if isinstance(v, list):\n",
    "        return [ _scalarize(u) for u in v ]\n",
    "    if isinstance(v, np.ndarray):\n",
    "        return v.tolist()\n",
    "    return v\n",
    "\n",
    "def _as_df(obj):\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return obj\n",
    "    if isinstance(obj, list) and len(obj) and isinstance(obj[0], dict):\n",
    "        return pd.DataFrame([{k: _scalarize(v) for k, v in row.items()} for row in obj])\n",
    "    if isinstance(obj, dict):\n",
    "        keys = list(obj.keys())\n",
    "        if all(isinstance(obj[k], (list, np.ndarray, torch.Tensor)) for k in keys):\n",
    "            return pd.DataFrame({k: _to_list(obj[k]) for k in keys})\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        x = obj.detach().cpu().numpy()\n",
    "        if x.ndim == 1:\n",
    "            return pd.DataFrame({0: x})\n",
    "        return pd.DataFrame(x)\n",
    "    try:\n",
    "        return pd.DataFrame(obj)\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _find(obj, name):\n",
    "    if isinstance(obj, dict):\n",
    "        if name in obj:\n",
    "            return obj[name]\n",
    "        for v in obj.values():\n",
    "            r = _find(v, name)\n",
    "            if r is not None:\n",
    "                return r\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        for v in obj:\n",
    "            r = _find(v, name)\n",
    "            if r is not None:\n",
    "                return r\n",
    "    return None\n",
    "\n",
    "ckpt = torch.load('legnn_eff_outputs.pt', weights_only=False)\n",
    "ckpt = _to_cpu(ckpt)\n",
    "\n",
    "raw_per_bill = _find(ckpt, 'per_bill')\n",
    "raw_actor_topic = _find(ckpt, 'actor_topic')\n",
    "raw_actor_overall = _find(ckpt, 'actor_overall')\n",
    "raw_committee_overall = _find(ckpt, 'committee_overall')\n",
    "\n",
    "per_bill = _as_df(raw_per_bill) if raw_per_bill is not None else pd.DataFrame(columns=['bill_id','topic_id','p_pass_total'])\n",
    "actor_topic = _as_df(raw_actor_topic) if raw_actor_topic is not None else pd.DataFrame(columns=['actor_id','actor_type','topic_id','stance','certainty','influence_delta_mean','engagement'])\n",
    "actor_overall = _as_df(raw_actor_overall) if raw_actor_overall is not None else pd.DataFrame(columns=['actor_id','actor_type','overall_influence'])\n",
    "committee_overall = _as_df(raw_committee_overall) if raw_committee_overall is not None else pd.DataFrame(columns=['committee_id','overall_influence','gate_index'])\n",
    "\n",
    "if 'bill_id' not in per_bill.columns and 'bill_ID' in per_bill.columns:\n",
    "    per_bill = per_bill.rename(columns={'bill_ID':'bill_id'})\n",
    "if 'topic' in per_bill.columns and 'topic_id' not in per_bill.columns:\n",
    "    per_bill = per_bill.rename(columns={'topic':'topic_id'})\n",
    "if 'p_pass' in per_bill.columns and 'p_pass_total' not in per_bill.columns:\n",
    "    per_bill = per_bill.rename(columns={'p_pass':'p_pass_total'})\n",
    "\n",
    "if 'actor' in actor_topic.columns and 'actor_id' not in actor_topic.columns:\n",
    "    actor_topic = actor_topic.rename(columns={'actor':'actor_id'})\n",
    "if 'overall' in actor_overall.columns and 'overall_influence' not in actor_overall.columns:\n",
    "    actor_overall = actor_overall.rename(columns={'overall':'overall_influence'})\n",
    "if 'gate_index' not in committee_overall.columns and 'overall_influence' in committee_overall.columns:\n",
    "    committee_overall['gate_index'] = committee_overall['overall_influence']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bcfb0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_route_baseline = route_archetypes.groupby('topic')['pass_rate'].mean().reset_index().rename(columns={'pass_rate':'topic_route_baseline'})\n",
    "route_baseline = route_archetypes.merge(topic_route_baseline, on='topic', how='left')\n",
    "route_baseline['route_uplift_vs_topic'] = route_baseline['pass_rate'] - route_baseline['topic_route_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af2b43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not per_bill.empty and {'bill_id','p_pass_total'}.issubset(per_bill.columns):\n",
    "    pb_topic = pd.DataFrame({'bill_ID': list(bill_labels.keys()), 'topic': list(bill_labels.values())})\n",
    "    modeled_join = pb_topic.merge(per_bill.rename(columns={'bill_id':'bill_ID','topic_id':'topic'}), on='bill_ID', how='left')\n",
    "    topic_modeled_pass = modeled_join.groupby('topic')['p_pass_total'].mean().reset_index().rename(columns={'p_pass_total':'modeled_pass_mean'})\n",
    "    topic_funnel_modeled = topic_funnel_obs.merge(topic_modeled_pass, on='topic', how='left')\n",
    "else:\n",
    "    topic_funnel_modeled = topic_funnel_obs.assign(modeled_pass_mean=np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99d6904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "readability_df = digests[['bill_id','DigestText']].dropna().copy()\n",
    "def _readability(s):\n",
    "    txt = str(s)\n",
    "    words = re.findall(r'\\w+', txt)\n",
    "    sents = re.split(r'[.!?]+', txt)\n",
    "    syllables = sum([len(re.findall(r'[aeiouyAEIOUY]+', w)) for w in words])\n",
    "    W = max(1,len(words)); S = max(1,len([x for x in sents if x.strip()]))\n",
    "    fk = 206.835 - 1.015*(W/S) - 84.6*(syllables/W)\n",
    "    return fk\n",
    "readability_df['flesch_kincaid'] = readability_df['DigestText'].apply(_readability)\n",
    "readability_df['bill_ID'] = readability_df['bill_id'].map(bv2b)\n",
    "readability_df = readability_df.dropna(subset=['bill_ID'])\n",
    "readability_outcomes = readability_df.merge(y_df[['bill_ID','outcome']], on='bill_ID', how='left')\n",
    "readability_stats = readability_outcomes.groupby('bill_ID').agg(fk_mean=('flesch_kincaid','mean')).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "db9537bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_31820/3884072434.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_late_flag)\n"
     ]
    }
   ],
   "source": [
    "late_drift = dv.copy()\n",
    "late_drift = late_drift.merge(stages_df[['bill_ID','asm_floor_pass','sen_floor_pass']], on='bill_ID', how='left')\n",
    "def _late_flag(df):\n",
    "    df = df.sort_values('VersionNum')\n",
    "    toks = [set(_tokenize(t)) for t in df['DigestText']]\n",
    "    sims=[]\n",
    "    for i in range(1,len(toks)):\n",
    "        sims.append(1-_jaccard(toks[i-1], toks[i]))\n",
    "    if not sims: return pd.Series({'late_drift_flag':0,'last_delta':np.nan})\n",
    "    return pd.Series({'late_drift_flag': int(np.percentile(sims,95)<=sims[-1]) if len(sims)>=1 else 0,'last_delta': sims[-1] if sims else np.nan})\n",
    "\n",
    "late_drift_flags = (\n",
    "    dv.groupby('bill_ID')\n",
    "      .apply(_late_flag)\n",
    "      .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "577d70ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bills_table = bills_table.merge(readability_stats, on='bill_ID', how='left').merge(late_drift_flags, on='bill_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a5749d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_roll_with_bill = summary_roll.copy()\n",
    "summary_roll_with_bill = summary_roll_with_bill.dropna(subset=['bill_id'])\n",
    "floor_only = summary_roll_with_bill[summary_roll_with_bill['location_code'].isin(['AFLOOR','SFLOOR'])].copy()\n",
    "floor_only['Date'] = pd.to_datetime(floor_only['Date'])\n",
    "last_floor = floor_only.sort_values('Date').groupby('bill_id').tail(1)[['bill_id','yes','no','total','Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "79423b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_floor = vv[vv['location_code'].isin(['AFLOOR','SFLOOR'])].copy().rename(columns={'bill_ID':'bill_id'})\n",
    "vv_floor['Date'] = pd.to_datetime(vv_floor['Date'])\n",
    "vv_floor = vv_floor.merge(last_floor[['bill_id','Date']], on=['bill_id','Date'], how='inner')\n",
    "vv_floor_major = vv_floor[vv_floor['party'].isin(['D','R'])]\n",
    "bill_party_rates = vv_floor_major.groupby(['bill_id','party'])['yes'].mean().unstack('party').reset_index().rename(columns={'D':'yes_D_last','R':'yes_R_last'})\n",
    "bill_party_rates[['yes_D_last','yes_R_last']] = bill_party_rates[['yes_D_last','yes_R_last']].astype(float)\n",
    "\n",
    "closest_vote = summary_roll_with_bill.copy()\n",
    "closest_vote['diff'] = (closest_vote['yes'] - closest_vote['no']).abs()\n",
    "closest_vote = closest_vote.sort_values(['bill_id','diff'])\n",
    "closest_pick = closest_vote.groupby('bill_id').head(1)[['bill_id','yes','no','total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2045080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_31820/1283415855.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mean_yes_ratio_versions = roll.groupby('bill_ID').apply(lambda g: float(np.nanmean((g['yes']/g['total'].replace(0,np.nan)).values)) if len(g)>0 else np.nan).reset_index().rename(columns={0:'mean_yes_ratio_versions'})\n"
     ]
    }
   ],
   "source": [
    "def _entropy_row(r):\n",
    "    y = float(r['yes']); n = float(r['no']); t = float(r['total'])\n",
    "    if t<=0: return 0.0\n",
    "    a = max(t - y - n, 0.0)\n",
    "    p = np.array([y,t - y - n,n], dtype=np.float64)/t\n",
    "    p = p[p>0]\n",
    "    return float(-(p*np.log(p)).sum())\n",
    "closest_pick['controversiality'] = 1 - (closest_pick['yes'] - closest_pick['no']).abs()/closest_pick['total'].replace(0,np.nan)\n",
    "closest_pick['vote_entropy'] = closest_pick.apply(_entropy_row, axis=1)\n",
    "\n",
    "mean_yes_ratio_versions = roll.groupby('bill_ID').apply(lambda g: float(np.nanmean((g['yes']/g['total'].replace(0,np.nan)).values)) if len(g)>0 else np.nan).reset_index().rename(columns={0:'mean_yes_ratio_versions'})\n",
    "\n",
    "bill_term = first_last[['bill_ID','First_action']].copy()\n",
    "bill_term['term'] = _safe_dt(bill_term['First_action']).apply(_term_from_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12f9ddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_31820/240316660.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bt['primary_authors'] = bt.groupby('bill_ID', group_keys=False).apply(_primary_authors).reset_index(level=0, drop=True)\n"
     ]
    }
   ],
   "source": [
    "bt = bills_table[['bill_ID', 'Last_action']].drop_duplicates()\n",
    "bt['Last_action'] = pd.to_datetime(bt['Last_action'])\n",
    "bt = bt.merge(history, left_on=['bill_ID', 'Last_action'], right_on=['bill_ID', 'Date'], how='left')\n",
    "bt = bt.merge(authors[['bill_id', 'Contribution', 'Name']], on='bill_id', how='left')\n",
    "\n",
    "author_type_map = {\n",
    "    'LEAD_AUTHOR': 'LEAD_AUTHOR',\n",
    "    'PRINCIPAL_COAUTHOR': 'PRINCIPAL_COAUTHOR',\n",
    "    'COAUTHOR': 'COAUTHOR',\n",
    "    'data=\"COAUTHOR\"': 'COAUTHOR',\n",
    "    'data=\"LEAD_AUTHOR\"': 'LEAD_AUTHOR',\n",
    "    'data=\"PRINCIPAL_COAUTHOR\"': 'PRINCIPAL_COAUTHOR',\n",
    "    'nan': 'AUTHOR'\n",
    "}\n",
    "author_levels = {\n",
    "    'AUTHOR': 1,\n",
    "    'COAUTHOR': 1,\n",
    "    'PRINCIPAL_COAUTHOR': 2,\n",
    "    'LEAD_AUTHOR': 3\n",
    "}\n",
    "\n",
    "bt['author_type'] = bt['Contribution'].map(author_type_map).fillna('AUTHOR')\n",
    "bt['author_level'] = bt['author_type'].map(author_levels).fillna(0).astype(int)\n",
    "\n",
    "def _primary_authors(g):\n",
    "    g = g.sort_values('author_level', ascending=False)\n",
    "    primary = g[g['author_level'] == g['author_level'].max()]\n",
    "    return list(set(primary['Name'].tolist()))\n",
    "\n",
    "bt['primary_authors'] = bt.groupby('bill_ID', group_keys=False).apply(_primary_authors).reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75d9d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_author_df = bt[['bill_ID', 'primary_authors']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "98602fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_insights = pd.DataFrame({'bill_ID': list(bill_labels.keys())})\n",
    "bill_insights = bill_insights.merge(pd.DataFrame({'bill_ID': list(version_id_mapping2.keys()), 'bill_id_raw': [version_id_mapping2[k][0] if len(version_id_mapping2[k])>0 else np.nan for k in version_id_mapping2.keys()]}), on='bill_ID', how='left')\n",
    "bill_insights['topic'] = bill_insights['bill_ID'].map(bill_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dbc3e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_floor['bill_ID'] = last_floor['bill_id']\n",
    "closest_pick['bill_ID'] = closest_pick['bill_id']\n",
    "bill_party_rates['bill_ID'] = bill_party_rates['bill_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a8da7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_insights = bill_insights.merge(lead_author_df, on='bill_ID', how='left').merge(mean_yes_ratio_versions, on='bill_ID', how='left').merge(bill_party_rates, on='bill_ID', how='left').merge(last_floor[['bill_ID','yes','no','total']], on='bill_ID', how='left').merge(closest_pick[['bill_ID','controversiality','vote_entropy']], on='bill_ID', how='left').merge(bill_term[['bill_ID','term']], on='bill_ID', how='left')\n",
    "bill_insights['bill_polarization'] = (bill_insights['yes_D_last'] - bill_insights['yes_R_last']).abs()\n",
    "bill_insights['bill_party_line'] = np.where(((bill_insights['yes_D_last']>0.5) & (bill_insights['yes_R_last']<0.5)) | ((bill_insights['yes_D_last']<0.5) & (bill_insights['yes_R_last']>0.5)), 1, 0)\n",
    "bill_insights = bill_insights.rename(columns={'controversiality':'bill_controversiality','vote_entropy':'bill_vote_entropy'})\n",
    "bill_insights = bill_insights[['bill_ID','bill_id_raw','topic','term','primary_authors','mean_yes_ratio_versions','bill_polarization','bill_party_line','bill_controversiality','bill_vote_entropy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ddd8fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_partisanship_summary = (bill_insights.groupby(['topic','term']).agg(mean_polarization=('bill_polarization','mean'), party_line_share=('bill_party_line','mean'), controversiality_index=('bill_controversiality','median'), vote_entropy=('bill_vote_entropy','median'), n_bills=('bill_ID','nunique')).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4f3720d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_with_term = route_df.merge(first_last[['bill_ID','First_action']], on='bill_ID', how='left')\n",
    "route_with_term['term'] = _safe_dt(route_with_term['First_action']).apply(_term_from_date)\n",
    "route_entropy = route_with_term.groupby(['topic','term']).agg(route_entropy=('route_key', lambda s: float(-np.sum(pd.Series(s).value_counts(normalize=True).apply(lambda p: p*np.log(p))))) if len(route_with_term)>0 else np.nan, n_routes=('route_key','nunique')).reset_index()\n",
    "\n",
    "def _betweenness_rows(r):\n",
    "    if not isinstance(r, tuple) or len(r)<3:\n",
    "        return []\n",
    "    return list(r[1:-1])\n",
    "mid_rows = []\n",
    "tmp = route_with_term[['bill_ID','topic','term','route']].dropna()\n",
    "for _, row in tmp.iterrows():\n",
    "    mids = _betweenness_rows(row['route'])\n",
    "    for c in mids:\n",
    "        mid_rows.append((c,row['topic'],row['term'],row['bill_ID']))\n",
    "mid_df = pd.DataFrame(mid_rows, columns=['committee','topic','term','bill_ID'])\n",
    "if mid_df.empty:\n",
    "    committee_betweenness_proxy = pd.DataFrame(columns=['committee','topic','term','proxy'])\n",
    "else:\n",
    "    counts = mid_df.groupby(['committee','topic','term'])['bill_ID'].nunique().reset_index(name='mid_count')\n",
    "    totals = route_with_term.groupby(['topic','term'])['bill_ID'].nunique().reset_index(name='n_bills')\n",
    "    committee_betweenness_proxy = counts.merge(totals, on=['topic','term'], how='left')\n",
    "    committee_betweenness_proxy['proxy'] = committee_betweenness_proxy['mid_count']/committee_betweenness_proxy['n_bills'].replace(0,np.nan)\n",
    "    committee_betweenness_proxy = committee_betweenness_proxy[['committee','topic','term','proxy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb55ef11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "bill_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "voting_place",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "2e28ccc5-6866-4e70-be02-4821405f508e",
       "rows": [
        [
         "0",
         "2002-01-24 00:00:00",
         "200120020AB323",
         "appropriations"
        ],
        [
         "21",
         "2002-08-21 00:00:00",
         "200120020SB902",
         "budget"
        ],
        [
         "101",
         "2001-05-29 00:00:00",
         "200120020ACR6",
         "transportation"
        ],
        [
         "140",
         "2001-05-03 00:00:00",
         "200120020ACR6",
         "budget"
        ],
        [
         "219",
         "2001-09-13 00:00:00",
         "200120020SB273",
         "transportation"
        ],
        [
         "259",
         "2001-09-10 00:00:00",
         "200120020SB273",
         "budget"
        ],
        [
         "339",
         "2002-04-01 00:00:00",
         "200120020AB2156",
         "natural resources"
        ],
        [
         "351",
         "2002-06-20 00:00:00",
         "200120020AB1765",
         "budget"
        ],
        [
         "431",
         "2002-06-17 00:00:00",
         "200120020AB1765",
         "appropriations"
        ],
        [
         "462",
         "2001-09-13 00:00:00",
         "200120020AB524",
         "transportation"
        ],
        [
         "542",
         "2002-08-12 00:00:00",
         "200120020AB2582",
         "transportation"
        ],
        [
         "574",
         "2002-05-13 00:00:00",
         "200120020AB2582",
         "budget"
        ],
        [
         "654",
         "2002-08-25 00:00:00",
         "200120020SB1712",
         "budget"
        ],
        [
         "734",
         "2002-05-28 00:00:00",
         "200120020SB1712",
         "transportation"
        ],
        [
         "769",
         "2002-08-08 00:00:00",
         "200120020SB1773",
         "transportation"
        ],
        [
         "804",
         "2002-08-01 00:00:00",
         "200120020SB1773",
         "budget"
        ],
        [
         "884",
         "2002-08-08 00:00:00",
         "200120020AJR47",
         "transportation"
        ],
        [
         "914",
         "2002-06-17 00:00:00",
         "200120020AJR47",
         "budget"
        ],
        [
         "994",
         "2002-08-31 00:00:00",
         "200120020AB2214",
         "budget"
        ],
        [
         "1074",
         "2002-08-31 00:00:00",
         "200120020AJR45",
         "budget"
        ],
        [
         "1154",
         "2002-08-22 00:00:00",
         "200120020AB1068",
         "budget"
        ],
        [
         "1234",
         "2002-08-19 00:00:00",
         "200120020AB1068",
         "transportation"
        ],
        [
         "1273",
         "2002-08-19 00:00:00",
         "200120020SB1774",
         "budget"
        ],
        [
         "1353",
         "2002-05-08 00:00:00",
         "200120020SB1774",
         "transportation"
        ],
        [
         "1392",
         "2001-05-25 00:00:00",
         "200120020AB440",
         "budget"
        ],
        [
         "1472",
         "2001-07-02 00:00:00",
         "200120020SB428",
         "budget"
        ],
        [
         "1552",
         "2001-05-21 00:00:00",
         "200120020SB428",
         "transportation"
        ],
        [
         "1591",
         "2001-09-06 00:00:00",
         "200120020SB575",
         "appropriations"
        ],
        [
         "1612",
         "2001-08-30 00:00:00",
         "200120020AB701",
         "budget"
        ],
        [
         "1692",
         "2001-08-27 00:00:00",
         "200120020AB701",
         "transportation"
        ],
        [
         "1732",
         "2001-09-14 00:00:00",
         "200120020AB1389",
         "budget"
        ],
        [
         "1812",
         "2001-09-14 00:00:00",
         "200120020AB1389",
         "transportation"
        ],
        [
         "1960",
         "2002-08-20 00:00:00",
         "200120020AB2931",
         "budget"
        ],
        [
         "2040",
         "2002-08-14 00:00:00",
         "200120020AB2931",
         "transportation"
        ],
        [
         "2079",
         "2001-09-06 00:00:00",
         "200120020SB769",
         "transportation"
        ],
        [
         "2111",
         "2001-09-04 00:00:00",
         "200120020SB769",
         "budget"
        ],
        [
         "2191",
         "2002-08-12 00:00:00",
         "200120020AB2509",
         "budget"
        ],
        [
         "2271",
         "2002-08-08 00:00:00",
         "200120020AB2509",
         "transportation"
        ],
        [
         "2305",
         "2002-08-20 00:00:00",
         "200120020SB1766",
         "budget"
        ],
        [
         "2385",
         "2002-08-29 00:00:00",
         "200120020AB1945",
         "budget"
        ],
        [
         "2465",
         "2002-09-01 00:00:00",
         "200120020AB444",
         "budget"
        ],
        [
         "2545",
         "2002-06-29 00:00:00",
         "200120020AB444",
         "transportation"
        ],
        [
         "2585",
         "2001-03-19 00:00:00",
         "200120020SCR8",
         "transportation"
        ],
        [
         "2617",
         "2001-09-06 00:00:00",
         "200120020SB432",
         "transportation"
        ],
        [
         "2657",
         "2001-08-31 00:00:00",
         "200120020SB432",
         "budget"
        ],
        [
         "2737",
         "2001-09-13 00:00:00",
         "200120020SB257",
         "transportation"
        ],
        [
         "2773",
         "2001-09-12 00:00:00",
         "200120020SB257",
         "budget"
        ],
        [
         "2853",
         "2002-08-31 00:00:00",
         "200120020SB807",
         "insurance"
        ],
        [
         "2858",
         "2002-08-30 00:00:00",
         "200120020SB807",
         "budget"
        ],
        [
         "2938",
         "2001-09-06 00:00:00",
         "200120020SB1007",
         "transportation"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 60742
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>bill_ID</th>\n",
       "      <th>voting_place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-01-24</td>\n",
       "      <td>200120020AB323</td>\n",
       "      <td>appropriations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2002-08-21</td>\n",
       "      <td>200120020SB902</td>\n",
       "      <td>budget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2001-05-29</td>\n",
       "      <td>200120020ACR6</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2001-05-03</td>\n",
       "      <td>200120020ACR6</td>\n",
       "      <td>budget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>2001-09-13</td>\n",
       "      <td>200120020SB273</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5962452</th>\n",
       "      <td>2018-05-25</td>\n",
       "      <td>201720180AB2184</td>\n",
       "      <td>appropriations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5962829</th>\n",
       "      <td>2018-08-23</td>\n",
       "      <td>201720180AB2143</td>\n",
       "      <td>rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5962909</th>\n",
       "      <td>2018-08-20</td>\n",
       "      <td>201720180AB2143</td>\n",
       "      <td>budget and fiscal review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963071</th>\n",
       "      <td>2017-09-11</td>\n",
       "      <td>201720180AB1264</td>\n",
       "      <td>budget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963150</th>\n",
       "      <td>2017-09-07</td>\n",
       "      <td>201720180AB1264</td>\n",
       "      <td>budget and fiscal review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60742 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date          bill_ID              voting_place\n",
       "0       2002-01-24   200120020AB323            appropriations\n",
       "21      2002-08-21   200120020SB902                    budget\n",
       "101     2001-05-29    200120020ACR6            transportation\n",
       "140     2001-05-03    200120020ACR6                    budget\n",
       "219     2001-09-13   200120020SB273            transportation\n",
       "...            ...              ...                       ...\n",
       "5962452 2018-05-25  201720180AB2184            appropriations\n",
       "5962829 2018-08-23  201720180AB2143                     rules\n",
       "5962909 2018-08-20  201720180AB2143  budget and fiscal review\n",
       "5963071 2017-09-11  201720180AB1264                    budget\n",
       "5963150 2017-09-07  201720180AB1264  budget and fiscal review\n",
       "\n",
       "[60742 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting[['Date', 'bill_ID', 'voting_place']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2d398c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sponsors_min = sponsors[['bill_id','Name']].dropna().copy()\n",
    "sponsors_min['bill_ID'] = sponsors_min['bill_id'].map(bv2b)\n",
    "coal = sponsors_min.merge(route_with_term[['bill_ID','route']], on='bill_ID', how='left').dropna(subset=['route'])\n",
    "coal['committees'] = coal['route'].apply(lambda r: list(dict.fromkeys([x for x in list(r) if isinstance(x,str)])))\n",
    "coal = coal.explode('committees')\n",
    "auth_comms = coal.groupby(['Name','bill_ID'])['committees'].nunique().reset_index()\n",
    "auth_span = auth_comms.groupby('Name').agg(bills=('bill_ID','nunique'), committees=('committees','sum')).reset_index()\n",
    "auth_span['breadth_norm'] = np.where(auth_span['bills']>0, auth_span['committees']/auth_span['bills'], np.nan)\n",
    "author_coalition_breadth = auth_span[['Name','breadth_norm']].rename(columns={'Name':'author'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "24abe8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = voting[['Date', 'bill_ID', 'voting_place', 'chamber']].drop_duplicates().rename(columns={'bill_ID': 'bill_id'})\n",
    "votes['committee_clean'] = votes['chamber'] + ' ' + votes['voting_place']\n",
    "hear_month = hear.copy().merge(votes, on=['bill_id', 'committee_clean'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d86b4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "hear_month['month'] = pd.to_datetime(hear_month['Date']).dt.to_period('M').astype(str)\n",
    "hear_month = hear_month.dropna(subset=['committee_clean','month'])\n",
    "\n",
    "mload = hear_month.groupby(['committee_clean','month'])['bill_id'].nunique().reset_index(name='hearings')\n",
    "committee_workload_median = mload.groupby('committee_clean')['hearings'].median().reset_index().rename(columns={'committee_clean':'committee','hearings':'median_monthly_hearings'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4d878fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name in ['pipeline_stage_funnel','pipeline_timestamps_wide','pipeline_stuck_candidates','route_archetypes','amendment_churn','risk_list','committee_gatekeeping','committee_workload_median','cross_chamber_friction','survival_curves','vote_similarity_edges','vote_communities','committee_floor_drift','text_lift_top_tokens','donor_portfolios_hhi','money_vote_alignment','money_event_time_curve','ca_legislator_funding','bills_table','topic_controversy','rollcall_party_splits','topic_funding_by_term','topic_funding_by_leg','donor_topic_by_term','topic_momentum','topic_funnel_obs','topic_funnel_modeled','route_baseline','bill_insights','topic_partisanship_summary','route_entropy','committee_betweenness_proxy','author_coalition_breadth']:\n",
    "    if df_name in locals():\n",
    "        df = locals()[df_name]\n",
    "        if isinstance(df, pd.DataFrame) and 'term' in df.columns:\n",
    "            locals()[df_name]['term'] = locals()[df_name]['term'].astype(str)\n",
    "\n",
    "precomp_outputs = {\n",
    "    'pipeline_stage_funnel': pipeline_stage_funnel,\n",
    "    'pipeline_timestamps_wide': pipeline_timestamps_wide,\n",
    "    'pipeline_stuck_candidates': pipeline_stuck_candidates,\n",
    "    'route_archetypes': route_archetypes,\n",
    "    'route_uplift_baseline': route_baseline,\n",
    "    'amendment_churn': amendment_churn,\n",
    "    'risk_list': risk_list,\n",
    "    'committee_gatekeeping': committee_gatekeeping,\n",
    "    'committee_workload_median': committee_workload_median,\n",
    "    'cross_chamber_friction': cross_chamber_friction,\n",
    "    'survival_curves': survival_curves,\n",
    "    'vote_similarity_edges': vote_similarity_edges,\n",
    "    'vote_communities': vote_communities,\n",
    "    'committee_floor_drift': committee_floor_drift,\n",
    "    'text_lift_top_tokens': text_lift_top_tokens,\n",
    "    'donor_portfolios_hhi': donor_portfolios_hhi,\n",
    "    'money_vote_alignment': money_vote_alignment,\n",
    "    'money_event_time_curve': money_event_time_curve,\n",
    "    'ca_legislator_funding_geo': ca_legislator_funding,\n",
    "    'ca_legislator_funding': reg_funds_,\n",
    "    'bills_table': bills_table,\n",
    "    'topic_controversy': topic_controversy,\n",
    "    'rollcall_party_splits': rollcall_party_splits,\n",
    "    'topic_funding_by_term': topic_funding_by_term,\n",
    "    'topic_funding_by_leg': topic_funding_by_leg,\n",
    "    'donor_topic_by_term': donor_topic_by_term,\n",
    "    'topic_momentum': topic_momentum,\n",
    "    'topic_funnel_obs': topic_funnel_obs,\n",
    "    'topic_funnel_modeled': topic_funnel_modeled,\n",
    "    'bill_insights': bill_insights,\n",
    "    'topic_partisanship_summary': topic_partisanship_summary,\n",
    "    'route_entropy': route_entropy,\n",
    "    'committee_betweenness_proxy': committee_betweenness_proxy,\n",
    "    'author_coalition_breadth': author_coalition_breadth\n",
    "}\n",
    "\n",
    "model_outputs = {\n",
    "    'per_bill': per_bill,\n",
    "    'actor_topic': actor_topic,\n",
    "    'actor_overall': actor_overall\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "060ab955",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [precomp_outputs, model_outputs]:\n",
    "    for k, v in d.items():\n",
    "        v.to_parquet(f'outs/{k}.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007907f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
