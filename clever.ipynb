{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import LEConv, GATv2Conv, GCNConv, HGTConv, LEConv, GENConv, SAGPooling, HeteroConv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time, pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('gnn_clean.pt', weights_only=False)\n",
    "metadata = (data.node_types, data.edge_types)\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  bill_version={\n",
       "    x=[142952, 389],\n",
       "    num_nodes=142952,\n",
       "  },\n",
       "  bill={\n",
       "    x=[43937, 769],\n",
       "    num_nodes=43937,\n",
       "  },\n",
       "  legislator={\n",
       "    x=[508, 385],\n",
       "    num_nodes=508,\n",
       "  },\n",
       "  legislator_term={\n",
       "    x=[1448, 3],\n",
       "    num_nodes=1448,\n",
       "  },\n",
       "  committee={\n",
       "    x=[1707, 385],\n",
       "    num_nodes=1707,\n",
       "  },\n",
       "  lobby_firm={\n",
       "    x=[1206, 384],\n",
       "    num_nodes=1206,\n",
       "  },\n",
       "  donor={\n",
       "    x=[429, 384],\n",
       "    num_nodes=429,\n",
       "  },\n",
       "  (bill_version, Version, bill)={\n",
       "    edge_index=[2, 146100],\n",
       "    edge_attr={ order=[146100, 1] },\n",
       "  },\n",
       "  (bill_version, nextVersion, bill_version)={\n",
       "    edge_index=[2, 102093],\n",
       "    edge_attr={},\n",
       "  },\n",
       "  (legislator_term, samePerson, legislator)={\n",
       "    edge_index=[2, 1448],\n",
       "    edge_attr={},\n",
       "  },\n",
       "  (committee, member, legislator_term)={\n",
       "    edge_index=[2, 17633],\n",
       "    edge_attr={ position=[17633, 1] },\n",
       "  },\n",
       "  (legislator_term, lobbying, lobby_firm)={\n",
       "    edge_index=[2, 65280],\n",
       "    edge_attr={\n",
       "      amount=[65280, 1],\n",
       "      date=[65280, 1],\n",
       "      expn_dscr=[65127, 384],\n",
       "    },\n",
       "  },\n",
       "  (committee, lobbying, lobby_firm)={\n",
       "    edge_index=[2, 3419],\n",
       "    edge_attr={\n",
       "      amount=[3419, 1],\n",
       "      date=[3419, 1],\n",
       "      expn_dscr=[3408, 384],\n",
       "    },\n",
       "  },\n",
       "  (legislator_term, campaigncontribution, donor)={\n",
       "    edge_index=[2, 7028],\n",
       "    edge_attr={\n",
       "      amount=[7028, 1],\n",
       "      date=[7028, 1],\n",
       "    },\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input_dims.pkl', 'rb') as f:\n",
    "    input_dims = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    def date_to_float(t):\n",
    "        if t.dtype == torch.float32:\n",
    "            return t\n",
    "        return (t.float() / 86_400)\n",
    "\n",
    "    for rel in data.edge_types:\n",
    "        attrs = data[rel].edge_attr\n",
    "        if attrs is None:\n",
    "            continue\n",
    "        for k,v in attrs.items():\n",
    "            if k == \"date\":\n",
    "                attrs[k] = date_to_float(v)\n",
    "\n",
    "    scalers = {}\n",
    "    for rel in data.edge_types:\n",
    "        attrs = data[rel].edge_attr\n",
    "        if attrs is None:\n",
    "            continue\n",
    "        num_cols = {k:v for k,v in attrs.items() if v.dim()==2 and v.size(1)==1}\n",
    "        if num_cols:\n",
    "            M = torch.cat(list(num_cols.values()), dim=0).cpu().numpy()\n",
    "            scaler = StandardScaler().fit(M)\n",
    "            scalers[rel] = scaler\n",
    "            for k,v in num_cols.items():\n",
    "                attrs[k] = torch.as_tensor(\n",
    "                    scaler.transform(v.cpu().numpy()), dtype=torch.float32)\n",
    "    return scalers\n",
    "scalers = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeAttrEncoder(nn.Module):\n",
    "    def __init__(self, attr_dict_sample: dict[str, torch.Tensor],\n",
    "                 bottleneck=128, out_dim=256):\n",
    "        super().__init__()\n",
    "        mods = []\n",
    "        for k, v in attr_dict_sample.items():\n",
    "            if v.dim() == 2 and v.size(1) == 1:\n",
    "                mods.append((k, nn.Linear(1, bottleneck, bias=False)))\n",
    "            elif v.size(1) == 384:\n",
    "                mods.append((k, nn.Linear(384, bottleneck, bias=False)))\n",
    "            else:\n",
    "                continue\n",
    "        self.feat_proj = nn.ModuleDict(mods)\n",
    "        self.mlp       = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(len(mods)*bottleneck, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, attr_dict: dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        if not attr_dict:\n",
    "            raise ValueError(\"EdgeAttrEncoder got empty attr_dict\")\n",
    "\n",
    "        max_len = max(v.size(0) for v in attr_dict.values())\n",
    "        device  = next(self.parameters()).device\n",
    "\n",
    "        feats = []\n",
    "        for k, proj in self.feat_proj.items():\n",
    "            col = attr_dict[k].to(device)\n",
    "            if col.size(0) < max_len:\n",
    "                pad_rows = max_len - col.size(0)\n",
    "                col = F.pad(col, (0, 0, 0, pad_rows))\n",
    "            feats.append(proj(col))\n",
    "\n",
    "        return self.mlp(torch.cat(feats, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_conv_dict = {\n",
    "    \"LEConv\": lambda h, md=None: LEConv(h, h),\n",
    "    \"GATv2Conv\": lambda h, md=None: GATv2Conv(\n",
    "        (-1, -1), h // 4, heads=4, add_self_loops=False),\n",
    "    \"GCNConv\": lambda h, md=None: GCNConv((-1, -1), h, add_self_loops=False),\n",
    "    \"HGTConv\": lambda h, md: HGTConv(\n",
    "        in_channels=h, out_channels=h, metadata=md),\n",
    "    \"GENConv\": lambda h, md=None: GENConv(h, h, aggr='mean',\n",
    "                                              t=1.0, learn_t=True, num_layers=2),\n",
    "}\n",
    "\n",
    "class StackedEncoder(nn.Module):\n",
    "    def __init__(self, metadata, in_dims,\n",
    "                hidden=256,\n",
    "                conv_names=None,\n",
    "                pool_ratio=0.3,\n",
    "                add_pool=False):\n",
    "        super().__init__()\n",
    "        self.lin_in = nn.ModuleDict({\n",
    "            n: nn.Sequential(\n",
    "                nn.Linear(in_dims[n], hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.HeteroLayerNorm(hidden)\n",
    "            ) for n in metadata[0]\n",
    "        })\n",
    "\n",
    "        conv_names = conv_names or [\"GATv2Conv\", \"HGTConv\", \"GENConv\"]\n",
    "        self.convs = nn.ModuleList()\n",
    "\n",
    "        for name in conv_names:\n",
    "            ctor = _conv_dict[name]\n",
    "            self.convs.append(\n",
    "                HeteroConv({rel: ctor(hidden, metadata) if name == \"HGTConv\"\n",
    "                          else ctor(hidden)\n",
    "                          for rel in metadata[1]},\n",
    "                         aggr=\"mean\")\n",
    "            )\n",
    "\n",
    "        self.use_pool = add_pool\n",
    "        if add_pool:\n",
    "            self.pool = SAGPooling(hidden, ratio=pool_ratio)\n",
    "\n",
    "        self.bn_pool = nn.BatchNorm1d(hidden)\n",
    "        self.hidden = hidden\n",
    "\n",
    "        self.layer_norms = nn.ModuleDict({\n",
    "            nt: nn.HeteroLayerNorm(hidden) for nt in metadata[0]\n",
    "        })\n",
    "\n",
    "    def _merge(self, prev, new):\n",
    "        out = {}\n",
    "        for nt in prev.keys() | new.keys():\n",
    "            if nt in prev and nt in new:\n",
    "                out[nt] = F.relu(prev[nt] + new[nt])\n",
    "                out[nt] = self.layer_norms[nt](out[nt])\n",
    "            else:\n",
    "                out[nt] = F.relu(new.get(nt, prev[nt]))\n",
    "        return out\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        h = {nt: self.lin_in[nt](x) for nt, x in x_dict.items()}\n",
    "        for conv in self.convs:\n",
    "            h = self._merge(h, conv(h, edge_index_dict))\n",
    "\n",
    "        if self.use_pool:\n",
    "            bv = h['bill_version']\n",
    "            bill_edges = None\n",
    "            edge_key = ('bill_version', 'nextVersion', 'bill_version')\n",
    "            if edge_key in edge_index_dict:\n",
    "                bill_edges = edge_index_dict[edge_key]\n",
    "\n",
    "            if bill_edges is None:\n",
    "                idx = torch.arange(bv.size(0), device=bv.device)\n",
    "                bill_edges = torch.stack([idx, idx])\n",
    "\n",
    "            bv, *rest = self.pool(bv, bill_edges)\n",
    "            h['bill_version'] = self.bn_pool(bv)\n",
    "\n",
    "        for nt in h:\n",
    "            h[nt] = F.normalize(h[nt], p=2, dim=1)\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkRecon(nn.Module):\n",
    "    def __init__(self, edge_attr_encoders: nn.ModuleDict, weight=1.0):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.encoders = edge_attr_encoders\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, z, data, num_neg=1):\n",
    "        loss = torch.tensor(0., device=z[next(iter(z))].device)\n",
    "        for rel, ei in data.edge_index_dict.items():\n",
    "            s_t, _, d_t = rel\n",
    "            src, dst = ei\n",
    "            pos_log = (z[s_t][src] * z[d_t][dst]).sum(-1)\n",
    "            pos_loss = self.bce(pos_log, torch.ones_like(pos_log))\n",
    "            attr_dict = data[rel].edge_attr\n",
    "            if str(rel) in self.encoders and attr_dict and len(attr_dict) > 0:\n",
    "                enc = self.encoders[str(rel)]\n",
    "                tgt = enc(attr_dict)\n",
    "                proj = nn.Linear(z[s_t].size(1), tgt.size(1), bias=False, device=z[s_t].device)\n",
    "                pred = proj(z[s_t][src] * z[d_t][dst])\n",
    "                pos_loss = pos_loss + self.mse(pred, tgt)\n",
    "\n",
    "            neg_dst = torch.randint(0, data[d_t].num_nodes, (src.size(0)*num_neg,), device=src.device)\n",
    "            neg_src = src.repeat(num_neg)\n",
    "            neg_log = (z[s_t][neg_src] * z[d_t][neg_dst]).sum(-1)\n",
    "            neg_loss = self.bce(neg_log, torch.zeros_like(neg_log))\n",
    "            loss = loss + pos_loss + neg_loss\n",
    "        return loss * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextVersionContrastive(nn.Module):\n",
    "    def __init__(self, margin=0.25, weight=0.25):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, z, data):\n",
    "        rel = ('bill_version', 'nextVersion', 'bill_version')\n",
    "        if rel not in data.edge_index_dict:\n",
    "            return torch.tensor(0., device=z[next(iter(z))].device)\n",
    "        src, dst = data.edge_index_dict[rel]\n",
    "        pos_dist = F.pairwise_distance(z['bill_version'][src], z['bill_version'][dst])\n",
    "\n",
    "        neg_dst  = dst[torch.randperm(dst.size(0))]\n",
    "        neg_dist = F.pairwise_distance(z['bill_version'][src], z['bill_version'][neg_dst])\n",
    "\n",
    "        zeros = torch.zeros_like(pos_dist)\n",
    "        loss  = torch.mean(torch.maximum(\n",
    "            zeros, self.margin + pos_dist - neg_dist))\n",
    "        return loss * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StakeholderContrastive(nn.Module):\n",
    "    def __init__(self, temperature=0.2, weight=0.5):\n",
    "        super().__init__()\n",
    "        self.temp = temperature\n",
    "        self.weight = weight\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, z, data, sample=1000):\n",
    "        rel = ('lobby_firm', 'Lobbying', 'legislator_term')\n",
    "        if rel not in data.edge_index_dict:\n",
    "            return torch.tensor(0., device=z[next(iter(z))].device)\n",
    "        src, dst = data.edge_index_dict[rel]\n",
    "        if src.size(0) > sample:\n",
    "            idx = torch.randperm(src.size(0))[:sample]\n",
    "            src, dst = src[idx], dst[idx]\n",
    "\n",
    "        h_dst = z['legislator_term'][dst]\n",
    "        h_src = z['lobby_firm'][src]\n",
    "        logits = (h_src @ h_dst.T) / self.temp\n",
    "        labels = torch.arange(src.size(0), device=device)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphStructureLearning(nn.Module):\n",
    "    def __init__(self, weight=0.4):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, z, data):\n",
    "        loss = torch.tensor(0., device=z[next(iter(z))].device)\n",
    "\n",
    "        key_relations = [\n",
    "            ('bill_version', 'nextVersion', 'bill_version'),\n",
    "            ('bill', 'Version',  'bill_version'),\n",
    "            ('lobby_firm', 'Lobbying', 'legislator_term'),\n",
    "            ('donor', 'CampaignContribution', 'legislator_term'),\n",
    "            ('legislator_term', 'Member', 'committee'),\n",
    "            ('legislator_term', 'Author', 'bill_version')\n",
    "        ]\n",
    "\n",
    "        for rel in key_relations:\n",
    "            if rel not in data.edge_index_dict:\n",
    "                continue\n",
    "\n",
    "            src_type, edge_type, dst_type = rel\n",
    "            src, dst = data.edge_index_dict[rel]\n",
    "            pos_score = (z[src_type][src] * z[dst_type][dst]).sum(dim=1)\n",
    "            neg_dst = dst[torch.randperm(dst.size(0))]\n",
    "            neg_score = (z[src_type][src] * z[dst_type][neg_dst]).sum(dim=1)\n",
    "\n",
    "            loss += self.criterion(pos_score, torch.ones_like(pos_score))\n",
    "            loss += self.criterion(neg_score, torch.zeros_like(neg_score))\n",
    "\n",
    "        return loss * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emeliasprott/miniconda3/envs/ml/lib/python3.12/site-packages/torch_geometric/nn/conv/hetero_conv.py:76: UserWarning: There exist node types ({'committee'}) whose representations do not get updated during message passing as they do not occur as destination type in any edge type. This may lead to unexpected behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def run_epoch(encoder, tasks, data, opt, device):\n",
    "    encoder.train()\n",
    "    z = encoder(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "    loss_total = torch.tensor(0., device=device)\n",
    "\n",
    "    for task in tasks:\n",
    "        loss = task(z, data)\n",
    "        loss_total += loss\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss_total.backward()\n",
    "    opt.step()\n",
    "    return float(loss_total)\n",
    "\n",
    "encoder = StackedEncoder(\n",
    "    data.metadata(), input_dims,\n",
    "    hidden=256,\n",
    "    conv_names=[\"GATv2Conv\", \"HGTConv\", \"GENConv\"],\n",
    "    pool_ratio=0.3,\n",
    "    add_pool=True\n",
    ").to(device)\n",
    "\n",
    "edge_attr_enc = nn.ModuleDict()\n",
    "for rel in data.edge_types:\n",
    "    attrs = data[rel].edge_attr\n",
    "    if attrs is None:\n",
    "        continue\n",
    "    usable = {k: v for k, v in attrs.items()\n",
    "              if (v.dim() == 2 and (v.size(1) == 1 or v.size(1) == 384)) or (v.dim() == 1 and v.size(0) != 0)}\n",
    "    if usable:\n",
    "        edge_attr_enc[str(rel)] = EdgeAttrEncoder(usable)\n",
    "edge_attr_enc = edge_attr_enc.to(device)\n",
    "\n",
    "\n",
    "tasks = [\n",
    "    LinkRecon(edge_attr_enc, weight=1.0),\n",
    "    StakeholderContrastive(weight=0.25),\n",
    "    NextVersionContrastive(weight=0.5),\n",
    "    GraphStructureLearning(weight=0.4)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    encoder.parameters(),\n",
    "    lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/100] loss=10009.3574  time=16.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 1/100 [00:28<46:41, 28.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/100] loss=nan  time=22.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 2/100 [01:05<54:17, 33.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/100] loss=nan  time=19.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 2/100 [01:27<1:11:32, 43.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m encoder.eval()\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     z = \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m torch.save(z, \u001b[33m\"\u001b[39m\u001b[33mnode_embeddings_2.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mStackedEncoder.forward\u001b[39m\u001b[34m(self, x_dict, edge_index_dict)\u001b[39m\n\u001b[32m     61\u001b[39m h = {nt: \u001b[38;5;28mself\u001b[39m.lin_in[nt](x) \u001b[38;5;28;01mfor\u001b[39;00m nt, x \u001b[38;5;129;01min\u001b[39;00m x_dict.items()}\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convs:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     h = \u001b[38;5;28mself\u001b[39m._merge(h, \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_pool \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mbill_version\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m h:\n\u001b[32m     66\u001b[39m     bv = h[\u001b[33m'\u001b[39m\u001b[33mbill_version\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch_geometric/nn/conv/hetero_conv.py:166\u001b[39m, in \u001b[36mHeteroConv.forward\u001b[39m\u001b[34m(self, *args_dict, **kwargs_dict)\u001b[39m\n\u001b[32m    163\u001b[39m         out_dict[dst].append(out)\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m out_dict.items():\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     out_dict[key] = \u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maggr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch_geometric/nn/conv/hetero_conv.py:24\u001b[39m, in \u001b[36mgroup\u001b[39m\u001b[34m(xs, aggr)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     23\u001b[39m     out = torch.stack(xs, dim=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     out = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     out = out[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "EPOCHS = 100\n",
    "for epoch in tqdm(range(1, EPOCHS+1), desc=\"Epochs\"):\n",
    "    t0   = time.time()\n",
    "    loss = run_epoch(encoder, tasks, data, optimizer, device)\n",
    "    dt   = time.time() - t0\n",
    "    print(f\"[{epoch:02d}/{EPOCHS}] \"\n",
    "            f\"loss={loss:.4f}  time={dt:.1f}s\")\n",
    "    if loss == np.nan or loss == None:\n",
    "        break\n",
    "\n",
    "    encoder.eval()\n",
    "    with torch.inference_mode():\n",
    "        z = encoder(data.x_dict, data.edge_index_dict)\n",
    "    torch.save(z, \"node_embeddings_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_embeddings(z, labels=None, title=\"Embedding Visualization\"):\n",
    "    tsne = TSNE(n_components=2, perplexity=50, random_state=42)\n",
    "    z_2d = tsne.fit_transform(z.cpu().detach().numpy())\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(z_2d[:, 0], z_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "    if labels is not None:\n",
    "        plt.legend(*scatter.legend_elements(), title=\"Labels\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE 1\")\n",
    "    plt.ylabel(\"t-SNE 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load('gnn_data.pt', weights_only=False)\n",
    "visualize_embeddings(z['legislator'], labels=d['legislator'].x['party'].tolist(), title=\"Legislators by Party\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_topics = [\n",
    "    \"Public health\",\n",
    "    \"Mental health services\",\n",
    "    \"Medi-Cal and health insurance\",\n",
    "    \"Substance abuse and harm reduction\",\n",
    "    \"Child welfare and foster care\",\n",
    "    \"Developmental and disability services\",\n",
    "    \"Elder care and long-term support\",\n",
    "\n",
    "    \"K-12 education funding\",\n",
    "    \"Curriculum and instruction\",\n",
    "    \"Higher education and UC/CSU systems\",\n",
    "    \"Community colleges\",\n",
    "    \"School construction and facilities\",\n",
    "    \"Special education\",\n",
    "    \"Charter schools and school choice\",\n",
    "\n",
    "    \"Climate change and carbon reduction\",\n",
    "    \"Air quality and pollution control\",\n",
    "    \"Water supply and drought\",\n",
    "    \"Coastal protection\",\n",
    "    \"Wildfire prevention and forestry\",\n",
    "    \"Environmental justice\",\n",
    "    \"Recycling and waste management\",\n",
    "\n",
    "    \"Criminal justice reform\",\n",
    "    \"Police oversight and accountability\",\n",
    "    \"Firearms and gun control\",\n",
    "    \"Corrections and parole\",\n",
    "    \"Emergency services and disaster response\",\n",
    "    \"Human trafficking prevention\",\n",
    "\n",
    "    \"State budget and fiscal policy\",\n",
    "    \"Personal and corporate income taxes\",\n",
    "    \"Sales and use taxes\",\n",
    "    \"Proposition 13 and property tax\",\n",
    "    \"State bonds and financing\",\n",
    "    \"Local government finance\",\n",
    "\n",
    "    \"Workplace safety and Cal/OSHA\",\n",
    "    \"Paid family leave\",\n",
    "    \"Minimum wage and wage theft\",\n",
    "    \"Public employee unions\",\n",
    "    \"Employment discrimination and DEI\",\n",
    "    \"Workforce development\",\n",
    "\n",
    "    \"Redistricting and electoral reform\",\n",
    "    \"Voter access and registration\",\n",
    "    \"Campaign finance and lobbying\",\n",
    "    \"Open meetings and transparency (Brown Act)\",\n",
    "    \"Public records and data access\",\n",
    "    \"Government agency operations\",\n",
    "\n",
    "    \"Roads and highways (Caltrans)\",\n",
    "    \"Public transit and rail\",\n",
    "    \"High-speed rail\",\n",
    "    \"Ports and logistics\",\n",
    "    \"Vehicle emissions and EV policy\",\n",
    "    \"Infrastructure resilience\",\n",
    "\n",
    "    \"Affordable housing development\",\n",
    "    \"Zoning and local control\",\n",
    "    \"Tenant protections and rent control\",\n",
    "    \"Homelessness and supportive housing\",\n",
    "    \"CEQA and environmental permitting\",\n",
    "    \"Redevelopment and gentrification\",\n",
    "\n",
    "    \"Electric grid and reliability\",\n",
    "    \"Renewable energy incentives\",\n",
    "    \"Utility regulation (CPUC)\",\n",
    "    \"Natural gas and oil regulation\",\n",
    "    \"Wildfire liability (PG&E)\",\n",
    "    \"Broadband and digital equity\",\n",
    "\n",
    "    \"Immigration and sanctuary laws\",\n",
    "    \"LGBTQ+ rights\",\n",
    "    \"Gender equity and reproductive health\",\n",
    "    \"Racial equity and anti-discrimination\",\n",
    "    \"Food insecurity and public benefits\",\n",
    "    \"Language access and cultural inclusion\",\n",
    "\n",
    "    \"Water rights and agriculture\",\n",
    "    \"Pesticide regulation\",\n",
    "    \"Farmworker labor conditions\",\n",
    "    \"Fisheries and marine policy\",\n",
    "    \"Wildlife conservation\",\n",
    "    \"Timber and land management\",\n",
    "\n",
    "    \"Small business support\",\n",
    "    \"Technology and innovation\",\n",
    "    \"Cannabis regulation\",\n",
    "    \"Insurance industry oversight\",\n",
    "    \"Economic stimulus and recovery\",\n",
    "    \"Licensing and regulation (e.g., BAR, ABC)\",\n",
    "\n",
    "    \"Financial services and predatory lending\",\n",
    "    \"Data privacy and cybersecurity\",\n",
    "    \"Product safety and recalls\",\n",
    "    \"Housing scams and fraud\",\n",
    "    \"Telemarketing and spam regulation\",\n",
    "\n",
    "    \"Freedom of speech and assembly\",\n",
    "    \"Facial recognition and surveillance\",\n",
    "    \"Disability rights\",\n",
    "    \"Due process protections\",\n",
    "    \"First Amendment in schools/public spaces\",\n",
    "\n",
    "    \"State-local relations\",\n",
    "    \"Tribal affairs\",\n",
    "    \"Military and veterans issues\",\n",
    "    \"COVID-19 response and recovery\",\n",
    "    \"Technology in government\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', truncate_dim=256)\n",
    "topic_embs = model.encode(california_topics, normalize_embeddings=True, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "z_text_all = torch.cat([\n",
    "    z[0]['bill'],\n",
    "    z[0]['bill_version'],\n",
    "    z[0]['committee'],\n",
    "    z[0]['legislator'],\n",
    "    z[0]['donor'],\n",
    "    z[0]['lobby_firm'],\n",
    "], dim=0)\n",
    "\n",
    "k = 25\n",
    "kmeans = KMeans(n_clusters=k, random_state=40)\n",
    "cluster_ids = kmeans.fit_predict(z_text_all.cpu())\n",
    "score = silhouette_score(z_text_all.cpu(), kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "sims = cosine_similarity(centroids, topic_embs.cpu().numpy())\n",
    "top_topic_ids = sims.argmax(axis=1)\n",
    "cluster_topic_labels = [california_topics[i] for i in top_topic_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'cluster': list(range(k)),\n",
    "    'predicted_topic': cluster_topic_labels,\n",
    "    'most_similar_score': sims.max(axis=1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
