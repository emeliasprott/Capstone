{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, datetime, json, pickle, re\n",
    "from pathlib import Path\n",
    "from torch_geometric.transforms import ToUndirected, RemoveIsolatedNodes\n",
    "from pathlib import Path\n",
    "import torch, numpy as np, pandas as pd\n",
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "\n",
    "DATA_PATH = Path(\"../../../data3.pt\")\n",
    "EXPORT_PATH = Path(\"../../shiny/data\")\n",
    "OUT_PATH = EXPORT_PATH\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "FLOAT = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m lobbying[\u001b[33m'\u001b[39m\u001b[33mexpn_date\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(lobbying[\u001b[33m'\u001b[39m\u001b[33mEXPN_DATE\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      6\u001b[39m lobbying[\u001b[33m'\u001b[39m\u001b[33mterm\u001b[39m\u001b[33m'\u001b[39m] = lobbying[\u001b[33m'\u001b[39m\u001b[33mexpn_date\u001b[39m\u001b[33m'\u001b[39m].dt.year.astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mlobbying\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlobbying\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexpn_date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m.\u001b[49m\u001b[43myear\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2025\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mterm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = [\u001b[32m2022\u001b[39m, \u001b[32m2014\u001b[39m, \u001b[32m2018\u001b[39m, \u001b[32m2018\u001b[39m, \u001b[32m2018\u001b[39m, \u001b[32m2018\u001b[39m, \u001b[32m2018\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/pandas/core/indexing.py:911\u001b[39m, in \u001b[36m_LocationIndexer.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28mself\u001b[39m._has_valid_setitem_indexer(key)\n\u001b[32m    910\u001b[39m iloc = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name == \u001b[33m\"\u001b[39m\u001b[33miloc\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj.iloc\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m \u001b[43miloc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/pandas/core/indexing.py:1942\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer\u001b[39m\u001b[34m(self, indexer, value, name)\u001b[39m\n\u001b[32m   1939\u001b[39m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[32m   1940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[32m   1941\u001b[39m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1942\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1944\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_single_block(indexer, value, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/pandas/core/indexing.py:1998\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer_split_path\u001b[39m\u001b[34m(self, indexer, value, name)\u001b[39m\n\u001b[32m   1993\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_integer(info_axis):\n\u001b[32m   1994\u001b[39m         \u001b[38;5;66;03m# This is a case like df.iloc[:3, [1]] = [0]\u001b[39;00m\n\u001b[32m   1995\u001b[39m         \u001b[38;5;66;03m#  where we treat as df.iloc[:3, 1] = 0\u001b[39;00m\n\u001b[32m   1996\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._setitem_with_indexer((pi, info_axis[\u001b[32m0\u001b[39m]), value[\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m1998\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1999\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMust have equal len keys and value \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2000\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwhen setting with an iterable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2001\u001b[39m     )\n\u001b[32m   2003\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m lplane_indexer == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) == \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj.index):\n\u001b[32m   2004\u001b[39m     \u001b[38;5;66;03m# We get here in one case via .loc with a all-False mask\u001b[39;00m\n\u001b[32m   2005\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Must have equal len keys and value when setting with an iterable"
     ]
    }
   ],
   "source": [
    "politicians = pd.read_csv(OUT_PATH / 'legislator_terms.csv')\n",
    "lobbying = pd.read_csv('../../../calaccess/lobbying_clean2.csv', dtype={'PAYEE_NAMS': str, 'BAKREF_TID': str})\n",
    "expend_assembly = pd.read_csv('../../../calaccess/expend_assembly_matched.csv', dtype={'TargetPropositionName': str})\n",
    "expend_senate = pd.read_csv('../../../calaccess/expend_senate_matched.csv', dtype={'TargetPropositionName': str})\n",
    "lobbying['expn_date'] = pd.to_datetime(lobbying['EXPN_DATE'])\n",
    "lobbying['term'] = lobbying['expn_date'].dt.year.astype(int)\n",
    "lobbying.loc[lobbying['expn_date'].dt.year > 2025, 'term'] = [2022, 2014, 2018, 2018, 2018, 2018, 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobbying.loc[(lobbying['term'].isin([int(r) for r in range(2000, 2026, 2)])) & (lobbying['expn_date'].dt.month < 11), 'term'] = lobbying['term'] - 1\n",
    "lobbying.loc[(lobbying['term'].isin([int(r) for r in range(2000, 2026, 2)])) & (lobbying['expn_date'].dt.month >= 11), 'term'] = lobbying['term'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob = lobbying.groupby(['clean_beneficiary', 'term']).agg({'AMOUNT': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "expend_assembly = expend_assembly.loc[expend_assembly['term_y'].apply(lambda x: isinstance(x, str))]\n",
    "expend_assembly['year'] = expend_assembly['term_y'].apply(lambda x: int(str(x).split('-')[0]))\n",
    "expend_assembly.loc[expend_assembly['year'] // 2 == 0, 'year'] = expend_assembly.loc[expend_assembly['year'] // 2 == 0, 'year'] - 1\n",
    "exp_as = expend_assembly[['Amount', 'year', 'full_name']].drop_duplicates().groupby(['full_name', 'year']).agg({'Amount': 'sum'}).reset_index().rename(columns={'year': 'term'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "expend_senate['year'] = expend_senate['term'].apply(lambda x: int(x.split('-')[0]))\n",
    "expend_senate.loc[expend_senate['year'] // 2 == 0, 'year'] = expend_senate.loc[expend_senate['year'] // 2 == 0, 'year'] - 1\n",
    "exp_sen = expend_senate.groupby(['full_name', 'year']).agg({'Amount': 'sum'}).reset_index().rename(columns={\"year\": 'term'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians['lower'] = politicians['full_name'].str.lower()\n",
    "pl = politicians.merge(lob, left_on=['term', 'lower'], right_on=['term', 'clean_beneficiary'], how='left').rename(columns={'AMOUNT': 'total_lobbying_'})\n",
    "pld = pl.merge(exp_as, on=['term', 'full_name'], how='left').rename(columns={'Amount': 'total_donations_'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pldd = pld.merge(exp_sen, on=['full_name', 'term'], how='left')\n",
    "pldd['total_donations_'] = pldd[['total_donations_', 'Amount']].sum(skipna=True, axis=1)\n",
    "pldd = pldd.drop(columns=['total_donations', 'total_lobbying', 'Amount', 'total_received']).rename(columns={'total_donations_': 'total_donations', 'total_lobbying_': 'total_lobbying'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pldd['total_received'] = pldd['total_donations'] + pldd['total_lobbying']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_73146/2832143459.py:1: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  pldd[['total_donations', 'total_lobbying', 'total_received']] = pldd[['total_donations', 'total_lobbying', 'total_received']].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "pldd[['total_donations', 'total_lobbying', 'total_received']] = pldd[['total_donations', 'total_lobbying', 'total_received']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pldd.to_csv(OUT_PATH / 'politicians2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- identify and map topic labels to all dfs\n",
    "- change column names to fit app\n",
    "- add more data to the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../node_id_map.json', 'r') as f:\n",
    "        node_id_map = json.load(f)\n",
    "\n",
    "with open('../../../bill_labels_updated.json', 'r') as f:\n",
    "    topic_cluster_labels_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(\"../..\")\n",
    "\n",
    "embeddings = torch.load(OUT_DIR / \"node_embeddings.pt\", map_location=DEVICE)\n",
    "preds = torch.load(OUT_DIR / \"predictions.pt\", map_location=DEVICE)\n",
    "\n",
    "\n",
    "bill_logits = preds[\"bill_logits\"].softmax(-1)\n",
    "bill_success_p = preds[\"success_logit\"].sigmoid()\n",
    "actor_align = preds[\"actor_align\"]\n",
    "actor_influence = preds[\"actor_influence\"]\n",
    "K_TOPICS = bill_logits.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_normalize_timestamps(timestamps, eps=1e-8):\n",
    "    timestamps = torch.nan_to_num(timestamps, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "    p5 = torch.quantile(timestamps, 0.05)\n",
    "    p95 = torch.quantile(timestamps, 0.95)\n",
    "\n",
    "    if (p95 - p5) < eps:\n",
    "        return torch.zeros_like(timestamps)\n",
    "\n",
    "    timestamps = torch.clamp(timestamps, p5, p95)\n",
    "    normalized = (timestamps - p5) / (p95 - p5)\n",
    "    return torch.nan_to_num(normalized, nan=0.0)\n",
    "\n",
    "def safe_standardize_time_format(time_data):\n",
    "    times = []\n",
    "    for t in time_data:\n",
    "        try:\n",
    "            if isinstance(t, (int, float)) and 1900 <= t  and t <= 2100:\n",
    "                td = datetime.datetime(int(t), 6, 15).timestamp()\n",
    "            elif (isinstance(t, str) or (isinstance(t, float))) and (float(t) < 2100 and float(t) > 1900):\n",
    "                td = datetime.datetime(int(float(t)), 6, 15).timestamp()\n",
    "            elif float(t) > 0 and float(t) < 1990:\n",
    "                td = t\n",
    "            elif float(t) > 17000000.0:\n",
    "                td = float(t)\n",
    "            elif isinstance(t, datetime.datetime):\n",
    "                td = t.timestamp()\n",
    "            else:\n",
    "                td = float(t) * 1e9\n",
    "        except:\n",
    "            td = datetime.datetime(2000, 6, 15).timestamp()\n",
    "        times.append(td)\n",
    "    return torch.tensor(times, dtype=torch.float32)\n",
    "\n",
    "def pull_timestamps(data):\n",
    "    timestamp_edges = [\n",
    "        ('donor', 'donated_to', 'legislator_term'),\n",
    "        ('legislator_term', 'rev_donated_to', 'donor'),\n",
    "        ('lobby_firm', 'lobbied', 'legislator_term'),\n",
    "        ('lobby_firm', 'lobbied', 'committee'),\n",
    "        ('committee', 'rev_lobbied', 'lobby_firm'),\n",
    "        ('legislator_term', 'rev_lobbied', 'lobby_firm'),\n",
    "        ('bill_version', 'rev_voted_on', 'legislator_term'),\n",
    "        ('legislator_term', 'voted_on', 'bill_version'),\n",
    "    ]\n",
    "    timestamp_nodes = ['legislator_term', 'bill_version', 'bill']\n",
    "\n",
    "    for et in timestamp_edges:\n",
    "        if hasattr(data[et], 'edge_attr') and data[et].edge_attr is not None and len(data[et].edge_attr.size()) > 1:\n",
    "            if data[et].edge_attr.size(1) > 1:\n",
    "                edge_attr = data[et].edge_attr\n",
    "                ts_col = edge_attr[:, -1]\n",
    "                if ts_col.abs().max() > 1e8 or ts_col.min() < 0:\n",
    "                    ts_col = safe_standardize_time_format(ts_col.tolist()).to(edge_attr.device)\n",
    "                data[et].timestamp = safe_normalize_timestamps(ts_col)\n",
    "                data[et].time = data[et].timestamp\n",
    "                data[et].edge_attr = edge_attr[:, :-1]\n",
    "\n",
    "    for nt in timestamp_nodes:\n",
    "        if hasattr(data[nt], 'x') and data[nt].x is not None:\n",
    "            try:\n",
    "                if len(data[nt].x.size()) > 1:\n",
    "                    if data[nt].x.size(1) > 1:\n",
    "                        x = data[nt].x\n",
    "                        ts_col = x[:, -1]\n",
    "                        if ts_col.abs().max() > 1e8 or ts_col.min() < 0:\n",
    "                            ts_col = safe_standardize_time_format(ts_col.tolist()).to(x.device)\n",
    "                        if nt in timestamp_nodes or ts_col.abs().max() > 1e6:\n",
    "                            data[nt].timestamp = safe_normalize_timestamps(ts_col)\n",
    "                            data[nt].time = data[nt].timestamp\n",
    "                            data[nt].x = x[:, :-1]\n",
    "            except:\n",
    "                pass\n",
    "    return data\n",
    "def clean_features(data):\n",
    "    for nt in data.node_types:\n",
    "        x = data[nt].x\n",
    "        x = torch.as_tensor(x, dtype=torch.float32)\n",
    "        x = torch.nan_to_num(x.float(), nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "        mean = x.mean(0, keepdim=True)\n",
    "        std = x.std(0, keepdim=True).clamp(min=1e-5)\n",
    "        x = ((x - mean) / std).clamp(-10, 10)\n",
    "        data[nt].x = x\n",
    "        data[nt].x_mean = mean\n",
    "        data[nt].x_std = std\n",
    "    data = pull_timestamps(data)\n",
    "    return data\n",
    "\n",
    "def compute_controversiality(data):\n",
    "    edge_type = ('legislator_term', 'voted_on', 'bill_version')\n",
    "    if edge_type not in data.edge_index_dict:\n",
    "        raise ValueError(\"Missing 'voted_on' edges in data.\")\n",
    "\n",
    "    ei = data[edge_type].edge_index\n",
    "    ea = data[edge_type].edge_attr\n",
    "\n",
    "    vote_signal = ea[:, 0]\n",
    "\n",
    "    src_nodes = ei[0]\n",
    "    tgt_nodes = ei[1]\n",
    "\n",
    "    num_bills = data['bill_version'].num_nodes\n",
    "    device = tgt_nodes.device\n",
    "\n",
    "    yes_votes = torch.zeros(num_bills, device=device)\n",
    "    no_votes = torch.zeros(num_bills, device=device)\n",
    "\n",
    "    yes_votes.index_add_(0, tgt_nodes, (vote_signal > 0).float())\n",
    "    no_votes.index_add_(0, tgt_nodes, (vote_signal < 0).float())\n",
    "\n",
    "    total_votes = yes_votes + no_votes + 1e-6\n",
    "\n",
    "    yes_ratio = yes_votes / total_votes\n",
    "    no_ratio = no_votes / total_votes\n",
    "\n",
    "    controversy = 4 * yes_ratio * no_ratio\n",
    "    controversy = controversy.clamp(0, 1)\n",
    "    data['bill_version'].controversy = controversy\n",
    "\n",
    "    return data\n",
    "\n",
    "def load_and_preprocess_data(path='../../../data3.pt'):\n",
    "    full_data = torch.load(path, weights_only=False)\n",
    "    for nt in full_data.node_types:\n",
    "        if hasattr(full_data[nt], 'x') and full_data[nt].x is not None:\n",
    "            flat = torch.as_tensor(full_data[nt].x).flatten(start_dim=1)\n",
    "            full_data[nt].x = flat\n",
    "            full_data[nt].num_nodes = flat.size(0)\n",
    "\n",
    "    for edge_type, edge_index in full_data.edge_index_dict.items():\n",
    "        src_type, _, dst_type = edge_type\n",
    "        max_src_idx = edge_index[0].max().item() if edge_index.size(1) > 0 else -1\n",
    "        max_dst_idx = edge_index[1].max().item() if edge_index.size(1) > 0 else -1\n",
    "        if max_src_idx >= full_data[src_type].num_nodes:\n",
    "            print(f\"Fixing {src_type} node count: {full_data[src_type].num_nodes} -> {max_src_idx + 1}\")\n",
    "            full_data[src_type].num_nodes = max_src_idx + 1\n",
    "\n",
    "        if max_dst_idx >= full_data[dst_type].num_nodes:\n",
    "            print(f\"Fixing {dst_type} node count: {full_data[dst_type].num_nodes} -> {max_dst_idx + 1}\")\n",
    "            full_data[dst_type].num_nodes = max_dst_idx + 1\n",
    "    full_data['bill'].y[np.where(full_data['bill'].y < 0)[0]] = 0\n",
    "    full_data['bill'].y = torch.as_tensor(full_data['bill'].y, dtype=torch.float32)\n",
    "\n",
    "    data = ToUndirected(merge=False)(full_data)\n",
    "    del full_data\n",
    "    gc.collect()\n",
    "    data = RemoveIsolatedNodes()(data)\n",
    "    data = compute_controversiality(clean_features(data))\n",
    "\n",
    "    for nt in data.node_types:\n",
    "        ids = torch.arange(data[nt].num_nodes, device='mps')\n",
    "        data[nt].node_id = ids\n",
    "    for store in data.stores:\n",
    "        for key, value in store.items():\n",
    "            if isinstance(value, torch.Tensor) and value.dtype == torch.float64:\n",
    "                store[key] = value.float()\n",
    "\n",
    "    return data\n",
    "\n",
    "data = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bill: 46054 nodes, 769 features\n",
      "bill_version: 198160 nodes, 389 features\n",
      "legislator: 509 nodes, 385 features\n",
      "legislator_term: 1434 nodes, 2 features\n",
      "committee: 1537 nodes, 385 features\n",
      "lobby_firm: 321 nodes, 384 features\n",
      "donor: 234 nodes, 384 features\n"
     ]
    }
   ],
   "source": [
    "for nt in data.node_types:\n",
    "    print(f\"{nt}: {data[nt].num_nodes} nodes, {data[nt].x.size(1) if hasattr(data[nt], 'x') else 0} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1500e+02, 1.0000e-06, 1.0000e-06,  ..., 7.4000e+01, 1.0000e-06,\n",
       "        1.0000e-06])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_type = ('legislator_term', 'voted_on', 'bill_version')\n",
    "if edge_type not in data.edge_index_dict:\n",
    "    raise ValueError(\"Missing 'voted_on' edges in data.\")\n",
    "\n",
    "ei = data[edge_type].edge_index\n",
    "ea = data[edge_type].edge_attr\n",
    "\n",
    "vote_signal = ea[:, 0]\n",
    "tgt_nodes = ei[1]\n",
    "\n",
    "num_bills = data['bill_version'].num_nodes\n",
    "device = tgt_nodes.device\n",
    "\n",
    "yes_votes = torch.zeros(num_bills, device=device)\n",
    "no_votes = torch.zeros(num_bills, device=device)\n",
    "\n",
    "yes_votes.index_add_(0, tgt_nodes, (vote_signal > 0).float())\n",
    "no_votes.index_add_(0, tgt_nodes, (vote_signal <= 0).float())\n",
    "\n",
    "total_votes = yes_votes + no_votes + 1e-6\n",
    "\n",
    "yes_ratio = yes_votes / total_votes\n",
    "no_ratio = no_votes / total_votes\n",
    "\n",
    "controversy = 4 * yes_ratio * no_ratio\n",
    "controversy = controversy.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0174, 0.0000, 0.0000,  ..., 0.4459, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 = data['bill'].n_id.tolist()\n",
    "key2 = data['bill'].node_id.tolist()\n",
    "key = {k1: k2 for k1, k2 in zip(key1, key2)}\n",
    "cluster_bill = {}\n",
    "nids = []\n",
    "for bill_nid, lab in topic_cluster_labels_dict.items():\n",
    "        if bill_nid in key:\n",
    "            cluster_bill[key[bill_nid]] = lab\n",
    "            nids.append(key[bill_nid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_ts = pickle.loads(open('../../../bill_dates_map.pkl', 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_ids = {v: k for k, v in node_id_map['bill_version'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2b_edge = tuple([et for et in data.edge_types\n",
    "                if et[0] == \"bill_version\" and et[2] == \"bill\"])[0]\n",
    "src, dst = data[v2b_edge].edge_index.numpy()\n",
    "\n",
    "bv_df = pd.DataFrame({\"bill_version\": src, \"bill_id\": data['bill'].n_id[dst]})\n",
    "bv_df['bill_version_id'] = bv_df['bill_version'].map(bv_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_dates = pd.DataFrame(bv_ts).T.reset_index().rename(columns={'index': 'bill_id'})\n",
    "bill_dates = bill_dates.loc[bill_dates['bill_id'].isin(bv_df['bill_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "controversy_df = pd.DataFrame({\n",
    "    'controversy': data['bill_version'].controversy[bv_df['bill_version'].unique()].numpy(),\n",
    "    'bill_version': bv_df['bill_version'].unique()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df = pd.DataFrame({\n",
    "    'bill_id': data['bill'].n_id,\n",
    "    'outcome': data['bill'].y\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills = bv_df.merge(controversy_df, on='bill_version', how='left').merge(outcome_df, on='bill_id', how='left')\n",
    "bills['topic_cluster'] = bills['bill_id'].map(topic_cluster_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_dates['longevity'] = bill_dates['Last_action'] - bill_dates['First_action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_df = bills.groupby('bill_id').agg({'outcome': 'max', 'controversy': 'max', 'topic_cluster': 'max'}).merge(bill_dates[['bill_id', 'longevity']], on='bill_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../bill_labels_updated.json', 'r') as f:\n",
    "    bill_subjects = np.array(list(json.load(f).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../bill_labels_updated.json', 'r') as f:\n",
    "    bill_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_originals = pickle.load(open('../../../subjects_original.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../bill_subjects.json', 'r') as f:\n",
    "    bill_subjects_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "so = {k: subject_originals[v] for k, v in bill_subjects_dict.items() if v in subject_originals}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = bill_df.loc[bill_df['topic_cluster'].notna()].copy()\n",
    "topics['term'] = topics['bill_id'].apply(lambda x: x[:4]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = topics.groupby(['term', 'topic_cluster']).agg({'outcome': lambda x: len(x.loc[x == 1]) / len(x), 'controversy': lambda x: np.mean(x.loc[x > 0]), 'bill_id': 'nunique', 'longevity': 'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_topics = bills[['bill_version', 'topic_cluster']].loc[bills['topic_cluster'].notna()].drop_duplicates().set_index('bill_version').to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_cluster = torch.full(\n",
    "    (data[\"bill_version\"].num_nodes,),\n",
    "    -1, dtype=torch.long)\n",
    "\n",
    "for bv_id, topic_id in bv_topics['topic_cluster'].items():\n",
    "    bv_cluster[bv_id] = int(topic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_to_leg = data.edge_index_dict[('legislator', 'samePerson', 'legislator_term')]\n",
    "leg_of_lt, lt_idx = lt_to_leg\n",
    "\n",
    "leg_align = actor_align[\"legislator\"]\n",
    "leg_topic_prob = torch.zeros(\n",
    "    data['legislator_term'].num_nodes, K_TOPICS, dtype=FLOAT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_cols(K):\n",
    "    return [f\"topic_{k}\" for k in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legislator_term_topic_df(id_name_map):\n",
    "    K = actor_align[\"legislator\"].size(1)\n",
    "    leg_infl = actor_influence[\"legislator\"]\n",
    "\n",
    "    infl_term = torch.zeros(data['legislator_term'].num_nodes, dtype=FLOAT)\n",
    "    infl_term.index_copy_(0, lt_idx, leg_infl[leg_of_lt])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        leg_topic_prob.numpy(),\n",
    "        columns=[f\"topic_{k}\" for k in range(K)]\n",
    "    )\n",
    "    df[\"influence\"] = infl_term.numpy()\n",
    "    df[\"legislator_term\"] = range(len(df))\n",
    "    df[\"name\"] = df[\"legislator_term\"].map(id_name_map)\n",
    "    return df, infl_term.numpy()\n",
    "\n",
    "leg_topics_df, infl_term = legislator_term_topic_df(node_id_map[\"legislator_term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_topic_df(nt):\n",
    "    prob = actor_align[nt]\n",
    "    infl = actor_influence[nt]\n",
    "    df = pd.DataFrame(prob.numpy(), columns=topic_cols(K_TOPICS))\n",
    "    df[nt] = np.arange(len(df))\n",
    "    df[\"name\"] = data[nt].n_id\n",
    "    df[\"influence\"] = infl.numpy()\n",
    "    return df\n",
    "\n",
    "donor_df = actor_topic_df(\"donor\")\n",
    "lobby_df = actor_topic_df(\"lobby_firm\")\n",
    "comm_df = actor_topic_df(\"committee\")\n",
    "leg_df = actor_topic_df(\"legislator\")\n",
    "lt_df = pd.DataFrame(leg_topic_prob.numpy(), columns=topic_cols(K_TOPICS))\n",
    "lt_df[\"influence\"] = infl_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LEG = data['legislator'].num_nodes\n",
    "N_LT = data['legislator_term'].num_nodes\n",
    "N_BILL = data['bill'].num_nodes\n",
    "N_COMM = data['committee'].num_nodes\n",
    "N_LOB = data['lobby_firm'].num_nodes\n",
    "N_DON = data['donor'].num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_don, _ = data.edge_index_dict[('donor','donated_to','legislator_term')]\n",
    "don_out = torch.zeros(N_DON)\n",
    "don_out.index_add_(0, src_don,\n",
    "                   data[('donor','donated_to','legislator_term')].edge_attr[:,0].abs())\n",
    "donor_df[\"total_spent\"] = don_out.numpy()\n",
    "\n",
    "src_lo1, _ = data.edge_index_dict[('lobby_firm','lobbied','legislator_term')]\n",
    "src_lo2, _ = data.edge_index_dict[('lobby_firm','lobbied','committee')]\n",
    "\n",
    "lob_out = torch.zeros(N_LOB)\n",
    "for src_lo, et in [(src_lo1, ('lobby_firm','lobbied','legislator_term')),\n",
    "                   (src_lo2, ('lobby_firm','lobbied','committee'))]:\n",
    "    lob_out.index_add_(0, src_lo,\n",
    "        data[et].edge_attr[:,0].abs())\n",
    "lobby_df[\"total_spent\"] = lob_out.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "don_src, don_dst = data.edge_index_dict[('donor','donated_to','legislator_term')]\n",
    "lob_src1, lob_dst1 = data.edge_index_dict[('lobby_firm','lobbied','legislator_term')]\n",
    "\n",
    "lt_in = torch.zeros(N_LT)\n",
    "lt_in.index_add_(0, don_dst,\n",
    "    data[('donor','donated_to','legislator_term')].edge_attr[:,0].abs())\n",
    "donations = lt_in.clone()\n",
    "lt_in.index_add_(0, lob_dst1,\n",
    "    data[('lobby_firm','lobbied','legislator_term')].edge_attr[:,0].abs())\n",
    "\n",
    "leg_in = torch.zeros(N_LEG)\n",
    "leg_in.index_add_(0, leg_of_lt, lt_in[lt_idx])\n",
    "don_in = torch.zeros(N_LEG)\n",
    "don_in.index_add_(0, leg_of_lt, donations[lt_idx])\n",
    "leg_df[\"total_received\"] = leg_in.numpy()\n",
    "leg_df[\"total_donations\"] = don_in.numpy()\n",
    "leg_df['total_lobbying'] = leg_df['total_received'] - leg_df['total_donations']\n",
    "\n",
    "_, com_dst = data.edge_index_dict[('lobby_firm','lobbied','committee')]\n",
    "com_in = torch.zeros(N_COMM)\n",
    "com_in.index_add_(0, com_dst,\n",
    "    data[('lobby_firm','lobbied','committee')].edge_attr[:,0].abs())\n",
    "comm_df[\"total_received\"] = com_in.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_year(ts_tensor):\n",
    "    return pd.to_datetime(ts_tensor.cpu().numpy(), unit=\"s\").year.astype(np.int16)\n",
    "\n",
    "def money_by_topic(edge_key, src_df):\n",
    "    src_idx, dst_idx = data.edge_index_dict[edge_key]\n",
    "    dollars  = data[edge_key].edge_attr[:,0].abs().cpu()\n",
    "\n",
    "    prob_src = torch.from_numpy(src_df[topic_cols(K_TOPICS)].to_numpy())\n",
    "    infl_src = torch.from_numpy(src_df[\"influence\"].to_numpy())\n",
    "\n",
    "    w = prob_src[src_idx] * infl_src[src_idx,None]\n",
    "    topic_dollars = torch.zeros(K_TOPICS)\n",
    "    topic_dollars.index_add_(0, torch.arange(K_TOPICS).repeat(len(w)),\n",
    "                             (w*dollars[:,None]).flatten())\n",
    "    return topic_dollars.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = pd.read_csv('../../../sampled_labels - sampled_labels.csv')\n",
    "big_labels = {row['cluster']: row['Label'] for _, row in sl.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_top_n(df, n=5):\n",
    "    top = (\n",
    "        df[topic_cols(K_TOPICS)]\n",
    "          .apply(lambda r: r.nlargest(n).index.str[6:], axis=1)\n",
    "    )\n",
    "    topics = top.apply(lambda r: [big_labels[int(t)] for t in r])\n",
    "    df['top_topics'] = topics\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _df in (donor_df, lobby_df, comm_df, leg_df):\n",
    "    add_top_n(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislators = pickle.load(open('../../../legislators.pkl', 'rb'))\n",
    "\n",
    "leg_ids = {v: k for k, v in node_id_map['legislator_term'].items()}\n",
    "\n",
    "def leg_term_to_name(leg_term_id):\n",
    "    if isinstance(leg_term_id, str):\n",
    "        num = int(leg_term_id.split('_')[0])\n",
    "        return legislators.get(num, None)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def leg_term_to_term(leg_term_id):\n",
    "    if isinstance(leg_term_id, str):\n",
    "        a = leg_term_id.split('_')[1]\n",
    "        return int(a.split('-')[0]) if a else None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "leg_df['legislator'] = leg_df['name'].astype(int).map(leg_ids).apply(leg_term_to_name)\n",
    "leg_df['term'] = leg_df['name'].astype(int).map(leg_ids).apply(leg_term_to_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians = pd.read_csv('../../../ca_leg/legislation_data/politicians.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = politicians.loc[politicians['District No.'].isna(), ['full_name', 'Term']].drop_duplicates()\n",
    "fix['District No.'] = [51, 58, 8, 58, 58, 29, 39, 48, 43, 48, 10, 43, 48, 48, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in fix.iterrows():\n",
    "    politicians.loc[(politicians['full_name'] == row['full_name']) & (politicians['Term'] == row['Term']), 'District No.'] = row['District No.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = politicians[['District No.', 'Term', 'full_name', 'chamber', 'Party']].drop_duplicates()\n",
    "pol['term'] = pol['Term'].apply(lambda x: x.split('-')[0]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfund = pol.merge(pldd, on=['full_name', 'term'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import tempfile, zipfile, pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zip(zip_path, crs=3857):\n",
    "    tmp = tempfile.TemporaryDirectory()\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        zf.extractall(tmp.name)\n",
    "    shp = next(pathlib.Path(tmp.name).rglob(\"*.shp\"))\n",
    "    gdf = gpd.read_file(shp).set_crs(epsg=crs)\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "    return gdf, tmp\n",
    "\n",
    "def district_cycle(year):\n",
    "    if year <= 2012: return \"2001\"\n",
    "    if year <= 2022: return \"2011\"\n",
    "    return \"current\"\n",
    "\n",
    "counties_gdf, _ = read_zip('../data/ca_counties.zip')\n",
    "counties_gdf = counties_gdf[['COUNTYFP', 'NAMELSAD', 'geometry']]\n",
    "counties_gdf['county_area'] = counties_gdf.geometry.area\n",
    "counties_gdf['county_id'] = counties_gdf['COUNTYFP'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgdf = counties_gdf.to_json(na='drop', to_wgs84=True)\n",
    "with open(OUT_PATH / 'counties.geojson', 'w') as f:\n",
    "    f.write(cgdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('../data')\n",
    "\n",
    "asm11_zip = data_dir / '2011_assembly_state_shp.zip'\n",
    "sen11_zip = data_dir / '2011_senate_state_shp.zip'\n",
    "asmcur_zip = data_dir / '2021_AD_Final_shp.zip'\n",
    "sencur_zip = data_dir / '2021_SD_Final_shp.zip'\n",
    "\n",
    "dist_info = [\n",
    "    (asm11_zip, \"assembly\", \"2011\", 4019),\n",
    "    (sen11_zip, \"senate\",   \"2011\", 4019),\n",
    "    (asmcur_zip, \"assembly\",\"current\", 4269),\n",
    "    (sencur_zip, \"senate\",  \"current\", 4269)\n",
    "]\n",
    "\n",
    "weight_records = []\n",
    "tmps = []\n",
    "for zp, house, cycle, crs in dist_info:\n",
    "    gdf, tmp = read_zip(zp, crs)\n",
    "    tmps.append(tmp)\n",
    "    gdf = gdf.rename(columns={gdf.columns[0]: \"district_id\"})[[\"district_id\", \"geometry\"]]\n",
    "    gdf[\"house\"] = house\n",
    "    gdf[\"cycle\"] = cycle\n",
    "    gdf[\"dist_area\"] = gdf.geometry.area\n",
    "\n",
    "    inter = gpd.overlay(gdf, counties_gdf, how=\"intersection\")\n",
    "    inter[\"fragment_area\"] = inter.geometry.area\n",
    "\n",
    "    weight_records.append(\n",
    "        inter[[\"house\", \"cycle\", \"district_id\", \"county_id\", \"fragment_area\", 'county_area', 'dist_area']].reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "weights = pd.concat(weight_records, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights['weight'] = weights['fragment_area'] / weights['county_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfund['District No.'] = lfund['District No.'].astype(str).apply(lambda x: re.sub(r'\\s', '', x)).astype(float).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "from collections import defaultdict\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_topics = defaultdict(list)\n",
    "for _, row in lfund.iterrows():\n",
    "    try:\n",
    "        for t in ast.literal_eval(row['top_topics']):\n",
    "            term_topics[(row['Term'], row['District No.'], row['chamber_x'])].append(t)\n",
    "    except:\n",
    "        pass\n",
    "term_topics_ = {k: mode(v) for k, v in term_topics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfund_ = lfund.groupby(['Term', 'District No.', 'chamber_x']).agg({\n",
    "    'total_donations': 'sum',\n",
    "    'total_lobbying': 'sum',\n",
    "    'total_received': 'sum',\n",
    "    'top_topics': lambda x: list(x)\n",
    "}).reset_index()\n",
    "lfund_['cycle'] = lfund_['Term'].apply(lambda x: '2011' if int(x.split('-')[0]) <= 2012 else 'current')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_funds = lfund_.merge(weights, left_on=['cycle', 'District No.', 'chamber_x'], right_on=['cycle', 'district_id', 'house'], how='left')\n",
    "\n",
    "reg_funds['total_donations'] *= reg_funds['weight']\n",
    "reg_funds['total_lobbying'] *= reg_funds['weight']\n",
    "reg_funds['total_received'] *= reg_funds['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_topics = defaultdict(list)\n",
    "for _, row in reg_funds.iterrows():\n",
    "    if row['top_topics'] == [np.nan] or row['top_topics'][0] is None:\n",
    "        continue\n",
    "    try:\n",
    "        for t in ast.literal_eval(row['top_topics'][0]):\n",
    "            if t not in ['Extraordinary Sessions', 'Health Facilities']:\n",
    "                county_topics[row['county_id']].append(t)\n",
    "    except:\n",
    "        pass\n",
    "county_topics_ = {k: mode(v) for k, v in county_topics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_funds_ = reg_funds.groupby(['county_id', 'house']).agg({\n",
    "    'total_donations': 'sum',\n",
    "    'total_lobbying': 'sum',\n",
    "    'total_received': 'sum'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_legislator_topics = pd.read_csv('../../shiny/data/ca_legislator_topics.csv')\n",
    "ca_legislator_funding = pd.read_csv('../../shiny/data/ca_legislator_funding.csv')\n",
    "last_topics = ca_legislator_topics.loc[ca_legislator_topics['Term'].apply(lambda x: int(x.split('-')[0]) == 2025)]\n",
    "cal = ca_legislator_funding.merge(last_topics, on=['county_id', 'house'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_funds_['topic'] = reg_funds_['county_id'].map(county_topics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_cal = reg_funds_.merge(counties_gdf, on='county_id', how='left')\n",
    "gpd.GeoDataFrame(co_cal, geometry='geometry').to_file(OUT_PATH / 'ca_legislator_funding.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_funds_.to_csv(OUT_PATH / 'ca_legislator_funding.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_topics = ltopic_.merge(weights, left_on=['cycle', 'District No.', 'chamber'], right_on=['cycle', 'district_id', 'house'], how='right')\n",
    "\n",
    "for i in range(K_TOPICS):\n",
    "    reg_topics[f'topic_{i}'] *= reg_topics['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_topics_ = reg_topics.groupby(['house', 'Term', 'county_id'])[topic_cols(K_TOPICS)].sum().reset_index().merge(counties_gdf[['county_id', 'NAMELSAD']], on='county_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_topics_ = add_top_n(reg_topics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_topics_[['house', 'Term', 'county_id', 'NAMELSAD', 'top_topics']].to_csv(\n",
    "    OUT_PATH / 'ca_legislator_topics.csv', index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei = data[(\"legislator_term\",\"wrote\",\"bill_version\")].edge_index.numpy()\n",
    "ea = data[(\"legislator_term\",\"wrote\",\"bill_version\")].edge_attr.numpy()\n",
    "author_edge = pd.DataFrame({\"legterm_id\": ei[0], \"bill_id\": ei[1], \"type\": ea[:,0]})\n",
    "author_edge['date'] = data[\"bill_version\"].time.numpy()[author_edge.bill_id]\n",
    "author_edge.loc[author_edge.date == 0, 'date'] = datetime.datetime(2000, 6, 15).timestamp()\n",
    "author_edge['date'] = pd.to_datetime(author_edge['date'], unit='s')\n",
    "\n",
    "eib = data[('bill_version','is_version', 'bill')].edge_index.numpy()\n",
    "eib = pd.DataFrame({\"src\": eib[0], \"dst\": eib[1], 'outcome': data['bill'].y[eib[1]]})\n",
    "eib['src'] = eib['src'].astype(int)\n",
    "eib['dst'] = eib['dst'].astype(int)\n",
    "author_edge['bill_id'] = author_edge['bill_id'].astype(int)\n",
    "\n",
    "author_edge = author_edge.merge(eib, left_on='bill_id', right_on='src', how='inner')\n",
    "author_edge['outcome'] = (author_edge['outcome'] == 1).astype(int)\n",
    "author_levels = {1: 'COAUTHOR', 2: 'PRINCIPAL_COAUTHOR', 3: 'LEAD_AUTHOR'}\n",
    "author_edge['author_type'] = author_edge['type'].map(author_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve = data[('bill_version', 'rev_voted_on', 'legislator_term')].edge_index.numpy()\n",
    "va = data[('bill_version', 'rev_voted_on', 'legislator_term')].edge_attr.numpy()\n",
    "vote_edge = pd.DataFrame({'bill_version': ve[0], 'legislator_term': ve[1], 'vote_signal': va[:, 0]})\n",
    "vote_edge = vote_edge.merge(eib, left_on='bill_version', right_on='src', how='left').merge(bv_df, on='bill_version', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_edge['full_name'] = vote_edge['legislator_term'].map(leg_ids).apply(leg_term_to_name)\n",
    "vote_edge['term'] = vote_edge['legislator_term'].map(leg_ids).apply(leg_term_to_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = vote_edge.groupby('bill_id').agg({'outcome': 'max', 'vote_signal': lambda x: (x > 0).sum() / len(x)})\n",
    "signals.loc[(signals['outcome'] == 0.0) & (signals['vote_signal'] == 1.0), 'vote_signal'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = author_edge.merge(bv_df, left_on='bill_id', right_on='bill_version', how='left').groupby('legterm_id').agg({\n",
    "    'outcome': 'mean',\n",
    "    'author_type': lambda x: sum(x == 'LEAD_AUTHOR'),\n",
    "    'bill_version': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "a3['full_name'] = a3['legterm_id'].map(leg_ids).apply(leg_term_to_name)\n",
    "a3['term'] = a3['legterm_id'].map(leg_ids).apply(leg_term_to_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4 = a3.merge(lfund, on=['full_name', 'term'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4[['outcome', 'author_type', 'bill_version', 'top_topics',  'full_name', 'term', 'total_donations', 'total_lobbying', 'total_received', 'Party', 'chamber']].copy().to_csv(OUT_PATH / 'legislator_terms.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "don = donor_df[['name', 'influence', 'total_spent', 'top_topics']].copy()\n",
    "don['type'] = 'donor'\n",
    "lob = lobby_df[['name', 'influence', 'total_spent', 'top_topics']].copy()\n",
    "lob['type'] = 'lobby_firm'\n",
    "donor_lobby = pd.concat([don, lob], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_lobby.to_csv(OUT_PATH / 'donor_lobby_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_edge['bill'] = data['bill'].n_id[author_edge['dst'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = author_edge.groupby('bill').agg({'date': 'max'}).reset_index().merge(author_edge, on=['bill', 'date'], how='inner').groupby('bill').agg({\n",
    "    'legterm_id': lambda x: ', '.join(x.astype(str).unique())}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_to_names(terms):\n",
    "    term_names = []\n",
    "    for t in terms.split(', '):\n",
    "        l = leg_ids.get(int(t.strip()), None)\n",
    "        if l is not None:\n",
    "            term_names.append(leg_term_to_name(l))\n",
    "    return ', '.join([n for n in term_names if n is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors['authors'] = authors['legterm_id'].apply(terms_to_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = bill_df.merge(authors, left_on='bill_id', right_on='bill', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['longevity'] = b['longevity'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = b.merge(bv_df.groupby('bill_id')['bill_version_id'].nunique().reset_index(), on='bill_id', how='left').merge(bill_dates[['bill_id', 'First_action']], on='bill_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi['term'] = bi['bill_id'].apply(lambda x: x[:4]).astype(int)\n",
    "bi['First_action'] = bi['First_action'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "bil = bi.merge(signals, on='bill_id', how='left')\n",
    "bil['vote_signal'] = bil['vote_signal'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bil['topic'] = bil['bill_id'].map(so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(pa.Table.from_pandas(bil), OUT_PATH / 'bills.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills = pq.read_table(OUT_PATH / 'bills.parquet').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills.to_csv(OUT_PATH / 'bills.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = bi.groupby(['topic_cluster', 'term']).agg({\n",
    "    'outcome': 'mean',\n",
    "    'controversy': 'mean',\n",
    "    'longevity': 'mean',\n",
    "    'bill_id': 'nunique',\n",
    "    'bill_version_id': 'mean'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lfund.groupby(['chamber', 'term', 'Party'])[topic_cols(K_TOPICS)].mean().reset_index()\n",
    "f = add_top_n(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ = f.pivot_table(\n",
    "    index='term',\n",
    "    columns='Party',\n",
    "    values=topic_cols(K_TOPICS),\n",
    "    aggfunc='mean'\n",
    ")\n",
    "f_.columns = [f\"{col[0]}_{col[1]}\" for col in f_.columns]\n",
    "\n",
    "f_ = f_[[c for c in f_.columns if not c.endswith('_I')]]\n",
    "f_ = f_.fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n",
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_48394/1155280054.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')\n"
     ]
    }
   ],
   "source": [
    "def partisan_split(row):\n",
    "    splits = {}\n",
    "    for t in topic_cols(K_TOPICS):\n",
    "        d = row[f\"{t}_D\"]\n",
    "        r = row[f\"{t}_R\"]\n",
    "        splits[t] = d - r\n",
    "    return splits\n",
    "\n",
    "f_[[f\"{t}_split\" for t in topic_cols(K_TOPICS)]] = f_.apply(partisan_split, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = f_.melt(\n",
    "    id_vars=['term'],\n",
    "    value_vars=[f\"{t}_split\" for t in topic_cols(K_TOPICS)],\n",
    "    var_name='topic',\n",
    "    value_name='partisan_split'\n",
    ")\n",
    "fg['topic_cluster'] = fg['topic'].apply(lambda x: re.search(r'_(\\d+)', x).group(1) if '_' in x else x).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ = t.merge(fg[['term', 'topic_cluster', 'partisan_split']], on=['term', 'topic_cluster'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_['topic'] = t_['topic_cluster'].map(big_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_.to_csv(OUT_PATH / 'topics_agg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['topic_cluster', 'term', 'outcome', 'controversy', 'longevity',\n",
       "       'bill_id', 'bill_version_id', 'partisan_split', 'topic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5(df, col_name):\n",
    "    df = df.copy().set_index(col_name)\n",
    "    top5_dict = {\n",
    "        int(topic.split('_')[1]): df[topic].nlargest(5).index.to_list()\n",
    "        for topic in df.columns if topic.startswith('topic_')\n",
    "    }\n",
    "    return top5_dict\n",
    "\n",
    "top_donors = top5(donor_df, 'name')\n",
    "top_lobby = top5(lobby_df, 'name')\n",
    "top_leg = top5(leg_df, 'legislator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "topents = pd.DataFrame({\n",
    "    'topic': top_donors.keys(),\n",
    "    'top_donors': top_donors.values(),\n",
    "    'top_lobby': top_lobby.values(),\n",
    "    'top_legislators': top_leg.values()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "topents['subject'] = topents['topic'].map(big_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "topents.to_csv(OUT_PATH / 'top_entities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
