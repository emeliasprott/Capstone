{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, datetime, json, pickle, re\n",
    "from pathlib import Path\n",
    "from torch_geometric.transforms import ToUndirected, RemoveIsolatedNodes\n",
    "from torch_scatter import scatter_mean\n",
    "from pathlib import Path\n",
    "import torch, numpy as np, pandas as pd\n",
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "\n",
    "DATA_PATH = Path(\"../../../data3.pt\")\n",
    "EXPORT_PATH = Path(\"../../shiny/data\")\n",
    "OUT_PATH = EXPORT_PATH\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "FLOAT = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- identify and map topic labels to all dfs\n",
    "- change column names to fit app\n",
    "- add more data to the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../node_id_map.json', 'r') as f:\n",
    "        node_id_map = json.load(f)\n",
    "\n",
    "with open('../../../bill_labels_updated.json', 'r') as f:\n",
    "    topic_cluster_labels_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(\"../../../\")\n",
    "\n",
    "preds = torch.load(OUT_DIR / \"model_outputs.pt\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_normalize_timestamps(timestamps, eps=1e-8):\n",
    "    timestamps = torch.nan_to_num(timestamps, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "    p5 = torch.quantile(timestamps, 0.05)\n",
    "    p95 = torch.quantile(timestamps, 0.95)\n",
    "\n",
    "    if (p95 - p5) < eps:\n",
    "        return torch.zeros_like(timestamps)\n",
    "\n",
    "    timestamps = torch.clamp(timestamps, p5, p95)\n",
    "    normalized = (timestamps - p5) / (p95 - p5)\n",
    "    return torch.nan_to_num(normalized, nan=0.0)\n",
    "\n",
    "def safe_standardize_time_format(time_data):\n",
    "    times = []\n",
    "    for t in time_data:\n",
    "        try:\n",
    "            if isinstance(t, (int, float)) and 1900 <= t  and t <= 2100:\n",
    "                td = datetime.datetime(int(t), 6, 15).timestamp()\n",
    "            elif (isinstance(t, str) or (isinstance(t, float))) and (float(t) < 2100 and float(t) > 1900):\n",
    "                td = datetime.datetime(int(float(t)), 6, 15).timestamp()\n",
    "            elif float(t) > 0 and float(t) < 1990:\n",
    "                td = t\n",
    "            elif float(t) > 17000000.0:\n",
    "                td = float(t)\n",
    "            elif isinstance(t, datetime.datetime):\n",
    "                td = t.timestamp()\n",
    "            else:\n",
    "                td = float(t) * 1e9\n",
    "        except:\n",
    "            td = datetime.datetime(2000, 6, 15).timestamp()\n",
    "        times.append(td)\n",
    "    return torch.tensor(times, dtype=torch.float32)\n",
    "\n",
    "def pull_timestamps(data):\n",
    "    timestamp_edges = [\n",
    "        ('donor', 'donated_to', 'legislator_term'),\n",
    "        ('legislator_term', 'rev_donated_to', 'donor'),\n",
    "        ('lobby_firm', 'lobbied', 'legislator_term'),\n",
    "        ('lobby_firm', 'lobbied', 'committee'),\n",
    "        ('committee', 'rev_lobbied', 'lobby_firm'),\n",
    "        ('legislator_term', 'rev_lobbied', 'lobby_firm'),\n",
    "        ('bill_version', 'rev_voted_on', 'legislator_term'),\n",
    "        ('legislator_term', 'voted_on', 'bill_version'),\n",
    "    ]\n",
    "    timestamp_nodes = ['legislator_term', 'bill_version', 'bill']\n",
    "\n",
    "    for et in timestamp_edges:\n",
    "        if hasattr(data[et], 'edge_attr') and data[et].edge_attr is not None and len(data[et].edge_attr.size()) > 1:\n",
    "            if data[et].edge_attr.size(1) > 1:\n",
    "                edge_attr = data[et].edge_attr\n",
    "                ts_col = edge_attr[:, -1]\n",
    "                if ts_col.abs().max() > 1e8 or ts_col.min() < 0:\n",
    "                    ts_col = safe_standardize_time_format(ts_col.tolist()).to(edge_attr.device)\n",
    "                data[et].timestamp = safe_normalize_timestamps(ts_col)\n",
    "                data[et].edge_attr = edge_attr[:, :-1]\n",
    "\n",
    "    for nt in timestamp_nodes:\n",
    "        if hasattr(data[nt], 'x') and data[nt].x is not None:\n",
    "            try:\n",
    "                if len(data[nt].x.size()) > 1:\n",
    "                    if data[nt].x.size(1) > 1:\n",
    "                        x = data[nt].x\n",
    "                        ts_col = x[:, -1]\n",
    "                        if ts_col.abs().max() > 1e8 or ts_col.min() < 0:\n",
    "                            ts_col = safe_standardize_time_format(ts_col.tolist()).to(x.device)\n",
    "                        if nt in timestamp_nodes or ts_col.abs().max() > 1e6:\n",
    "                            data[nt].timestamp = safe_normalize_timestamps(ts_col)\n",
    "                            data[nt].x = x[:, :-1]\n",
    "            except:\n",
    "                pass\n",
    "    return data\n",
    "\n",
    "def clean_features(data):\n",
    "    for nt in data.node_types:\n",
    "        x = data[nt].x\n",
    "        x = torch.as_tensor(x, dtype=torch.float32)\n",
    "        x = torch.nan_to_num(x.float(), nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "        mean = x.mean(0, keepdim=True)\n",
    "        std = x.std(0, keepdim=True).clamp(min=1e-5)\n",
    "        x = ((x - mean) / std).clamp(-10, 10)\n",
    "        data[nt].x = x\n",
    "        data[nt].x_mean = mean\n",
    "        data[nt].x_std = std\n",
    "    data = pull_timestamps(data)\n",
    "    return data\n",
    "\n",
    "def compute_controversiality(data):\n",
    "    edge_type = ('legislator_term', 'voted_on', 'bill_version')\n",
    "    if edge_type not in data.edge_index_dict:\n",
    "        raise ValueError(\"Missing 'voted_on' edges in data.\")\n",
    "\n",
    "    ei = data[edge_type].edge_index\n",
    "    ea = data[edge_type].edge_attr\n",
    "    vote_signal = ea[:, 0]\n",
    "    tgt_nodes = ei[1]\n",
    "\n",
    "    num_bv = data['bill_version'].num_nodes\n",
    "    device = vote_signal.device\n",
    "    yes = torch.zeros(num_bv, device=device)\n",
    "    no = torch.zeros(num_bv, device=device)\n",
    "\n",
    "    yes.index_add_(0, tgt_nodes, (vote_signal > 0).float())\n",
    "    no.index_add_(0, tgt_nodes, (vote_signal <= 0).float())\n",
    "    total = yes + no + 1e-6\n",
    "    controversy_bv = 4 * (yes / total) * (no / total)\n",
    "    controversy_bv = controversy_bv.clamp(0, 1)\n",
    "    data['bill_version'].controversy = controversy_bv\n",
    "\n",
    "    is_version_et = ('bill_version', 'is_version', 'bill')\n",
    "    if is_version_et not in data.edge_index_dict:\n",
    "        raise ValueError(\"Missing 'is_version' edges for bill aggregation.\")\n",
    "\n",
    "    src, dst = data.edge_index_dict[is_version_et]\n",
    "    agg_b = scatter_mean(controversy_bv[src], dst, dim=0, dim_size=data['bill'].num_nodes)\n",
    "    data['bill'].controversy = agg_b\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(path='../../../data3.pt'):\n",
    "    full_data = torch.load(path, weights_only=False)\n",
    "    for nt in full_data.node_types:\n",
    "        if hasattr(full_data[nt], 'x') and full_data[nt].x is not None:\n",
    "            flat = torch.as_tensor(full_data[nt].x).flatten(start_dim=1)\n",
    "            full_data[nt].x = flat\n",
    "            full_data[nt].num_nodes = flat.size(0)\n",
    "\n",
    "    for edge_type, edge_index in full_data.edge_index_dict.items():\n",
    "        src_type, _, dst_type = edge_type\n",
    "        max_src_idx = edge_index[0].max().item() if edge_index.size(1) > 0 else -1\n",
    "        max_dst_idx = edge_index[1].max().item() if edge_index.size(1) > 0 else -1\n",
    "        if max_src_idx >= full_data[src_type].num_nodes:\n",
    "            print(f\"Fixing {src_type} node count: {full_data[src_type].num_nodes} -> {max_src_idx + 1}\")\n",
    "            full_data[src_type].num_nodes = max_src_idx + 1\n",
    "\n",
    "        if max_dst_idx >= full_data[dst_type].num_nodes:\n",
    "            print(f\"Fixing {dst_type} node count: {full_data[dst_type].num_nodes} -> {max_dst_idx + 1}\")\n",
    "            full_data[dst_type].num_nodes = max_dst_idx + 1\n",
    "    full_data['bill'].y[np.where(full_data['bill'].y < 0)[0]] = 0\n",
    "    full_data['bill'].y = torch.as_tensor(full_data['bill'].y, dtype=torch.float32)\n",
    "\n",
    "    data = ToUndirected(merge=False)(full_data)\n",
    "    del full_data\n",
    "    gc.collect()\n",
    "    data = RemoveIsolatedNodes()(data)\n",
    "    data = compute_controversiality(clean_features(data))\n",
    "\n",
    "    for nt in data.node_types:\n",
    "        ids = torch.arange(data[nt].num_nodes, device='mps')\n",
    "        data[nt].node_id = ids\n",
    "    for store in data.stores:\n",
    "        for key, value in store.items():\n",
    "            if isinstance(value, torch.Tensor) and value.dtype == torch.float64:\n",
    "                store[key] = value.float()\n",
    "\n",
    "    return data\n",
    "\n",
    "data = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 = data['bill'].n_id.tolist()\n",
    "key2 = data['bill'].node_id.tolist()\n",
    "key = {k1: k2 for k1, k2 in zip(key1, key2)}\n",
    "cluster_bill = {}\n",
    "nids = []\n",
    "for bill_nid, lab in topic_cluster_labels_dict.items():\n",
    "        if bill_nid in key:\n",
    "            cluster_bill[key[bill_nid]] = lab\n",
    "            nids.append(key[bill_nid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_ts = pickle.loads(open('../../../bill_dates_map.pkl', 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_ids = {v: k for k, v in node_id_map['bill_version'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2b_edge = tuple([et for et in data.edge_types\n",
    "                if et[0] == \"bill_version\" and et[2] == \"bill\"])[0]\n",
    "src, dst = data[v2b_edge].edge_index.numpy()\n",
    "\n",
    "bv_df = pd.DataFrame({\"bill_version\": src, \"bill_id\": data['bill'].n_id[dst]})\n",
    "bv_df['bill_version_id'] = bv_df['bill_version'].map(bv_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_dates = pd.DataFrame(bv_ts).T.reset_index().rename(columns={'index': 'bill_id'})\n",
    "bill_dates = bill_dates.loc[bill_dates['bill_id'].isin(bv_df['bill_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "controversy_df = pd.DataFrame.from_dict(preds['controversy'], orient='index', columns=['controversy']).reset_index().rename(columns={'index': 'bill_node'})\n",
    "controversy_df['bill_id'] = controversy_df['bill_node'].map({v: k for k, v in key.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df = pd.DataFrame({\n",
    "    'bill_id': data['bill'].n_id,\n",
    "    'outcome': data['bill'].y\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills = controversy_df.merge(outcome_df, on='bill_id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills['topic_cluster'] = bills['bill_id'].map(topic_cluster_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_dates['longevity'] = bill_dates['Last_action'] - bill_dates['First_action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_df = bills.merge(bill_dates, on='bill_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../bill_labels_updated.json', 'r') as f:\n",
    "    bill_subjects = np.array(list(json.load(f).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_originals = pickle.load(open('../../../subjects_original.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../bill_subjects.json', 'r') as f:\n",
    "    bill_subjects_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "so = {k: subject_originals[v] for k, v in bill_subjects_dict.items() if v in subject_originals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_labels = pd.read_csv('../../../sampled_labels - sampled_labels.csv')\n",
    "sol = {row['cluster']: row['Label'] for _, row in sampled_labels.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_df = bill_df.rename(columns={'topic_cluster': 'topic_id'})\n",
    "bill_df['topic'] = bill_df['topic_id'].map(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_df['controversy'] = bill_df['controversy'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = bill_df.loc[bill_df['topic_id'].notna()].copy()\n",
    "topics['term'] = topics['bill_id'].apply(lambda x: x[:4]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = topics.groupby(['term', 'topic']).agg({'outcome': lambda x: len(x.loc[x == 1]) / len(x), 'controversy': 'mean', 'bill_id': 'nunique', 'longevity': 'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tids = bill_df[['topic', 'topic_id']].drop_duplicates().sort_values(by='topic_id').reset_index(drop=True).reset_index(names='tid')\n",
    "topic_columns = tids['topic'].values.tolist() + ['Miscellaneous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_table(actor, key):\n",
    "    if key == 'alignment':\n",
    "        return pd.DataFrame.from_dict(preds[key][actor], orient='index', columns=topic_columns).reset_index().rename(columns={'index': f'{actor}_id'})\n",
    "    else:\n",
    "        return pd.DataFrame.from_dict(preds[key][actor], orient='index', columns=['influence']).reset_index().rename(columns={'index': f'{actor}_id'})\n",
    "\n",
    "leg_align = actor_table('legislator', 'alignment')\n",
    "leg_inf = actor_table('legislator', 'influence')\n",
    "donor_align = actor_table('donor', 'alignment')\n",
    "donor_inf = actor_table('donor', 'influence')\n",
    "lobby_align = actor_table('lobby_firm', 'alignment')\n",
    "lobby_inf = actor_table('lobby_firm', 'influence')\n",
    "committee_align = actor_table('committee', 'alignment')\n",
    "committee_inf = actor_table('committee', 'influence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_top_n(df, n=5):\n",
    "    top = (\n",
    "        df[[t for t in topic_columns if t not in ['Miscellaneous', 'Education Finance']]]\n",
    "          .apply(lambda r: r.nlargest(n).index.values, axis=1)\n",
    "    )\n",
    "    return top\n",
    "leg_align['top_topics'] = add_top_n(leg_align, 5)\n",
    "donor_align['top_topics'] = add_top_n(donor_align, 5)\n",
    "lobby_align['top_topics'] = add_top_n(lobby_align, 5)\n",
    "committee_align['top_topics'] = add_top_n(committee_align, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_year(ts_tensor):\n",
    "    return pd.to_datetime(ts_tensor.cpu().numpy(), unit=\"s\").year.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians = pd.read_csv(OUT_PATH / 'legislator_terms.csv')\n",
    "lobbying = pd.read_csv('../../../calaccess/lobbying_clean2.csv', dtype={'PAYEE_NAMS': str, 'BAKREF_TID': str})\n",
    "expend_assembly = pd.read_csv('../../../calaccess/expend_assembly_matched.csv', dtype={'TargetPropositionName': str})\n",
    "expend_senate = pd.read_csv('../../../calaccess/expend_senate_matched.csv', dtype={'TargetPropositionName': str})\n",
    "lobbying['expn_date'] = pd.to_datetime(lobbying['EXPN_DATE'])\n",
    "lobbying['term'] = lobbying['expn_date'].dt.year.astype(int)\n",
    "lobbying.loc[lobbying['expn_date'].dt.year > 2025, 'term'] = [2022, 2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobbying.loc[(lobbying['term'].isin([int(r) for r in range(2000, 2026, 2)])) & (lobbying['expn_date'].dt.month < 11), 'term'] = lobbying['term'] - 1\n",
    "lobbying.loc[(lobbying['term'].isin([int(r) for r in range(2000, 2026, 2)])) & (lobbying['expn_date'].dt.month >= 11), 'term'] = lobbying['term'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob = lobbying.groupby(['clean_beneficiary', 'term']).agg({'AMOUNT': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "expend_assembly = expend_assembly.loc[expend_assembly['term'].apply(lambda x: isinstance(x, str))]\n",
    "expend_assembly['year'] = expend_assembly['term'].apply(lambda x: int(str(x).split('-')[0]))\n",
    "expend_assembly.loc[expend_assembly['year'] // 2 == 0, 'year'] = expend_assembly.loc[expend_assembly['year'] // 2 == 0, 'year'] - 1\n",
    "exp_as = expend_assembly[['Amount', 'year', 'matched_target_name']].drop_duplicates().groupby(['matched_target_name', 'year']).agg({'Amount': 'sum'}).reset_index().rename(columns={'year': 'term'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "expend_senate['year'] = expend_senate['term'].apply(lambda x: int(x.split('-')[0]))\n",
    "expend_senate.loc[expend_senate['year'] // 2 == 0, 'year'] = expend_senate.loc[expend_senate['year'] // 2 == 0, 'year'] - 1\n",
    "exp_sen = expend_senate.groupby(['matched_target_name', 'year']).agg({'Amount': 'sum'}).reset_index().rename(columns={\"year\": 'term'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians['lower'] = politicians['full_name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_swap(name):\n",
    "    name = name.lower()\n",
    "    names = name.split(' ')\n",
    "    return names[-1] + ' ' + ' '.join(names[:-1])\n",
    "\n",
    "politicians['name2'] = politicians['full_name'].apply(name_swap)\n",
    "politicians.loc[politicians['name2'].isin([p for p in politicians['lower'].unique() if p in lob['clean_beneficiary'].unique()]), 'name2'] = politicians.loc[politicians['name2'].isin([p for p in politicians['lower'].unique() if p in lob['clean_beneficiary'].unique()]), 'lower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = politicians.merge(lob, left_on=['term', 'name2'], right_on=['term', 'clean_beneficiary'], how='left').rename(columns={'AMOUNT': 'total_lobbying_'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_as['name2'] = exp_as['matched_target_name'].apply(lambda x: re.sub(r'\\,', '', x.lower()))\n",
    "pld = pl.merge(exp_as, on=['term', 'name2'], how='left').rename(columns={'Amount': 'total_donations_'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_sen['name2'] = exp_sen['matched_target_name'].apply(lambda x: re.sub(r'\\,', '', x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pldd = pld.merge(exp_sen, on=['term', 'name2'], how='left')\n",
    "pldd['total_donations_'] = pldd[['total_donations_', 'Amount']].sum(skipna=True, axis=1)\n",
    "pldd = pldd.drop(columns=['total_donations', 'total_lobbying', 'Amount', 'total_received']).rename(columns={'total_donations_': 'total_donations', 'total_lobbying_': 'total_lobbying'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pldd['total_received'] = pldd['total_donations'] + pldd['total_lobbying']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['total_donations', 'total_lobbying', 'total_received']:\n",
    "    pldd[c] = pldd[c].fillna(0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_df = leg_align.merge(leg_inf, on='legislator_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislators = pickle.load(open('../../../legislators.pkl', 'rb'))\n",
    "\n",
    "leg_ids = {v: k for k, v in node_id_map['legislator_term'].items()}\n",
    "\n",
    "def leg_term_to_name(leg_term_id):\n",
    "    if isinstance(leg_term_id, str):\n",
    "        num = int(leg_term_id.split('_')[0])\n",
    "        return legislators.get(num, None)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def leg_term_to_term(leg_term_id):\n",
    "    if isinstance(leg_term_id, str):\n",
    "        a = leg_term_id.split('_')[1]\n",
    "        return int(a.split('-')[0]) if a else None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "leg_df['legislator'] = leg_df['legislator_id'].astype(int).map(leg_ids).apply(leg_term_to_name)\n",
    "leg_df['term'] = leg_df['legislator_id'].astype(int).map(leg_ids).apply(leg_term_to_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians = pd.read_csv(\"../../../ca_leg/legislation_data/politicians.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = politicians.loc[politicians['District No.'].isna(), ['full_name', 'Term']].drop_duplicates()\n",
    "fix['District No.'] = [51, 51, 51, 51, 57, 57, 57, 57, 36, 36, 36, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in fix.iterrows():\n",
    "    politicians.loc[(politicians['full_name'] == row['full_name']) & (politicians['Term'] == row['Term']), 'District No.'] = row['District No.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = politicians[['District No.', 'Term', 'full_name', 'chamber', 'Party']].drop_duplicates()\n",
    "pol['term'] = pol['Term'].apply(lambda x: x.split('-')[0]).astype(int)\n",
    "pol['full_name'] = pol['full_name'].apply(lambda x: (x.split(',')[1] + ' ' + x.split(',')[0]).strip() if ',' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfund = pol.merge(pldd, on=['full_name', 'term'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import tempfile, zipfile, pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zip(zip_path, crs=3857):\n",
    "    tmp = tempfile.TemporaryDirectory()\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        zf.extractall(tmp.name)\n",
    "    shp = next(pathlib.Path(tmp.name).rglob(\"*.shp\"))\n",
    "    gdf = gpd.read_file(shp).set_crs(epsg=crs)\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "    return gdf, tmp\n",
    "\n",
    "def district_cycle(year):\n",
    "    if year <= 2012: return \"2001\"\n",
    "    if year <= 2022: return \"2011\"\n",
    "    return \"current\"\n",
    "\n",
    "counties_gdf, _ = read_zip('../data/ca_counties.zip')\n",
    "counties_gdf = counties_gdf[['COUNTYFP', 'NAMELSAD', 'geometry']]\n",
    "counties_gdf['county_area'] = counties_gdf.geometry.area\n",
    "counties_gdf['county_id'] = counties_gdf['COUNTYFP'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgdf = counties_gdf.to_json(na='drop', to_wgs84=True)\n",
    "with open(OUT_PATH / 'counties.geojson', 'w') as f:\n",
    "    f.write(cgdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('../data')\n",
    "\n",
    "asm11_zip = data_dir / '2011_assembly_state_shp.zip'\n",
    "sen11_zip = data_dir / '2011_senate_state_shp.zip'\n",
    "asmcur_zip = data_dir / '2021_AD_Final_shp.zip'\n",
    "sencur_zip = data_dir / '2021_SD_Final_shp.zip'\n",
    "\n",
    "dist_info = [\n",
    "    (asm11_zip, \"assembly\", \"2011\", 4019),\n",
    "    (sen11_zip, \"senate\",   \"2011\", 4019),\n",
    "    (asmcur_zip, \"assembly\",\"current\", 4269),\n",
    "    (sencur_zip, \"senate\",  \"current\", 4269)\n",
    "]\n",
    "\n",
    "weight_records = []\n",
    "tmps = []\n",
    "for zp, house, cycle, crs in dist_info:\n",
    "    gdf, tmp = read_zip(zp, crs)\n",
    "    tmps.append(tmp)\n",
    "    gdf = gdf.rename(columns={gdf.columns[0]: \"district_id\"})[[\"district_id\", \"geometry\"]]\n",
    "    gdf[\"house\"] = house\n",
    "    gdf[\"cycle\"] = cycle\n",
    "    gdf[\"dist_area\"] = gdf.geometry.area\n",
    "\n",
    "    inter = gpd.overlay(gdf, counties_gdf, how=\"intersection\")\n",
    "    inter[\"fragment_area\"] = inter.geometry.area\n",
    "\n",
    "    weight_records.append(\n",
    "        inter[[\"house\", \"cycle\", \"district_id\", \"county_id\", \"fragment_area\", 'county_area', 'dist_area']].reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "weights = pd.concat(weight_records, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights['weight'] = weights['fragment_area'] / weights['county_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfund['District No.'] = lfund['District No.'].astype(str).apply(lambda x: re.sub(r'\\s', '', x)).astype(float).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "from collections import defaultdict\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_topics = defaultdict(list)\n",
    "for _, row in lfund.iterrows():\n",
    "    try:\n",
    "        for t in ast.literal_eval(row['top_topics']):\n",
    "            term_topics[(row['Term'], row['District No.'], row['chamber_x'])].append(t)\n",
    "    except:\n",
    "        pass\n",
    "term_topics_ = {k: mode(v) for k, v in term_topics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfund_ = lfund.groupby(['Term', 'District No.', 'chamber_x']).agg({\n",
    "    'total_donations': 'sum',\n",
    "    'total_lobbying': 'sum',\n",
    "    'total_received': 'sum',\n",
    "    'top_topics': lambda x: list(x)\n",
    "}).reset_index()\n",
    "lfund_['cycle'] = lfund_['Term'].apply(lambda x: '2011' if int(x.split('-')[0]) <= 2012 else 'current')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_funds = lfund_.merge(weights, left_on=['cycle', 'District No.', 'chamber_x'], right_on=['cycle', 'district_id', 'house'], how='left')\n",
    "\n",
    "reg_funds['total_donations'] *= reg_funds['weight']\n",
    "reg_funds['total_lobbying'] *= reg_funds['weight']\n",
    "reg_funds['total_received'] *= reg_funds['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_topics = defaultdict(list)\n",
    "for _, row in reg_funds.iterrows():\n",
    "    if row['top_topics'] == [np.nan] or row['top_topics'][0] is None:\n",
    "        continue\n",
    "    try:\n",
    "        for t in ast.literal_eval(row['top_topics'][0]):\n",
    "            if t not in ['Extraordinary Sessions', 'Health Facilities']:\n",
    "                county_topics[row['county_id']].append(t)\n",
    "    except:\n",
    "        pass\n",
    "county_topics_ = {k: mode(v) for k, v in county_topics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_funds_ = reg_funds.groupby(['county_id', 'house']).agg({\n",
    "    'total_donations': 'sum',\n",
    "    'total_lobbying': 'sum',\n",
    "    'total_received': 'sum'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_funds_['topic'] = reg_funds_['county_id'].map(county_topics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_cal = reg_funds_.merge(counties_gdf, on='county_id', how='left')\n",
    "gpd.GeoDataFrame(co_cal, geometry='geometry').to_file(OUT_PATH / 'ca_legislator_funding.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_funds_.to_csv(OUT_PATH / 'ca_legislator_funding.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legislators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei = data[(\"legislator_term\",\"wrote\",\"bill_version\")].edge_index.numpy()\n",
    "ea = data[(\"legislator_term\",\"wrote\",\"bill_version\")].edge_attr.numpy()\n",
    "author_edge = pd.DataFrame({\"legterm_id\": ei[0], \"bill_id\": ei[1], \"type\": ea[:,0]})\n",
    "author_edge['date'] = data[\"bill_version\"].timestamp.numpy()[author_edge.bill_id]\n",
    "author_edge.loc[author_edge.date == 0, 'date'] = datetime.datetime(2000, 6, 15).timestamp()\n",
    "author_edge['date'] = pd.to_datetime(author_edge['date'], unit='s')\n",
    "\n",
    "eib = data[('bill_version','is_version', 'bill')].edge_index.numpy()\n",
    "eib = pd.DataFrame({\"src\": eib[0], \"dst\": eib[1], 'outcome': data['bill'].y[eib[1]]})\n",
    "eib['src'] = eib['src'].astype(int)\n",
    "eib['dst'] = eib['dst'].astype(int)\n",
    "author_edge['bill_id'] = author_edge['bill_id'].astype(int)\n",
    "\n",
    "author_edge = author_edge.merge(eib, left_on='bill_id', right_on='src', how='inner')\n",
    "author_edge['outcome'] = (author_edge['outcome'] == 1).astype(int)\n",
    "author_levels = {1: 'COAUTHOR', 2: 'PRINCIPAL_COAUTHOR', 3: 'LEAD_AUTHOR'}\n",
    "author_edge['author_type'] = author_edge['type'].map(author_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve = data[('bill_version', 'rev_voted_on', 'legislator_term')].edge_index.numpy()\n",
    "va = data[('bill_version', 'rev_voted_on', 'legislator_term')].edge_attr.numpy()\n",
    "vote_edge = pd.DataFrame({'bill_version': ve[0], 'legislator_term': ve[1], 'vote_signal': va[:, 0]})\n",
    "vote_edge = vote_edge.merge(eib, left_on='bill_version', right_on='src', how='left').merge(bv_df, on='bill_version', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_edge['full_name'] = vote_edge['legislator_term'].map(leg_ids).apply(leg_term_to_name)\n",
    "vote_edge['term'] = vote_edge['legislator_term'].map(leg_ids).apply(leg_term_to_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = vote_edge.groupby('bill_id').agg({'outcome': 'max', 'vote_signal': lambda x: (x > 0).sum() / len(x)})\n",
    "signals.loc[(signals['outcome'] == 0.0) & (signals['vote_signal'] == 1.0), 'vote_signal'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = author_edge.merge(bv_df, left_on='bill_id', right_on='bill_version', how='left').groupby('legterm_id').agg({\n",
    "    'outcome': 'mean',\n",
    "    'author_type': lambda x: sum(x == 'LEAD_AUTHOR'),\n",
    "    'bill_version': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "a3['full_name'] = a3['legterm_id'].map(leg_ids).apply(leg_term_to_name)\n",
    "a3['term'] = a3['legterm_id'].map(leg_ids).apply(leg_term_to_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3['full_name'] = a3['full_name'].apply(lambda x: (x.split(',')[1] + ' ' + x.split(',')[0]).strip() if isinstance(x, str) and ',' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4 = a3.merge(lfund, on=['full_name', 'term'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4[['outcome_x', 'author_type_x', 'bill_version_x', 'top_topics',  'full_name', 'term', 'total_donations', 'total_lobbying', 'total_received', 'Party_x', 'chamber_x']].rename(columns={'outcome_x': 'outcome', 'author_type_x': 'author_type', 'bill_version_x': 'bill_versions', 'Party_x': 'Party', 'chamber_x': 'chamber'}).to_csv(OUT_PATH / 'legislator_terms.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donations and Lobbying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "donations = pd.concat([expend_senate.groupby('ExpenderName')['Amount'].sum().reset_index(), expend_assembly.groupby('ExpenderName')['Amount'].sum().reset_index()]).groupby('ExpenderName').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobby = lobbying.groupby('FIRM_NAME')['AMOUNT'].sum().reset_index()\n",
    "lobby['FIRM_NAME'] = lobby['FIRM_NAME'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_don_df(don_align, don_inf, type):\n",
    "    don = don_align[[f'{type}_id', 'top_topics']].merge(don_inf, on=f'{type}_id')\n",
    "    don['name'] = don[f'{type}_id'].map({v: k for k, v in node_id_map[type].items()})\n",
    "    if type == 'donor':\n",
    "        don = don.merge(donations, left_on='name', right_on='ExpenderName', how='left').rename(columns={'Amount': 'total_spent'})\n",
    "    else:\n",
    "        don = don.merge(lobby, left_on='name', right_on='FIRM_NAME', how='left').rename(columns={'AMOUNT': 'total_spent'})\n",
    "    don['total_spent'] = don['total_spent'].fillna(0)\n",
    "    return don\n",
    "\n",
    "donor_df = combine_don_df(donor_align, donor_inf, 'donor')\n",
    "lobby_df = combine_don_df(lobby_align, lobby_inf, 'lobby_firm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "don = donor_df[['name', 'influence', 'total_spent', 'top_topics']].copy()\n",
    "don['type'] = 'donor'\n",
    "lob = lobby_df[['name', 'influence', 'total_spent', 'top_topics']].copy()\n",
    "lob['type'] = 'lobby_firm'\n",
    "donor_lobby = pd.concat([don, lob], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_lobby.to_csv(OUT_PATH / 'donor_lobby_topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_edge['bill'] = data['bill'].n_id[author_edge['dst'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = author_edge.groupby('bill').agg({'date': 'max'}).reset_index().merge(author_edge, on=['bill', 'date'], how='inner').groupby('bill').agg({\n",
    "    'legterm_id': lambda x: ', '.join(x.astype(str).unique())}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_to_names(terms):\n",
    "    term_names = []\n",
    "    for t in terms.split(', '):\n",
    "        l = leg_ids.get(int(t.strip()), None)\n",
    "        if l is not None:\n",
    "            term_names.append(leg_term_to_name(l))\n",
    "    return ', '.join([n for n in term_names if n is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors['authors'] = authors['legterm_id'].apply(terms_to_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = bill_df.merge(authors, left_on='bill_id', right_on='bill', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['longevity'] = b['longevity'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = b.merge(bv_df.groupby('bill_id')['bill_version_id'].nunique().reset_index(), on='bill_id', how='left').merge(bill_dates[['bill_id', 'First_action']], on='bill_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi['term'] = bi['bill_id'].apply(lambda x: x[:4]).astype(int)\n",
    "bi['First_action'] = bi['First_action_x'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "bil = bi.merge(signals, on='bill_id', how='left')\n",
    "bil['vote_signal'] = bil['vote_signal'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "bil['topic'] = bil['bill_id'].map(so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(pa.Table.from_pandas(bil), OUT_PATH / 'bills.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills = pq.read_table(OUT_PATH / 'bills.parquet').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills.to_csv(OUT_PATH / 'bills.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics pt. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = bi.groupby(['topic', 'term']).agg({\n",
    "    'outcome': 'mean',\n",
    "    'controversy': 'mean',\n",
    "    'longevity': 'mean',\n",
    "    'bill_id': 'nunique',\n",
    "    'bill_version_id': 'mean'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_align['full_name'] = leg_align['legislator_id'].apply(lambda x: leg_ids.get(int(x), None)).apply(leg_term_to_name).apply(lambda x: (x.split(',')[1] + ' ' + x.split(',')[0]).strip() if ',' in x else x)\n",
    "leg_align['term'] = leg_align['legislator_id'].apply(lambda x: leg_ids.get(int(x), None)).apply(leg_term_to_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties = pol[['full_name', 'Party']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_party_align = leg_align.merge(parties, on='full_name', how='left')\n",
    "\n",
    "partisanship_by_term = {}\n",
    "\n",
    "for topic in topic_columns:\n",
    "    if topic in ['Miscellaneous']:\n",
    "        continue\n",
    "\n",
    "    term_party_means = leg_party_align.groupby(['term', 'Party'])[topic].mean().unstack(fill_value=0)\n",
    "\n",
    "    for term in term_party_means.index:\n",
    "        if term not in partisanship_by_term:\n",
    "            partisanship_by_term[term] = {}\n",
    "\n",
    "        if len(term_party_means.columns) >= 2:\n",
    "            dem_mean_term = term_party_means.loc[term, 'D'] if 'D' in term_party_means.columns else 0\n",
    "            rep_mean_term = term_party_means.loc[term, 'R'] if 'R' in term_party_means.columns else 0\n",
    "\n",
    "            partisanship_term = abs(dem_mean_term - rep_mean_term)\n",
    "\n",
    "            partisanship_by_term[term][topic] = {\n",
    "                'party_difference': partisanship_term,\n",
    "                'dem_alignment': dem_mean_term,\n",
    "                'rep_alignment': rep_mean_term,\n",
    "                'alignment_ratio': rep_mean_term / (dem_mean_term + 1e-6)\n",
    "            }\n",
    "\n",
    "term_partisanship_data = []\n",
    "for term, topics in partisanship_by_term.items():\n",
    "    for topic, scores in topics.items():\n",
    "        term_partisanship_data.append({\n",
    "            'term': term,\n",
    "            'topic': topic,\n",
    "            **scores\n",
    "        })\n",
    "\n",
    "term_alignments = pd.DataFrame(term_partisanship_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ = t.merge(term_alignments, on=['term', 'topic'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_.to_csv(OUT_PATH / 'topics_agg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "committees = pickle.load(open('../../../committees.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5_actors(align, id):\n",
    "    top5 = defaultdict(list)\n",
    "    for topic in topic_columns:\n",
    "        if topic in ['Miscellaneous']:\n",
    "            continue\n",
    "        results = align.iloc[align[topic].nlargest(5).index.tolist()][id].tolist()\n",
    "        if id != 'full_name':\n",
    "            if id != 'lobby_firm_id':\n",
    "                name = id.split('_')[0]\n",
    "            else:\n",
    "                name = 'lobby_firm'\n",
    "            if id != 'committee_id':\n",
    "                codes = {v: k for k, v in node_id_map[name].items()}\n",
    "            else:\n",
    "                codes = committees\n",
    "            results = [codes[r] for r in results if r in codes]\n",
    "\n",
    "        top5[topic] = results\n",
    "\n",
    "    return top5\n",
    "top_leg = top5_actors(leg_align, 'full_name')\n",
    "top_donors = top5_actors(donor_align, 'donor_id')\n",
    "top_lobby = top5_actors(lobby_align, 'lobby_firm_id')\n",
    "top_committee = top5_actors(committee_align, 'committee_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "topents = pd.DataFrame({\n",
    "    'topic': top_donors.keys(),\n",
    "    'top_donors': top_donors.values(),\n",
    "    'top_lobby': top_lobby.values(),\n",
    "    'top_legislators': top_leg.values()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "topents.to_csv(OUT_PATH / 'top_entities.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
