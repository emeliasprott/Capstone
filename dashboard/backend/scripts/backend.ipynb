{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e48a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, gc, warnings, json, datetime, torch, pickle\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.transforms import ToUndirected, RemoveIsolatedNodes\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9ed898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def herfindahl(shares):\n",
    "    return float(np.square(shares).sum())\n",
    "\n",
    "def entropy(shares, eps=1e-9):\n",
    "    shares = shares.clip(min=eps)\n",
    "    return float(-(shares * np.log(shares)).sum())\n",
    "\n",
    "def jsd(p, q, eps=1e-9):\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (\n",
    "        F.kl_div(m.log(), p, reduction='none').sum(-1) +\n",
    "        F.kl_div(m.log(), q, reduction='none').sum(-1))\n",
    "\n",
    "def slope(y):\n",
    "    if y.size < 2:\n",
    "        return 0.0\n",
    "    x = np.arange(y.size, dtype=np.float32)\n",
    "    return float(np.polyfit(x, y, 1)[0])\n",
    "\n",
    "def safe_ratio(n, d):\n",
    "    return 0.0 if d == 0 else n / d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80122cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_parquet(glob_pat):\n",
    "    files = Path('../data').glob(glob_pat)\n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "bills = union_parquet('bills_kpis_*.parquet')\n",
    "leg_term_kpi = union_parquet('legislator_kpis_*.parquet')\n",
    "committee_kpi = union_parquet('committee_kpis_*.parquet')\n",
    "donor_kpi = union_parquet('donor_kpis_*.parquet')\n",
    "lobby_kpi = union_parquet('lobby_firm_kpis_*.parquet')\n",
    "topic_snapshot = union_parquet('topic_snapshot_*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2ed9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_topic_probs(df, actor_type):\n",
    "    if \"topic_probs\" not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    out = (\n",
    "        df[[\"node_id\", \"topic_probs\"]]\n",
    "        .explode(\"topic_probs\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    out[\"topic_id\"]   = out.groupby(\"node_id\").cumcount()\n",
    "    out[\"topic_prob\"] = out[\"topic_probs\"].astype(float)\n",
    "    out = out.drop(columns=\"topic_probs\")\n",
    "    out[\"actor_type\"] = actor_type\n",
    "    return out\n",
    "SRC = Path('../data')\n",
    "topic_prob_dfs = []\n",
    "for nt in [\"legislator\", \"committee\", \"donor\", \"lobby_firm\"]:\n",
    "    df = union_parquet(f\"*{nt}_topic_probs_*.parquet\")\n",
    "    if not df.empty:\n",
    "        topic_prob_dfs.append(explode_topic_probs(df, nt))\n",
    "actor_topic_long = (\n",
    "    pd.concat(topic_prob_dfs, ignore_index=True) if topic_prob_dfs\n",
    "    else pd.DataFrame(columns=[\"node_id\",\"topic_id\",\"topic_prob\",\"actor_type\"])\n",
    ")\n",
    "actor_topic_long.to_parquet(SRC / \"actor_topic_relevance.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "63dc8bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing bill node count: 13164 -> 45350\n",
      "Fixing legislator node count: 478 -> 508\n"
     ]
    }
   ],
   "source": [
    "def compute_controversiality(data):\n",
    "    edge_type = ('legislator_term', 'voted_on', 'bill_version')\n",
    "    if edge_type not in data.edge_index_dict:\n",
    "        raise ValueError(\"Missing 'voted_on' edges in data.\")\n",
    "\n",
    "    ei = data[edge_type].edge_index\n",
    "    ea = data[edge_type].edge_attr\n",
    "\n",
    "    vote_signal = ea[:, 0]\n",
    "\n",
    "    src_nodes = ei[0]\n",
    "    tgt_nodes = ei[1]\n",
    "\n",
    "    num_bills = data['bill_version'].num_nodes\n",
    "    device = tgt_nodes.device\n",
    "\n",
    "    yes_votes = torch.zeros(num_bills, device=device)\n",
    "    no_votes = torch.zeros(num_bills, device=device)\n",
    "\n",
    "    yes_votes.index_add_(0, tgt_nodes, (vote_signal > 0).float())\n",
    "    no_votes.index_add_(0, tgt_nodes, (vote_signal < 0).float())\n",
    "\n",
    "    total_votes = yes_votes + no_votes + 1e-6\n",
    "\n",
    "    yes_ratio = yes_votes / total_votes\n",
    "    no_ratio = no_votes / total_votes\n",
    "\n",
    "    controversy = 4 * yes_ratio * no_ratio\n",
    "    controversy = controversy.clamp(0, 1)\n",
    "    data['bill_version'].controversy = controversy\n",
    "\n",
    "    return data\n",
    "\n",
    "def safe_normalize_timestamps(timestamps: torch.Tensor) -> torch.Tensor:\n",
    "    timestamps = torch.nan_to_num(timestamps, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "    min_time = timestamps.min()\n",
    "    max_time = timestamps.max()\n",
    "    if (max_time - min_time) < 1e-4:\n",
    "        return torch.zeros_like(timestamps)\n",
    "    return (timestamps - min_time) / (max_time - min_time)\n",
    "\n",
    "def safe_standardize_time_format(time_data) -> torch.Tensor:\n",
    "    times = []\n",
    "    for t in time_data:\n",
    "        try:\n",
    "            if isinstance(t, (int, float)) and 1900 <= t  and t <= 2100:\n",
    "                td = datetime.datetime(int(t), 6, 15).timestamp()\n",
    "            elif (isinstance(t, str) or (isinstance(t, float))) and (float(t) < 2100 and float(t) > 1900):\n",
    "                td = datetime.datetime(int(float(t)), 6, 15).timestamp()\n",
    "            elif float(t) > 0 and float(t) < 1990:\n",
    "                td = t\n",
    "            elif float(t) > 17000000.0:\n",
    "                td = float(t)\n",
    "            elif isinstance(t, datetime.datetime):\n",
    "                td = t.timestamp()\n",
    "            else:\n",
    "                td = float(t) * 1e9\n",
    "        except:\n",
    "            td = datetime.datetime(2000, 6, 15).timestamp()\n",
    "        times.append(td)\n",
    "    return torch.tensor(times, dtype=torch.float32)\n",
    "\n",
    "def pull_timestamps(data):\n",
    "    timestamp_edges = [\n",
    "        ('donor', 'donated_to', 'legislator_term'),\n",
    "        ('legislator_term', 'rev_donated_to', 'donor'),\n",
    "        ('lobby_firm', 'lobbied', 'legislator_term'),\n",
    "        ('lobby_firm', 'lobbied', 'committee'),\n",
    "        ('committee', 'rev_lobbied', 'lobby_firm'),\n",
    "        ('legislator_term', 'rev_lobbied', 'lobby_firm'),\n",
    "        ('bill_version', 'rev_voted_on', 'legislator_term'),\n",
    "        ('legislator_term', 'voted_on', 'bill_version'),\n",
    "    ]\n",
    "    timestamp_nodes = ['legislator_term', 'bill_version', 'bill']\n",
    "\n",
    "    for et in timestamp_edges:\n",
    "        if hasattr(data[et], 'edge_attr') and data[et].edge_attr is not None and len(data[et].edge_attr.size()) > 1:\n",
    "            if data[et].edge_attr.size(1) > 1:\n",
    "                edge_attr = data[et].edge_attr\n",
    "                ts_col = edge_attr[:, -1]\n",
    "                data[et].timestamp = safe_normalize_timestamps(ts_col)\n",
    "                data[et].time = ts_col\n",
    "                data[et].edge_attr = edge_attr[:, :-1]\n",
    "\n",
    "    for nt in timestamp_nodes:\n",
    "        if hasattr(data[nt], 'x') and data[nt].x is not None:\n",
    "            try:\n",
    "                if len(data[nt].x.size()) > 1:\n",
    "                    if data[nt].x.size(1) > 1:\n",
    "                        x = data[nt].x\n",
    "                        ts_col = x[:, -1]\n",
    "                        data[nt].timestamp = safe_normalize_timestamps(ts_col)\n",
    "                        data[nt].x = x[:, :-1]\n",
    "                        data[nt].time = ts_col\n",
    "            except:\n",
    "                pass\n",
    "    return data\n",
    "def clean_features(data):\n",
    "    data = pull_timestamps(data)\n",
    "    for nt in data.node_types:\n",
    "        x = data[nt].x\n",
    "        if not isinstance(x, torch.Tensor) or x.numel() == 0:\n",
    "            data[nt].x = torch.from_numpy(np.vstack(x)).float()\n",
    "            x = data[nt].x\n",
    "        x = torch.nan_to_num(x.float(), nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "        if x.size(0) < 2 or torch.all(x == x[0]):\n",
    "            mean = x.clone()\n",
    "            std = torch.ones_like(x)\n",
    "            x_clean = x.clone()\n",
    "        else:\n",
    "            mean = x.mean(dim=0, keepdim=True)\n",
    "            std = x.std(dim=0, keepdim=True).clamp(min=1e-5)\n",
    "            x_clean = (x - mean) / std\n",
    "            x_clean = x_clean.clamp(-10.0, 10.0)\n",
    "        data[nt].x = x_clean\n",
    "        data[nt].x_mean = mean\n",
    "        data[nt].x_std = std\n",
    "    return data\n",
    "\n",
    "def load_and_preprocess_data(path='../../../GNN/data2.pt'):\n",
    "    full_data = torch.load(path, weights_only=False)\n",
    "    for nt in full_data.node_types:\n",
    "        if hasattr(full_data[nt], 'x') and full_data[nt].x is not None:\n",
    "            full = torch.from_numpy(full_data[nt].x)\n",
    "            s = full.size()\n",
    "            full = torch.flatten(full, start_dim=1, end_dim=-1)\n",
    "            full_data[nt].x = full\n",
    "            full_data[nt].num_nodes = full.size(0)\n",
    "\n",
    "    # Check and fix edge indices before transformation\n",
    "    for edge_type, edge_index in full_data.edge_index_dict.items():\n",
    "        src_type, _, dst_type = edge_type\n",
    "\n",
    "        # Get max node indices\n",
    "        max_src_idx = edge_index[0].max().item() if edge_index.size(1) > 0 else -1\n",
    "        max_dst_idx = edge_index[1].max().item() if edge_index.size(1) > 0 else -1\n",
    "\n",
    "        # Ensure node counts are sufficient\n",
    "        if max_src_idx >= full_data[src_type].num_nodes:\n",
    "            print(f\"Fixing {src_type} node count: {full_data[src_type].num_nodes} -> {max_src_idx + 1}\")\n",
    "            full_data[src_type].num_nodes = max_src_idx + 1\n",
    "\n",
    "        if max_dst_idx >= full_data[dst_type].num_nodes:\n",
    "            print(f\"Fixing {dst_type} node count: {full_data[dst_type].num_nodes} -> {max_dst_idx + 1}\")\n",
    "            full_data[dst_type].num_nodes = max_dst_idx + 1\n",
    "\n",
    "    data = ToUndirected(merge=False)(full_data)\n",
    "    del full_data\n",
    "    gc.collect()\n",
    "    data = RemoveIsolatedNodes()(data)\n",
    "    data = compute_controversiality(clean_features(data))\n",
    "    for store in data.stores:\n",
    "        for key, value in store.items():\n",
    "            if isinstance(value, torch.Tensor) and value.dtype == torch.float64:\n",
    "                store[key] = value.float()\n",
    "    return data\n",
    "\n",
    "data = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "23992f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nt in data.node_types:\n",
    "    data[nt].node_id = torch.arange(data[nt].num_nodes, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b3f14ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2b_edge = next(et for et in data.edge_types\n",
    "                if et[0] == \"bill_version\" and et[2] == \"bill\")\n",
    "src, dst = data[v2b_edge].edge_index.numpy()\n",
    "bv_ts  = data[\"bill_version\"].time.numpy()\n",
    "bv_df  = pd.DataFrame({\"bill_version\": src, \"bill_id\": dst, \"ts\": bv_ts[src]})\n",
    "bill_dates = (\n",
    "    bv_df.groupby(\"bill_id\")[\"ts\"]\n",
    "         .agg(intro_date=\"min\", last_action=\"max\")\n",
    "         .reset_index()\n",
    "         .assign(\n",
    "            intro_date=lambda d: pd.to_datetime(d.intro_date, unit='s'),\n",
    "            last_action=lambda d: pd.to_datetime(d.last_action, unit='s'))\n",
    ")\n",
    "bill = bills.merge(bill_dates, on=\"bill_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8d8088ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill[\"week\"] = bill[\"intro_date\"].dt.to_period(\"W\").dt.start_time\n",
    "bill[\"bill_velocity_days\"] = (bill[\"last_action\"] - bill[\"intro_date\"]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9180f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../node_id_map.json\", \"r\") as f:\n",
    "    node_id_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29b31432",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../committees.pkl\", \"rb\") as f:\n",
    "    committees = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7dd4f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_names = {v:k for k,v in node_id_map[\"committee\"].items()}\n",
    "committee_ids = {}\n",
    "for d in data['committee'].node_id.numpy():\n",
    "    name = com_names.get(int(d), None)\n",
    "    if name is not None:\n",
    "        committee_ids[d] = name\n",
    "committee_ids = pd.DataFrame(committee_ids.items(), columns=[\"committee_id\", \"committee_name\"])\n",
    "committee_ids['term'] = committee_ids['committee_name'].str.split('_').str[1].astype(int)\n",
    "committee_ids['com_id'] = committee_ids['committee_name'].str.split('_').str[0].astype(int)\n",
    "committee_ids['name'] = committee_ids['com_id'].map(committees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "20a780f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "committee = committee_ids.join(committee_kpi, on='committee_id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5794b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_embs = torch.load(\"../../../GNN/policy_embeddings.pt\", map_location='cpu', weights_only=False)\n",
    "T = policy_embs.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59b79105",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_node_id_rep = pd.DataFrame.from_dict({v:k for k,v in node_id_map['bill'].items()}, orient='index').reset_index().rename(columns={0:'bill_id', 'index': 'original_node_id'}).sort_values('original_node_id').reset_index(names='node_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "157721dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "66ec4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_nt = {\"bill\", \"legislator_term\", \"committee\", \"donor\", \"lobby_firm\"}\n",
    "proj = HeteroData()\n",
    "for nt in keep_nt: proj[nt].num_nodes = data[nt].num_nodes\n",
    "proj_edges = [et for et in data.edge_types if et[0] in keep_nt and et[2] in keep_nt and hasattr(data[et], 'edge_index')]\n",
    "for et in proj_edges:\n",
    "    proj[et].edge_index = data[et].edge_index\n",
    "G = to_networkx(proj)\n",
    "pagerank = nx.pagerank(G, alpha=0.9)\n",
    "degree = dict(G.degree())\n",
    "\n",
    "def add_centrality(df, ntype):\n",
    "    df = df.copy()\n",
    "    df[\"pagerank\"] = df.node_id.map(lambda i: pagerank.get((ntype,int(i)),0.0))\n",
    "    df[\"degree\"]   = df.node_id.map(lambda i: degree.get((ntype,int(i)),0))\n",
    "    return df\n",
    "\n",
    "committee = add_centrality(committee, \"committee\")\n",
    "donor = add_centrality(donor_kpi, \"donor\")\n",
    "lobby = add_centrality(lobby_kpi, \"lobby_firm\")\n",
    "leg_term = add_centrality(leg_term_kpi, \"legislator_term\")\n",
    "bill[\"pagerank\"] = bill.bill_id.map(lambda i: pagerank.get((\"bill\",int(i)),0.0))\n",
    "bill[\"degree\"] = bill.bill_id.map(lambda i: degree.get((\"bill\",int(i)),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a8b3f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src, dst = data[(\"bill_version\",\"rev_wrote\",\"legislator_term\")].edge_index.numpy()\n",
    "sponsor_map = pd.DataFrame({\"bill_id\": src, \"node_id\": dst})\n",
    "sponsor_infl = leg_term.set_index(\"node_id\")[\"influence\"]\n",
    "valid_sponsor_map = sponsor_map[sponsor_map[\"node_id\"].isin(sponsor_infl.index)]\n",
    "sponsor_power = (valid_sponsor_map.groupby(\"bill_id\")[\"node_id\"]\n",
    "                                 .agg(lambda ids: sponsor_infl.loc[ids].mean())\n",
    "                                 .rename(\"sponsor_power\"))\n",
    "bill = bill.join(sponsor_power, on=\"bill_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "145ccefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_topk_topics(df, actor_type, K=3):\n",
    "    if actor_topic_long.empty: return df\n",
    "    tk = (actor_topic_long[actor_topic_long.actor_type == actor_type]\n",
    "            .sort_values([\"node_id\",\"topic_prob\"], ascending=[True,False])\n",
    "            .groupby(\"node_id\").head(K))\n",
    "    wide = (tk.set_index([\"node_id\", tk.groupby(\"node_id\").cumcount() + 1])\n",
    "              .unstack(level=1))\n",
    "    wide.columns = [f\"{col[0]}_{col[1]}\" for col in wide.columns]\n",
    "    return df.merge(wide.reset_index(), on=\"node_id\", how=\"left\")\n",
    "\n",
    "committee = attach_topk_topics(committee, \"committee\")\n",
    "donor = attach_topk_topics(donor, \"donor\")\n",
    "lobby = attach_topk_topics(lobby, \"lobby_firm\")\n",
    "leg_term = attach_topk_topics(leg_term, \"legislator_term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cb788a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_df(et):\n",
    "    ei = data[et].edge_index.numpy()\n",
    "    ea = data[et].edge_attr.numpy() if data[et].edge_attr is not None else None\n",
    "    if ea is None or ea.shape[1] == 0:\n",
    "        return pd.DataFrame(columns=[\"src\",\"dst\",\"amount\"])\n",
    "    amt = ea[:,0]\n",
    "    return pd.DataFrame({\"src\": ei[0], \"dst\": ei[1], \"amount\": amt})\n",
    "\n",
    "don_edge = edge_df((\"donor\",\"donated_to\",\"legislator_term\"))\n",
    "don_edge[\"donor_id\"] = don_edge.src\n",
    "don_edge[\"legterm_id\"] = don_edge.dst\n",
    "don_edge[\"type\"] = \"donor\"\n",
    "\n",
    "lob_edge = edge_df((\"lobby_firm\",\"lobbied\",\"legislator_term\"))\n",
    "lob_edge[\"lobby_id\"] = lob_edge.src\n",
    "lob_edge[\"legterm_id\"] = lob_edge.dst\n",
    "lob_edge[\"type\"] = \"lobby\"\n",
    "\n",
    "fund_edge = pd.concat([don_edge, lob_edge], ignore_index=True)\n",
    "term_ts = data[\"legislator_term\"].time.numpy()\n",
    "leg_term[\"session_start\"] = term_ts[leg_term.node_id].astype(int)\n",
    "\n",
    "leg_map = leg_term[[\"node_id\",\"top_topic\",\"session_start\"]].rename(\n",
    "           columns={\"node_id\":\"legterm_id\"})\n",
    "\n",
    "fund_edge = fund_edge.merge(leg_map, on=\"legterm_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8d52a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill['year'] = bill.intro_date.dt.year\n",
    "bill['session'] = bill.apply(lambda x: (x.year - 1 if x.year % 2 == 0 else x.year) if pd.notnull(x.year) else np.nan, axis=1).astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e39b4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_weekly = (\n",
    "    bill.groupby([\"dominant_topic\",\"session\"])\n",
    "        .agg(n_bills=(\"bill_id\",\"size\"),\n",
    "             avg_polar=(\"polarisation_score\",\"mean\"))\n",
    "        .reset_index()\n",
    "        .rename(columns={\"dominant_topic\":\"topic\"})\n",
    ")\n",
    "\n",
    "fund_weekly = (fund_edge.groupby([\"top_topic\",\"session_start\"])\n",
    "                        .agg(total_funding=(\"amount\",\"sum\"))\n",
    "                        .reset_index()\n",
    "                        .rename(columns={\"top_topic\":\"topic\", 'session_start':\"session\"}))\n",
    "\n",
    "policy_weekly = policy_weekly.merge(fund_weekly, on=[\"topic\",\"session\"], how=\"left\")\n",
    "policy_weekly[\"total_funding\"]   = policy_weekly.total_funding.fillna(0.0)\n",
    "policy_weekly[\"fund_volatility\"] = (policy_weekly.sort_values(\"session\")\n",
    "                                      .groupby(\"topic\")[\"total_funding\"]\n",
    "                                      .transform(lambda s: s.rolling(4,min_periods=2).std()))\n",
    "\n",
    "policy_weekly.to_parquet(SRC/\"policy_session.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8d31a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_base = (bill.groupby(\"dominant_topic\")\n",
    "                 .agg(n_bills=(\"bill_id\",\"size\"),\n",
    "                      avg_success=(\"success_risk\",\"mean\"),\n",
    "                      avg_polar=(\"polarisation_score\",\"mean\"),\n",
    "                      avg_velocity=(\"bill_velocity_days\",\"mean\"),\n",
    "                      avg_sponsor_power=(\"sponsor_power\",\"mean\"))\n",
    "                 .reset_index()\n",
    "                 .rename(columns={\"dominant_topic\":\"topic\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "701d6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_funding = (fund_edge.groupby(\"top_topic\")\n",
    "                           .agg(total_dollars=(\"amount\",\"sum\"))\n",
    "                           .rename(columns={\"top_topic\":\"topic\"})\n",
    "                           .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d1e0010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope(y):\n",
    "    if len(y)<2: return 0.0\n",
    "    x = np.arange(len(y)); return np.polyfit(x,y,1)[0]\n",
    "\n",
    "pol_slope = (policy_weekly.groupby(\"topic\")[\"avg_polar\"]\n",
    "                        .apply(slope).reset_index()\n",
    "                        .rename(columns={\"avg_polar\":\"polarisation_slope\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "73f96698",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_infl = pd.concat([\n",
    "    donor[[\"top_topic\",\"influence\"]],\n",
    "    lobby[[\"top_topic\",\"influence\"]],\n",
    "    committee[[\"top_topic\",\"influence\"]],\n",
    "    leg_term[[\"top_topic\",\"influence\"]]],\n",
    "    ignore_index=True).dropna()\n",
    "\n",
    "power_conc = (actor_infl.groupby(\"top_topic\")\n",
    "                        .agg(power_concentration=(\"influence\", herfindahl))\n",
    "                        .rename(columns={\"top_topic\":\"topic\"})\n",
    "                        .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9434c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "bip_gap = (bill.assign(bipart = lambda d: d.polarisation_score<0.25)\n",
    "               .groupby(\"dominant_topic\")[\"bipart\"]\n",
    "               .agg(lambda s: s.mean()*2 -1)\n",
    "               .reset_index()\n",
    "               .rename(columns={\"dominant_topic\":\"topic\",\n",
    "                                \"bipart\":\"bipartisan_gap\"}))\n",
    "topic_funding.rename(columns={\"top_topic\":\"topic\"}, inplace=True)\n",
    "power_conc.rename(columns={\"top_topic\":\"topic\"}, inplace=True)\n",
    "topic_summary = (topic_base\n",
    "                 .merge(topic_funding, on=\"topic\", how=\"left\")\n",
    "                 .merge(power_conc,   on=\"topic\", how=\"left\")\n",
    "                 .merge(bip_gap,      on=\"topic\", how=\"left\")\n",
    "                 .merge(pol_slope,    on=\"topic\", how=\"left\")\n",
    "                 .merge(topic_snapshot[[\"topic_id\",\"recent_momentum\",\"power_balance\"]],\n",
    "                        left_on=\"topic\", right_on=\"topic_id\", how=\"left\")\n",
    "                 .drop(columns=\"topic_id\")\n",
    "                 .fillna({\"total_dollars\":0.0}))\n",
    "\n",
    "topic_summary.to_parquet(SRC/\"topic_summary.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3ceb3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_ids = {v: k for k, v in node_id_map['legislator_term'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8193df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislators = pickle.load(open('../../../legislators.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a9464119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leg_term_to_name(leg_term_id):\n",
    "    if isinstance(leg_term_id, str):\n",
    "        num = int(leg_term_id.split('_')[0])\n",
    "        return legislators.get(num, None)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3c622de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def legislator_node_matching(node_id):\n",
    "    n = node_id_map['legislator'].get(str(node_id), None)\n",
    "    if n is not None:\n",
    "        name = legislators.get(n, None)\n",
    "        if name is not None:\n",
    "            return name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1705a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_term['name'] = leg_term['node_id'].map(leg_ids).apply(leg_term_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f7387655",
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_term.to_parquet(SRC/\"legislator_term.parquet\", index=False)\n",
    "donor.to_parquet(SRC/\"donor.parquet\", index=False)\n",
    "lobby.to_parquet(SRC/\"lobby_firm.parquet\", index=False)\n",
    "committee.to_parquet(SRC/\"committee.parquet\", index=False)\n",
    "bill.to_parquet(SRC/\"bill.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ca9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "src, dst = data[(\"bill\",\"sponsored_by\",\"legislator_term\")].edge_index.numpy()\n",
    "s_tbl = (pd.DataFrame({\"bill_id\":src,\"node_id\":dst})\n",
    "            .merge(leg_term[[\"node_id\",\"name\"]], on=\"node_id\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
