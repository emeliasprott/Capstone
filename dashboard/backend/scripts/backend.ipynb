{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e48a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, gc, warnings, json, datetime, torch\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.transforms import ToUndirected, RemoveIsolatedNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9ed898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def herfindahl(shares):\n",
    "    return float(np.square(shares).sum())\n",
    "\n",
    "def entropy(shares, eps=1e-9):\n",
    "    shares = shares.clip(min=eps)\n",
    "    return float(-(shares * np.log(shares)).sum())\n",
    "\n",
    "def jsd(p, q, eps=1e-9):\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (\n",
    "        F.kl_div(m.log(), p, reduction='none').sum(-1) +\n",
    "        F.kl_div(m.log(), q, reduction='none').sum(-1))\n",
    "\n",
    "def slope(y):\n",
    "    if y.size < 2:\n",
    "        return 0.0\n",
    "    x = np.arange(y.size, dtype=np.float32)\n",
    "    return float(np.polyfit(x, y, 1)[0])\n",
    "\n",
    "def safe_ratio(n, d):\n",
    "    return 0.0 if d == 0 else n / d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63dc8bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing bill node count: 13164 -> 45350\n",
      "Fixing legislator node count: 478 -> 508\n"
     ]
    }
   ],
   "source": [
    "def compute_controversiality(data):\n",
    "    edge_type = ('legislator_term', 'voted_on', 'bill_version')\n",
    "    if edge_type not in data.edge_index_dict:\n",
    "        raise ValueError(\"Missing 'voted_on' edges in data.\")\n",
    "\n",
    "    ei = data[edge_type].edge_index\n",
    "    ea = data[edge_type].edge_attr\n",
    "\n",
    "    vote_signal = ea[:, 0]\n",
    "\n",
    "    src_nodes = ei[0]\n",
    "    tgt_nodes = ei[1]\n",
    "\n",
    "    num_bills = data['bill_version'].num_nodes\n",
    "    device = tgt_nodes.device\n",
    "\n",
    "    yes_votes = torch.zeros(num_bills, device=device)\n",
    "    no_votes = torch.zeros(num_bills, device=device)\n",
    "\n",
    "    yes_votes.index_add_(0, tgt_nodes, (vote_signal > 0).float())\n",
    "    no_votes.index_add_(0, tgt_nodes, (vote_signal < 0).float())\n",
    "\n",
    "    total_votes = yes_votes + no_votes + 1e-6\n",
    "\n",
    "    yes_ratio = yes_votes / total_votes\n",
    "    no_ratio = no_votes / total_votes\n",
    "\n",
    "    controversy = 4 * yes_ratio * no_ratio\n",
    "    controversy = controversy.clamp(0, 1)\n",
    "    data['bill_version'].controversy = controversy\n",
    "\n",
    "    return data\n",
    "\n",
    "def safe_normalize_timestamps(timestamps: torch.Tensor) -> torch.Tensor:\n",
    "    timestamps = torch.nan_to_num(timestamps, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "    min_time = timestamps.min()\n",
    "    max_time = timestamps.max()\n",
    "    if (max_time - min_time) < 1e-4:\n",
    "        return torch.zeros_like(timestamps)\n",
    "    return (timestamps - min_time) / (max_time - min_time)\n",
    "\n",
    "def safe_standardize_time_format(time_data) -> torch.Tensor:\n",
    "    times = []\n",
    "    for t in time_data:\n",
    "        try:\n",
    "            if isinstance(t, (int, float)) and 1900 <= t  and t <= 2100:\n",
    "                td = datetime.datetime(int(t), 6, 15).timestamp()\n",
    "            elif (isinstance(t, str) or (isinstance(t, float))) and (float(t) < 2100 and float(t) > 1900):\n",
    "                td = datetime.datetime(int(float(t)), 6, 15).timestamp()\n",
    "            elif float(t) > 0 and float(t) < 1990:\n",
    "                td = t\n",
    "            elif float(t) > 17000000.0:\n",
    "                td = float(t)\n",
    "                while td > 10:\n",
    "                    td = td / 10\n",
    "            elif float(t) < 0:\n",
    "                td = -float(t)\n",
    "            else:\n",
    "                td = t.timestamp()\n",
    "        except:\n",
    "            td = datetime.datetime(2000, 6, 15).timestamp()\n",
    "        times.append(td)\n",
    "    return torch.tensor(times, dtype=torch.float32)\n",
    "\n",
    "def pull_timestamps(data):\n",
    "    timestamp_edges = [\n",
    "        ('donor', 'donated_to', 'legislator_term'),\n",
    "        ('legislator_term', 'rev_donated_to', 'donor'),\n",
    "        ('lobby_firm', 'lobbied', 'legislator_term'),\n",
    "        ('lobby_firm', 'lobbied', 'committee'),\n",
    "        ('committee', 'rev_lobbied', 'lobby_firm'),\n",
    "        ('legislator_term', 'rev_lobbied', 'lobby_firm'),\n",
    "        ('bill_version', 'rev_voted_on', 'legislator_term'),\n",
    "        ('legislator_term', 'voted_on', 'bill_version'),\n",
    "    ]\n",
    "    timestamp_nodes = ['legislator_term', 'bill_version', 'bill', 'committee']\n",
    "\n",
    "    for et in timestamp_edges:\n",
    "        if hasattr(data[et], 'edge_attr') and data[et].edge_attr is not None and len(data[et].edge_attr.size()) > 1:\n",
    "            if data[et].edge_attr.size(1) > 1:\n",
    "                edge_attr = data[et].edge_attr\n",
    "                ts_col = edge_attr[:, -1]\n",
    "                if ts_col.abs().max() > 1e8 or ts_col.min() < 0:\n",
    "                    ts_col = safe_standardize_time_format(ts_col.tolist()).to(edge_attr.device)\n",
    "                data[et].timestamp = safe_normalize_timestamps(ts_col)\n",
    "                data[et].edge_attr = edge_attr[:, :-1]\n",
    "\n",
    "    for nt in timestamp_nodes:\n",
    "        if hasattr(data[nt], 'x') and data[nt].x is not None:\n",
    "            try:\n",
    "                if len(data[nt].x.size()) > 1:\n",
    "                    if data[nt].x.size(1) > 1:\n",
    "                        x = data[nt].x\n",
    "                        ts_col = x[:, -1]\n",
    "                        if ts_col.abs().max() > 1e8 or ts_col.min() < 0:\n",
    "                            ts_col = safe_standardize_time_format(ts_col.tolist()).to(x.device)\n",
    "                        if nt in timestamp_nodes or ts_col.abs().max() > 1e6:\n",
    "                            data[nt].timestamp = safe_normalize_timestamps(ts_col)\n",
    "                            data[nt].x = x[:, :-1]\n",
    "            except:\n",
    "                pass\n",
    "    return data\n",
    "def clean_features(data):\n",
    "    for nt in data.node_types:\n",
    "        x = data[nt].x\n",
    "        if not isinstance(x, torch.Tensor) or x.numel() == 0:\n",
    "            data[nt].x = torch.from_numpy(np.vstack(x)).float()\n",
    "            x = data[nt].x\n",
    "        x = torch.nan_to_num(x.float(), nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "        if x.size(0) < 2 or torch.all(x == x[0]):\n",
    "            mean = x.clone()\n",
    "            std = torch.ones_like(x)\n",
    "            x_clean = x.clone()\n",
    "        else:\n",
    "            mean = x.mean(dim=0, keepdim=True)\n",
    "            std = x.std(dim=0, keepdim=True).clamp(min=1e-5)\n",
    "            x_clean = (x - mean) / std\n",
    "            x_clean = x_clean.clamp(-10.0, 10.0)\n",
    "        data[nt].x = x_clean\n",
    "        data[nt].x_mean = mean\n",
    "        data[nt].x_std = std\n",
    "    data = pull_timestamps(data)\n",
    "    return data\n",
    "\n",
    "def load_and_preprocess_data(path='../../../GNN/data2.pt'):\n",
    "    full_data = torch.load(path, weights_only=False)\n",
    "    for nt in full_data.node_types:\n",
    "        if hasattr(full_data[nt], 'x') and full_data[nt].x is not None:\n",
    "            full = torch.from_numpy(full_data[nt].x)\n",
    "            s = full.size()\n",
    "            full = torch.flatten(full, start_dim=1, end_dim=-1)\n",
    "            full_data[nt].x = full\n",
    "            full_data[nt].num_nodes = full.size(0)\n",
    "\n",
    "    # Check and fix edge indices before transformation\n",
    "    for edge_type, edge_index in full_data.edge_index_dict.items():\n",
    "        src_type, _, dst_type = edge_type\n",
    "\n",
    "        # Get max node indices\n",
    "        max_src_idx = edge_index[0].max().item() if edge_index.size(1) > 0 else -1\n",
    "        max_dst_idx = edge_index[1].max().item() if edge_index.size(1) > 0 else -1\n",
    "\n",
    "        # Ensure node counts are sufficient\n",
    "        if max_src_idx >= full_data[src_type].num_nodes:\n",
    "            print(f\"Fixing {src_type} node count: {full_data[src_type].num_nodes} -> {max_src_idx + 1}\")\n",
    "            full_data[src_type].num_nodes = max_src_idx + 1\n",
    "\n",
    "        if max_dst_idx >= full_data[dst_type].num_nodes:\n",
    "            print(f\"Fixing {dst_type} node count: {full_data[dst_type].num_nodes} -> {max_dst_idx + 1}\")\n",
    "            full_data[dst_type].num_nodes = max_dst_idx + 1\n",
    "\n",
    "    data = ToUndirected(merge=False)(full_data)\n",
    "    del full_data\n",
    "    gc.collect()\n",
    "    data = RemoveIsolatedNodes()(data)\n",
    "    data = compute_controversiality(clean_features(data))\n",
    "    for store in data.stores:\n",
    "        for key, value in store.items():\n",
    "            if isinstance(value, torch.Tensor) and value.dtype == torch.float64:\n",
    "                store[key] = value.float()\n",
    "    return data\n",
    "\n",
    "data = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_data_dir = \"../data\"\n",
    "bills_kpis = pd.read_parquet(f\"{back_data_dir}/bill_kpis.parquet\")\n",
    "legislator_kpis = pd.read_parquet(f\"{back_data_dir}/legislator_kpis.parquet\")\n",
    "committee_kpis = pd.read_parquet(f\"{back_data_dir}/committee_kpis.parquet\")\n",
    "donor_kpis = pd.read_parquet(f\"{back_data_dir}/donor_kpis.parquet\")\n",
    "lobby_firm_kpis = pd.read_parquet(f\"{back_data_dir}/lobby_firm_kpis.parquet\")\n",
    "topic_snapshot = pd.read_parquet(f\"{back_data_dir}/topic_snapshot.parquet\")\n",
    "policy_embs = torch.load(\"../../../GNN/policy_embeddings.pt\", map_location='cpu', weights_only=False)\n",
    "T = policy_embs.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be752c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2b = data[('bill_version','is_version','bill')].edge_index.numpy()\n",
    "bv_ids, bill_ids = v2b[0], v2b[1]\n",
    "\n",
    "cont_df = pl.DataFrame({\n",
    "        'bill_id': bill_ids,\n",
    "        'controversy': data['bill_version'].controversy[bv_ids].cpu().numpy()\n",
    "        }).group_by('bill_id').agg(\n",
    "    pl.col('controversy').mean().alias('avg_controversy')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab879a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_96145/4256062050.py:5: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  ).drop_nulls().group_by('bill_id').agg(pl.count().alias('amendment_count'))\n"
     ]
    }
   ],
   "source": [
    "# amendment count\n",
    "pv = data[('bill_version','priorVersion','bill_version')].edge_index[0].cpu().numpy()\n",
    "amend_df = pl.DataFrame({'bv': pv}).with_columns(\n",
    "\tpl.col('bv').map_elements(lambda x: v2b[1][list(v2b[0]).index(x)] if x in v2b[0] else None, return_dtype=int).alias('bill_id')\n",
    ").drop_nulls().group_by('bill_id').agg(pl.count().alias('amendment_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c575163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vote count\n",
    "vote_idx = data[('legislator_term','voted_on','bill_version')].edge_index[1].cpu().numpy()\n",
    "vote_df = pl.DataFrame({'bv': vote_idx}).with_columns(\n",
    "\tpl.col('bv').map_elements(lambda x: v2b[1][list(v2b[0]).index(x)] if x in v2b[0] else None, return_dtype=int).alias('bill_id')\n",
    ").drop_nulls().groupby('bill_id').agg(pl.count().alias('vote_count'))\n",
    "# avg vote margin\n",
    "vote_attr = data[('legislator_term','voted_on','bill_version')].edge_attr.cpu().numpy()\n",
    "margins = vote_attr[:, 0]\n",
    "vote_margin_df = pl.DataFrame({\n",
    "\t'bv': vote_idx,\n",
    "\t'margin': margins\n",
    "}).with_columns(\n",
    "\tpl.col('bv').map_elements(lambda x: v2b[1][list(v2b[0]).index(x)] if x in v2b[0] else None, return_dtype=int).alias('bill_id')\n",
    ").drop_nulls().groupby('bill_id').agg(pl.col('margin').mean().alias('avg_vote_margin'))\n",
    "# join all\n",
    "bill_kpis = (cont_df\n",
    "\t.join(amend_df, on='bill_id', how='outer')\n",
    "\t.join(vote_df, on='bill_id', how='outer')\n",
    "\t.join(vote_margin_df, on='bill_id', how='outer')\n",
    "\t.fill_null(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_ratio(n, d):\n",
    "    return 0.0 if d == 0 else n / d\n",
    "\n",
    "lt2leg = dict(zip(*data[('legislator','samePerson','legislator_term')].edge_index.cpu().numpy()))\n",
    "# donation events\n",
    "d_ei = data[('donor','donated_to','legislator_term')].edge_index.cpu().numpy()\n",
    "d_attr = data[('donor','donated_to','legislator_term')].edge_attr.cpu().numpy()\n",
    "don_df = pl.DataFrame({\n",
    "    'lt': d_ei[1],\n",
    "    'amt': d_attr[:,0],\n",
    "    'ts': d_attr[:,1]\n",
    "}).with_columns(\n",
    "    pl.col('lt').apply(lambda x: lt2leg.get(int(x))).alias('leg_id'),\n",
    "    pl.col('ts').cast(pl.Datetime(time_unit='s')).dt.year().alias('year')\n",
    ").drop_nulls()\n",
    "don_tot = don_df.groupby(['leg_id','year']).agg(pl.col('amt').sum().alias('donation_total'))\n",
    "don_cnt = don_df.groupby(['leg_id','year']).agg(pl.count().alias('donation_count'))\n",
    "# lobbying events\n",
    "l_ei = data[('lobby_firm','lobbied','legislator_term')].edge_index.cpu().numpy()\n",
    "l_attr = data[('lobby_firm','lobbied','legislator_term')].edge_attr.cpu().numpy()\n",
    "lobby_df = pl.DataFrame({\n",
    "    'lf': l_ei[0],\n",
    "    'lt': l_ei[1],\n",
    "    'amt': l_attr[:,0],\n",
    "    'ts': l_attr[:,1]\n",
    "}).with_columns(\n",
    "    pl.col('lt').apply(lambda x: lt2leg.get(int(x))).alias('leg_id'),\n",
    "    pl.col('ts').cast(pl.Datetime(time_unit='s')).dt.year().alias('year')\n",
    ").drop_nulls()\n",
    "lobby_tot = lobby_df.groupby(['leg_id','year']).agg(pl.col('amt').sum().alias('lobby_total'))\n",
    "lobby_cnt = lobby_df.groupby(['leg_id','year']).agg(pl.count().alias('lobby_count'))\n",
    "# vote events\n",
    "v_ei = data[('legislator_term','voted_on','bill_version')].edge_index.cpu().numpy()\n",
    "v_attr = data[('legislator_term','voted_on','bill_version')].edge_attr.cpu().numpy()\n",
    "vote_df = pl.DataFrame({\n",
    "    'lt': v_ei[0],\n",
    "    'ts': v_attr[:,1]\n",
    "}).with_columns(\n",
    "    pl.col('lt').apply(lambda x: lt2leg.get(int(x))).alias('leg_id'),\n",
    "    pl.col('ts').cast(pl.Datetime(time_unit='s')).dt.year().alias('year')\n",
    ").drop_nulls()\n",
    "vote_cnt = vote_df.groupby(['leg_id','year']).agg(pl.count().alias('votes_cast'))\n",
    "# committee membership\n",
    "m_ei = data[('legislator_term','member_of','committee')].edge_index.cpu().numpy()\n",
    "m_attr = data[('legislator_term','member_of','committee')].edge_attr.cpu().numpy()\n",
    "mem_df = pl.DataFrame({\n",
    "    'lt': m_ei[0],\n",
    "    'ts': m_attr[:,1]\n",
    "}).with_columns(\n",
    "    pl.col('lt').apply(lambda x: lt2leg.get(int(x))).alias('leg_id'),\n",
    "    pl.col('ts').cast(pl.Datetime(time_unit='s')).dt.year().alias('year')\n",
    ").drop_nulls()\n",
    "mem_cnt = mem_df.groupby(['leg_id','year']).agg(pl.count().alias('committee_memberships'))\n",
    "# authored bills\n",
    "a_ei = data[('legislator_term','wrote','bill_version')].edge_index.cpu().numpy()\n",
    "a_attr = data[('legislator_term','wrote','bill_version')].edge_attr.cpu().numpy()\n",
    "auth_df = pl.DataFrame({\n",
    "    'lt': a_ei[0],\n",
    "    'ts': a_attr[:,1]\n",
    "}).with_columns(\n",
    "    pl.col('lt').apply(lambda x: lt2leg.get(int(x))).alias('leg_id'),\n",
    "    pl.col('ts').cast(pl.Datetime(time_unit='s')).dt.year().alias('year')\n",
    ").drop_nulls()\n",
    "auth_cnt = auth_df.groupby(['leg_id','year']).agg(pl.count().alias('bills_authored'))\n",
    "# combine all per-leg, per-year\n",
    "leg_kpis = (don_tot\n",
    "    .join(don_cnt, on=['leg_id','year'], how='outer')\n",
    "    .join(lobby_tot, on=['leg_id','year'], how='outer')\n",
    "    .join(lobby_cnt, on=['leg_id','year'], how='outer')\n",
    "    .join(vote_cnt, on=['leg_id','year'], how='outer')\n",
    "    .join(mem_cnt, on=['leg_id','year'], how='outer')\n",
    "    .join(auth_cnt, on=['leg_id','year'], how='outer')\n",
    "    .fill_null(0)\n",
    "    .with_columns((pl.col('donation_total')/pl.col('donation_count')).apply(lambda x: _safe_ratio(x,1.0)).alias('avg_donation'))\n",
    "    .with_columns((pl.col('lobby_total')/pl.col('lobby_count')).apply(lambda x: _safe_ratio(x,1.0)).alias('avg_lobby'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dbe925",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_ei = data[('bill_version', 'priorVersion', 'bill_version')].edge_index.numpy()\n",
    "\n",
    "am_cnt, am_gap = defaultdict(int), defaultdict(float)\n",
    "gap_tmp  = defaultdict(list)\n",
    "for src_bv, dst_bv in zip(*pv_ei):\n",
    "    b = v2b.get(int(src_bv))\n",
    "    if b is None:\n",
    "        continue\n",
    "    am_cnt[b] += 1\n",
    "    ts = data['bill_version'].timestamp[int(src_bv)].item()\n",
    "    gap_tmp[b].append(float(ts))\n",
    "\n",
    "for b, times in gap_tmp.items():\n",
    "    times = np.sort(times)\n",
    "    am_gap[b] = np.diff(times).mean()/86400 if len(times) > 1 else np.nan\n",
    "\n",
    "bill_kpis['amendment_avg_gap_days'] = bill_kpis['bill_id'].map(am_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64df4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_series = bill_kpis.set_index('bill_id')['dominant_topic'].to_dict()\n",
    "am_topic_ent = {\n",
    "    b: entropy(np.bincount([topic_series.get(b, 0)], minlength=T) / 1)\n",
    "    for b in bill_kpis['bill_id']\n",
    "}\n",
    "bill_kpis['amendment_topic_entropy'] = bill_kpis['bill_id'].map(am_topic_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "725cb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt2comm = defaultdict(set)\n",
    "for lt,c in zip(*data[('legislator_term','member_of','committee')].edge_index.numpy()):\n",
    "    lt2comm[int(lt)].add(int(c))\n",
    "leg_inf = dict(zip(legislator_kpis[\"node_id\"],legislator_kpis[\"influence\"]))\n",
    "comm_lever=defaultdict(float)\n",
    "for lt,comms in lt2comm.items():\n",
    "    inf=leg_inf.get(lt2leg.get(lt),0)\n",
    "    for c in comms: comm_lever[c]+=inf\n",
    "committee_kpis[\"member_leverage_sum\"]=committee_kpis[\"node_id\"].map(comm_lever).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add12600",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pol = bill_kpis.groupby(\"dominant_topic\")[\"polarisation_score\"].mean().to_dict()\n",
    "topic_snapshot[\"avg_polarisation\"]=topic_snapshot[\"topic_id\"].map(avg_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7addfc2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4b3e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_kpis.to_parquet(f\"{back_data_dir}/bill_kpis.parquet\",compression='zstd',index=False)\n",
    "legislator_kpis.to_parquet(f\"{back_data_dir}/legislator_kpis.parquet\",compression='zstd',index=False)\n",
    "committee_kpis.to_parquet(f\"{back_data_dir}/committee_kpis.parquet\",compression='zstd',index=False)\n",
    "donor_kpis.to_parquet(f\"{back_data_dir}/donor_kpis.parquet\",compression='zstd',index=False)\n",
    "lobby_firm_kpis.to_parquet(f\"{back_data_dir}/lobby_firm_kpis.parquet\",compression='zstd',index=False)\n",
    "topic_snapshot.to_parquet(f\"{back_data_dir}/topic_snapshot.parquet\",compression='zstd',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2651cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
