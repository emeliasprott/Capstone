{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch, datetime, hashlib, json, re, warnings\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_votes = pd.read_csv('ca_leg/legislation_data/bill_summary_vote_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_history = pd.read_csv('ca_leg/legislation_data/bill_history_tbl.csv', dtype={'action_status': str, 'primary_location': str, 'secondary_location': str, 'end_status': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('ca_leg/legislation_data/authors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv('ca_leg/legislation_data/history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions = pd.read_csv('ca_leg/legislation_data/bill_versions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes = pd.read_csv('ca_leg/legislation_data/bill_detail_vote_tbl.csv', parse_dates=['session_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_summary = pd.read_csv('ca_leg/legislation_data/bill_summary_vote_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_motions = pd.read_csv('ca_leg/legislation_data/bill_motion_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv('ca_leg/legislation_data/committee_codes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians = pd.read_csv('ca_leg/legislation_data/politicians.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobbying = pd.read_csv('calaccess/lobbying_clean2.csv', dtype={'PAYEE_NAMS': str, 'BAKREF_TID': str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "expend_assembly = pd.read_csv('calaccess/expend_assembly_matched.csv', dtype={'TargetPropositionName': str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "expend_senate = pd.read_csv('calaccess/expend_senate_matched.csv', dtype={'TargetPropositionName': str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "digests = pd.read_csv('ca_leg/legislation_data/digest.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hearings = pd.read_csv('ca_leg/legislation_data/committee_hearing_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_expenditures = pd.read_csv(\"calaccess/Independent-Expenditure-Jan-30th-26.csv\")\n",
    "independent_expenditures = independent_expenditures.loc[independent_expenditures['TargetPropositionName'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes['vote_date_time'] = pd.to_datetime(bill_votes['vote_date_time']).apply(lambda x: x.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes['legislator_name'] = bill_votes['legislator_name'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_KEYWORDS = [\n",
    "    \"Assembly Third Reading\", \"Assembly 3rd reading\", 'senate 3rd reading', \"Senate Third Reading\",\"Concurrence - Urgency Added\", \"Concurrence in Senate Amendments\", \"Do pass as amended, and re-refer\", \"Do pass as amended, but re-refer\", \"Do pass as amended\", \"Do pass and be re-referred\",\n",
    "    \"Concurrence\", \"Consent Calendar\", \"Urgency Clause\", \"Special Consent\",\n",
    "    \"Motion to Reconsider\", \"Do pass\", \"Reconsideration\", \"Committee amendments\",\n",
    "    \"W/O REF. TO FILE\", \"Be re-referred to the Committee\",\n",
    "    \"Lay on the Table\", \"Amend by\", \"Unfinished Business\", \"Placed on Appropriations Suspense File\",\n",
    "]\n",
    "\n",
    "def extract_action(motion_text):\n",
    "    if not isinstance(motion_text, str) or motion_text is None:\n",
    "        return None\n",
    "    motion = motion_text.upper()\n",
    "\n",
    "    action = next((act for act in ACTION_KEYWORDS if act.upper() in motion), None)\n",
    "    if action != 'Reconsideration' and 'RECONSIDER' in motion:\n",
    "        if action is not None:\n",
    "            action += ' Reconsideration'\n",
    "        else:\n",
    "            action = 'Reconsideration'\n",
    "\n",
    "\n",
    "    return action if action else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_motions['simplified_motion'] = bill_motions['motion_text'].apply(extract_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_coms = {}\n",
    "for i, row in locations.iterrows():\n",
    "    if row['committee_code'].startswith('CZ'):\n",
    "        continue\n",
    "    name = row['committee_name']\n",
    "    if row['committee_code'].startswith('CS'):\n",
    "        if name.startswith('Sen.'):\n",
    "            cname = re.sub(r'Sen. ', 'senate ', name).lower()\n",
    "        elif name.startswith('Senate '):\n",
    "            cname = name.lower()\n",
    "        else:\n",
    "            cname = 'senate ' + name.lower()\n",
    "    elif row['committee_code'].startswith('CX'):\n",
    "        if name.lower().startswith('assembly'):\n",
    "            cname = name.lower()\n",
    "        else:\n",
    "            cname = 'assembly ' + name.lower()\n",
    "    if re.search(r'x\\d$', cname) is not None:\n",
    "        cname = re.sub(r'x(?=\\d$)', 'no. ', cname)\n",
    "    clean_coms[row['committee_code']] = cname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "leg_committees = [f\"{row['chamber']} {row['committee_clean']}\".lower() for _, row in politicians[['committee_clean', 'chamber']].drop_duplicates().iterrows()]\n",
    "\n",
    "def match_committees(_names, clean_coms, threshold=92):\n",
    "    clean_c = list(clean_coms.values())\n",
    "    clean_codes = list(clean_coms.keys())\n",
    "    name_mapping = {}\n",
    "    for i, clean in enumerate(clean_c):\n",
    "        code = clean_codes[i]\n",
    "        matches = []\n",
    "        matches.append(process.extractOne(\n",
    "            clean,\n",
    "            _names,\n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            score_cutoff=threshold\n",
    "        ))\n",
    "        matches.append(process.extractOne(\n",
    "            clean,\n",
    "            _names,\n",
    "            scorer=fuzz.partial_ratio,\n",
    "            score_cutoff=threshold\n",
    "        ))\n",
    "        valid_matches = [m for m in matches if m is not None]\n",
    "        if len(valid_matches) > 0:\n",
    "            best_match = max(valid_matches, key=lambda x: x[1])\n",
    "            name_mapping[code] = best_match[0]\n",
    "        else:\n",
    "            fall_back = process.extractOne(\n",
    "                clean,\n",
    "                _names,\n",
    "                scorer=fuzz.token_sort_ratio,\n",
    "                score_cutoff=threshold - 8\n",
    "            )\n",
    "            if fall_back is not None:\n",
    "                name_mapping[code] = fall_back[0]\n",
    "            else:\n",
    "                name_mapping[code] = None\n",
    "    return name_mapping\n",
    "\n",
    "committee_matches = match_committees(leg_committees, clean_coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations['committee_clean'] = locations['committee_code'].map(committee_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.loc[locations['committee_name'] == 'EDUCATION X5', 'committee_clean'] = 'Budget and Fiscal Review: Education'\n",
    "locations.loc[locations['committee_code'] == 'CX12', 'committee_clean'] = 'Budget No. 1 on Health and Human Services'\n",
    "locations.loc[locations['committee_code'] == 'CS68', 'committee_clean'] = 'Budget No. 3 - Health and Human Services'\n",
    "locations.loc[locations['committee_code'] == 'CS66', 'committee_clean'] = 'Senate Veterans Affairs'\n",
    "locations.loc[locations['committee_code'] == 'CS56', 'committee_clean'] = 'Senate Public Employment and Retirement'\n",
    "locations.loc[locations['committee_code'] == 'CS62', 'committee_clean'] = 'Senate Budget and Fiscal Review'\n",
    "locations.loc[locations['committee_code'] == 'CX23', 'committee_clean'] = 'Assembly Utilities and Commerce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_codes = {\n",
    "    row['motion_id']: row['simplified_motion']\n",
    "    for _, row in bill_motions.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_votes['motion_text'] = summary_votes['motion_id'].map(motion_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_bill_id(id):\n",
    "    front, end = id[:4], id[4:]\n",
    "    if re.search(r'\\d{4}$', front):\n",
    "        return f\"{front}{int(front) + 1}{end}\"\n",
    "    else:\n",
    "        return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions['ID'] = versions['bill_id'].apply(lambda x: repair_bill_id(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob_repairs = {}\n",
    "for _, row in lobbying.loc[(lobbying['FILING_ID'].isin(lobbying.loc[lobbying['FIRM_NAME'].isna(), 'FILING_ID'].drop_duplicates().tolist())) & (lobbying['FIRM_NAME'].notna()), ['FILING_ID', 'FIRM_NAME']].drop_duplicates().iterrows():\n",
    "    lob_repairs[row['FILING_ID']] = row['FIRM_NAME']\n",
    "\n",
    "def fix_firm_name(row):\n",
    "    if pd.isna(row['FIRM_NAME']):\n",
    "        if row['FILING_ID'] in lob_repairs.keys():\n",
    "            return lob_repairs.get(row['FILING_ID'])\n",
    "        else:\n",
    "            try:\n",
    "                id = int(re.sub(r'\\s+', '', str(row['FILING_ID'])).strip())\n",
    "            except:\n",
    "                id = None\n",
    "            if id == None:\n",
    "                if not pd.isna(row['FILING_ID']):\n",
    "                    return row['FILING_ID']\n",
    "            elif str(id) in lob_repairs.keys():\n",
    "                return lob_repairs.get(str(id))\n",
    "    return row['FIRM_NAME']\n",
    "lobbying['FIRM'] = lobbying.apply(fix_firm_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "STOPWORDS = {\"the\", \"and\", \"of\", \"&\", \"for\", \"to\"}\n",
    "LEGAL = {\n",
    "    \"inc\", \"incorporated\", \"corp\", \"corporation\",\n",
    "    \"llc\", \"l.l.c\", \"lp\", \"l.p\", \"llp\", \"l.l.p\",\n",
    "    \"co\", \"company\", \"group\", \"partners\",\n",
    "    \"holdings\", \"association\", \"assn\", \"assoc\"\n",
    "}\n",
    "\n",
    "def clean_tokens(name):\n",
    "    if not isinstance(name, str):\n",
    "        return []\n",
    "\n",
    "    name = unicodedata.normalize(\"NFKD\", name)\n",
    "    name = name.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"[^\\w\\s]\", \" \", name).strip()\n",
    "\n",
    "    return sorted(\n",
    "        t for t in name.split()\n",
    "        if t not in STOPWORDS and t not in LEGAL\n",
    "    )\n",
    "\n",
    "def group_similar_names(canonicals, threshold=93):\n",
    "    groups = []\n",
    "    group_ids = [-1] * len(canonicals)\n",
    "    current_gid = 0\n",
    "\n",
    "    for i, name_i in tqdm(enumerate(canonicals), total=len(canonicals)):\n",
    "        if group_ids[i] != -1:\n",
    "            continue\n",
    "\n",
    "        group_ids[i] = current_gid\n",
    "\n",
    "        for j in range(i + 1, len(canonicals)):\n",
    "            if group_ids[j] != -1:\n",
    "                continue\n",
    "\n",
    "            score = fuzz.token_set_ratio(name_i, canonicals[j])\n",
    "            if score >= threshold:\n",
    "                group_ids[j] = current_gid\n",
    "\n",
    "        current_gid += 1\n",
    "\n",
    "    return group_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121019/121019 [02:51<00:00, 704.57it/s] \n"
     ]
    }
   ],
   "source": [
    "lobbying['tokens'] = lobbying['FIRM'].apply(clean_tokens)\n",
    "lobbying['canonical'] = lobbying[\"tokens\"].apply(lambda t: \" \".join(t))\n",
    "lobbying['name_group'] = group_similar_names(lobbying['canonical'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [00:02<00:00, 519.57it/s]\n"
     ]
    }
   ],
   "source": [
    "donor_n = pd.concat([expend_assembly['ExpenderName'], expend_senate['ExpenderName'], independent_expenditures['ExpenderName']]).drop_duplicates().to_frame()\n",
    "donor_n['tokens'] = donor_n['ExpenderName'].apply(clean_tokens)\n",
    "donor_n['canonical'] = donor_n['tokens'].apply(lambda t: \" \".join(t))\n",
    "donor_n['name_group'] = group_similar_names(donor_n['canonical'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob_names = {}\n",
    "for _, row in lobbying.groupby('name_group').agg({'FIRM': 'first'}).reset_index().iterrows():\n",
    "    lob_names[row['name_group']] = row['FIRM']\n",
    "lobbying['FIRM'] = lobbying['name_group'].map(lob_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "don_names = {}\n",
    "for _, row in donor_n.groupby('name_group').agg({'ExpenderName': 'first'}).reset_index().iterrows():\n",
    "    don_names[row['name_group']] = row['ExpenderName']\n",
    "\n",
    "don_name_group_map = {}\n",
    "for _, row in donor_n[['ExpenderName', 'name_group']].drop_duplicates().iterrows():\n",
    "    don_name_group_map[row['ExpenderName']] = row['name_group']\n",
    "\n",
    "expend_assembly['Expender'] = expend_assembly['ExpenderName'].apply(lambda x: don_names.get(don_name_group_map.get(x)))\n",
    "expend_senate['Expender'] = expend_senate['ExpenderName'].apply(lambda x: don_names.get(don_name_group_map.get(x)))\n",
    "independent_expenditures['Expender'] = independent_expenditures['ExpenderName'].apply(lambda x: don_names.get(don_name_group_map.get(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_vers = versions.loc[versions['bill_id'].str.startswith('2')]\n",
    "for i, row in bill_vers.iterrows():\n",
    "    tail = f\"{row['VersionNum']}{row['MeasureState']}\"\n",
    "    repaired = repair_bill_id(re.sub(tail, '', row['bill_id']))\n",
    "    end = int(repaired[-4:])\n",
    "\n",
    "    bill_vers.loc[i, 'bill_ID'] = f\"{repaired[:-4]}{end}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_types = versions['MeasureType'].drop_duplicates().values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROP_RE = re.compile(\n",
    "    r'\\bprop(?:osition)?\\.?\\s*(\\d+)\\b',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_prop_number(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    m = PROP_RE.search(text)\n",
    "    return f\"PROP {m.group(1)}\" if m else None\n",
    "\n",
    "independent_expenditures['prop_no'] = independent_expenditures['TargetPropositionName'].apply(extract_prop_number)\n",
    "independent_expenditures = independent_expenditures.dropna(subset='DateEnd')\n",
    "independent_expenditures.loc[independent_expenditures['DateEnd'].str.startswith('3011'), 'DateEnd'] = \"2011-05-26T00:00:00Z\"\n",
    "independent_expenditures['year'] = independent_expenditures['DateEnd'].apply(lambda x: x[:4]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_groups = independent_expenditures.dropna(subset=\"prop_no\").groupby(['year', 'prop_no'], as_index=False)\n",
    "\n",
    "MEASURE_ALT = \"|\".join(map(re.escape, measure_types))\n",
    "\n",
    "BILL_RE = re.compile(\n",
    "    rf\"\\b({MEASURE_ALT})\\s*[- ]?\\s*(\\d+)\\b\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_bills(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    matches = BILL_RE.findall(str(text))\n",
    "    return [(t.upper(), int(n)) for t, n in matches]\n",
    "\n",
    "\n",
    "independent_expenditures['bill_refs'] = independent_expenditures['TargetPropositionName'].apply(extract_bills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ves = versions[['MeasureType', 'MeasureNum', 'SessionYear', 'ID']].drop_duplicates()\n",
    "ves['bill_ID'] = ves['ID'].apply(lambda x: x[:-5])\n",
    "ves = ves[['MeasureType', 'MeasureNum', 'SessionYear', 'bill_ID']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bill_match(row):\n",
    "    if row['bill_refs'] == []:\n",
    "        return None\n",
    "    refs = []\n",
    "    for x in row['bill_refs']:\n",
    "        v = ves.loc[(ves['MeasureType'] == x[0]) & (ves['MeasureNum'] == x[1]) & (ves['SessionYear'] == row['year'])]\n",
    "        if v.shape[0] > 0:\n",
    "            refs.append(v['bill_ID'].values[0])\n",
    "    return refs\n",
    "\n",
    "independent_expenditures['bill_matches'] = independent_expenditures.apply(bill_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = independent_expenditures.copy()[['Amount', 'DateEnd', 'Expender', 'bill_matches', 'ExpenderPosition']]\n",
    "ie = ie.loc[ie['bill_matches'].notna()]\n",
    "ie['DateEnd'] = pd.to_datetime(pd.to_datetime(ie['DateEnd']).dt.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "stance_map = {'S': 1, 'O': -1}\n",
    "ie['ExpenderPosition'] = ie['ExpenderPosition'].map(stance_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in {'Horton': 'Shirley', 'Calderon': 'Ron', 'Berryhill': 'Tom', 'Stone': 'Mark', 'Rubio': 'Susan', 'Rivas': 'Robert', 'Nguyen': 'Janet'}.items():\n",
    "    politicians.loc[(politicians['full_name'].isna()) & (politicians['Last'] == k), 'full_name'] = f\"{k}, {v}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_fixes = {}\n",
    "manual = {'Steinberg, Darrell': [6, 'D'],\n",
    " 'Calderon, Ron': [30, 'D'],\n",
    " 'Stone, Mark': [29, 'D'],\n",
    " 'Rubio, Susan': [22, 'D'],\n",
    " 'Nguyen, Janet': [36, 'R'],\n",
    " 'Berryhill, Tom': [8, 'R']}\n",
    "\n",
    "for _, row in politicians.loc[politicians['District No.'].isna(), ['full_name', 'chamber', 'Term']].drop_duplicates().iterrows():\n",
    "    if politicians.loc[(politicians['chamber'] == row['chamber']) & (politicians['full_name'] == row['full_name']) & (politicians['Term'] == row['Term'])].dropna(subset='District No.').shape[0] > 0:\n",
    "        f = politicians.loc[(politicians['chamber'] == row['chamber']) & (politicians['full_name'] == row['full_name']) & (politicians['Term'] == row['Term'])].dropna(subset='District No.')\n",
    "        pol_fixes[(row['full_name'], row['chamber'], row['Term'])] = {'party': f['Party'].values[0], 'district_id': re.sub(r'\\s+', '', f['District No.'].values[0])}\n",
    "\n",
    "    else:\n",
    "        pol_fixes[(row['full_name'], row['chamber'], row['Term'])] = {'party': manual.get(row['full_name'])[1], 'district_id': manual.get(row['full_name'])[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def party_district_fix(rows):\n",
    "    party, district = [], []\n",
    "    for _, row in rows.iterrows():\n",
    "        fix = pol_fixes.get((row['full_name'], row['chamber'], row['Term']))\n",
    "        party.append(fix['party'])\n",
    "        district.append(fix['district_id'])\n",
    "    return party, district\n",
    "\n",
    "parties, districts = party_district_fix(politicians.loc[politicians['District No.'].isna()])\n",
    "politicians.loc[politicians['District No.'].isna(), 'Party'] = parties\n",
    "politicians.loc[politicians['District No.'].isna(), 'District No.'] = districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislators = {i: pol for i, pol in enumerate(politicians['full_name'].unique().tolist())}\n",
    "\n",
    "leg_parties = {row['full_name']: row['Party'] for _, row in politicians[['full_name', 'Party']].drop_duplicates().iterrows()}\n",
    "leg_occupations = {row['full_name']: row['Occupation'] for _, row in politicians[['full_name', 'Occupation']].drop_duplicates().iterrows()}\n",
    "committees = {i: com for i, com in enumerate(politicians['committee_clean'].unique().tolist())}\n",
    "lobby_firms = {i: firm for i, firm in enumerate(lobbying['FIRM'].unique().tolist())}\n",
    "\n",
    "donor_names = list(set(expend_assembly['Expender'].unique().tolist() + expend_senate['Expender'].unique().tolist() + ie['Expender'].unique().tolist()))\n",
    "donors = {i: donor for i, donor in enumerate(donor_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dict(d, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, d in zip(\n",
    "    ['legislators', 'committees', 'lobby_firms', 'donors'],\n",
    "    [legislators, committees, lobby_firms, donors]\n",
    "):\n",
    "    save_dict(d, f'{name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_titles = {row['bill_ID']: row['Title'] for _, row in bill_vers[['bill_ID', 'Title']].drop_duplicates().iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_subjects = {row['bill_ID']: row['GeneralSubject'] for _, row in bill_vers.loc[bill_vers['GeneralSubject'].apply(lambda x: x is not None and isinstance(x, str)), ['bill_ID', 'GeneralSubject']].drop_duplicates().iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_ids = list(set(bill_votes.loc[bill_votes['bill_id'].str.startswith('2'), 'bill_id'].unique().tolist() + summary_votes.loc[summary_votes['bill_id'].str.startswith('2'), 'bill_id'].unique().tolist() + bill_history['bill_id'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_id_codes = {row['bill_id']: row['bill_ID'] for _, row in bill_vers.drop_duplicates(subset=['bill_id', 'bill_ID']).iterrows()}\n",
    "history['bill_ID'] = history['bill_id'].map(bill_id_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['Date'] = pd.to_datetime(history['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_dates = {}\n",
    "for v, group in history.loc[history['bill_ID'].isin(bill_ids)].groupby('bill_ID'):\n",
    "    introduction_dates[v] = {'Dates': group['Date'].unique().tolist(), 'Actions': group.sort_values('Date', ascending=True).drop_duplicates(subset=['Action', 'Date'])['Action'].tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_id_mapping = {i: list(group.values) for i, group in bill_vers.groupby('bill_ID')['ID']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict(version_id_mapping, 'version_id_mapping.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_id_mapping2 = {i: list(group.values) for i, group in bill_vers.groupby('bill_ID')['bill_id']}\n",
    "bv2b = {v: k for k, val in version_id_mapping2.items() for v in val}\n",
    "history['bill_ID'] = history['bill_id'].map(bv2b)\n",
    "save_dict(bv2b, 'bill_id_mapping.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ranges = {}\n",
    "\n",
    "for k, v in introduction_dates.items():\n",
    "    first, last = min(v['Dates']), max(v['Dates'])\n",
    "    date_ranges[k] = {'First_action': first, 'Last_action': last}\n",
    "\n",
    "save_dict(date_ranges, 'bill_dates_map.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = history.loc[history['bill_ID'].notna()].sort_values('Date', ascending=False).groupby('bill_ID').first().reset_index()[['bill_ID', 'Action']]\n",
    "outcomes.loc[outcomes['Action'].isin(['CHAPTERED', 'ENROLLED', 'FILED', 'APPROVED']), 'Outcome'] = 1\n",
    "outcomes.loc[outcomes['Action'] == 'VETOED', 'Outcome'] = -1\n",
    "outcomes.loc[outcomes['Outcome'].isna(), 'Outcome'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = outcomes.set_index('bill_ID')['Outcome'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict(outcome, 'bill_outcomes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_bill_ids = {}\n",
    "for i in summary_votes.loc[summary_votes['bill_id'].isin(bill_ids)].groupby(['year', 'motion_id'])['bill_id'].value_counts().index:\n",
    "    year, motion_id, bill_id = i\n",
    "    if (year, motion_id) not in vote_bill_ids.keys():\n",
    "        vote_bill_ids[(year, motion_id)] = [bill_id]\n",
    "    else:\n",
    "        vote_bill_ids[(year, motion_id)].append(bill_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_vers_dig = bill_vers.merge(digests, on='bill_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians.loc[(politicians['full_name'] == 'Torlakson, Tom') & (politicians['District No.'] == '6 7'), 'District No.'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislators_last_names = {}\n",
    "for _, row in politicians[['chamber', 'Last', 'Term', 'full_name']].drop_duplicates().iterrows():\n",
    "    legislators_last_names[(row['chamber'], row['Last'].lower(), row['Term'])] = row['full_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {row['ID']: {\n",
    "    'digest': row['DigestText'],\n",
    "    'MeasureState': row['MeasureState'],\n",
    "    'VoteRequired': row['VoteRequired'] if row['VoteRequired'] is not None else 'No',\n",
    "    'VersionNum': row['VersionNum'] if row['VersionNum'] is not None else 'No',\n",
    "    'LocalProgram': row['LocalProgram'] if row['LocalProgram'] is not None else 'No',\n",
    "    'FiscalCommittee': row['FiscalCommittee'] if row['FiscalCommittee'] is not None else 'No',\n",
    "    'TaxLevy': row['TaxLevy'] if row['TaxLevy'] is not None else 'No',\n",
    "    'Urgency': row['Urgency'] if row['Urgency'] is not None else 'No'} for _, row in bill_vers_dig.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislator_codes = {v: k for k, v in legislators.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_codes = {v.lower(): k for k, v in committees.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes['chamber'] = bill_votes['location_code'].apply(lambda x: 'assembly' if x == 'AFLOOR' or x.startswith('CX') else 'senate' if x == 'SFLOOR' or x.startswith('CS') else 'full')\n",
    "bill_votes['vote_date_time'] = pd.to_datetime(bill_votes['vote_date_time'])\n",
    "bill_votes['term'] = bill_votes['vote_date_time'].apply(lambda x: f\"{x.year}-{x.year + 1}\" if x.year % 2 == 1  else f\"{x.year - 1}-{x.year}\" if x.year % 2 == 0 and x < pd.Timestamp(year=x.year, month=11, day=2) else f\"{x.year + 1}-{x.year + 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_locations = authors.loc[(authors['House'] == 'UNKNOWN') & (authors['bill_id'].map(bill_id_codes).isin(bill_ids)), ['bill_id', 'Name']].drop_duplicates()\n",
    "for i, row in author_locations.iterrows():\n",
    "    if 'AB' in row['bill_id']:\n",
    "        author_locations.loc[i, 'name'] = 'Assembly ' + row['Name']\n",
    "    elif 'SB' in row['bill_id']:\n",
    "        author_locations.loc[i, 'name'] = 'Senate ' + row['Name']\n",
    "    else:\n",
    "        author_locations.loc[i, 'name'] = 'Joint ' + row['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fuzzy_strings(source_list, target_list):\n",
    "    def preprocess_name(name):\n",
    "        if not isinstance(name, str):\n",
    "            return \"\"\n",
    "        name = name.lower()\n",
    "        name = re.sub(r'\\(.*?\\)', '', name)\n",
    "        name = re.sub(r'committee on', '', name)\n",
    "        name = re.sub(r'[^a-z\\s]', ' ', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        return name\n",
    "\n",
    "    clean_source = [preprocess_name(c) for c in source_list]\n",
    "    clean_target = [preprocess_name(c) for c in target_list]\n",
    "\n",
    "    keywords = [\"education\", \"health\", \"finance\", \"budget\", \"transportation\",\n",
    "                \"judiciary\", \"environment\", \"agriculture\", \"energy\", \"labor\",\n",
    "                \"housing\", \"veterans affairs\", \"public safety\", \"insurance\", \"banking\", \"public health\", \"small business\", \"redistricting\",\n",
    "                \"public utilities\", \"natural resources\", \"water\",\n",
    "                \"technology\", \"communications\", \"elections\", \"government\",\n",
    "                \"appropriations\", \"rules\", \"ethics\", 'criminal justice', \"environmental protection\", \"college and university\", \"human services\", \"reproductive health\", \"mental health\", \"technology\", \"aggriculture\", \"urban development\", \"renewable energy\", \"gun violence\", \"commerce\", \"privacy\", \"cybersecurity\", \"infrastructure\", \"disaster preparedness\", \"prisons\", \"aging\"]\n",
    "\n",
    "    def get_committee_keywords(name):\n",
    "        return set(kw for kw in keywords if kw in name)\n",
    "\n",
    "    target_keywords = [get_committee_keywords(name) for name in clean_target]\n",
    "\n",
    "    def calculate_similarity(source_idx, target_idx):\n",
    "        source = clean_source[source_idx]\n",
    "        target = clean_target[target_idx]\n",
    "\n",
    "        if not source or not target:\n",
    "            return 0\n",
    "\n",
    "        if source == target:\n",
    "            return 100\n",
    "\n",
    "        token_sort = fuzz.token_sort_ratio(source, target)\n",
    "        token_set = fuzz.token_set_ratio(source, target)\n",
    "        partial = fuzz.partial_ratio(source, target)\n",
    "\n",
    "        source_kw = get_committee_keywords(source)\n",
    "        keyword_overlap = len(source_kw.intersection(target_keywords[target_idx]))\n",
    "        keyword_bonus = min(20, keyword_overlap * 10)\n",
    "        weighted_score = (token_sort * 0.3) + (token_set * 0.5) + (partial * 0.2) + keyword_bonus\n",
    "\n",
    "        return weighted_score\n",
    "\n",
    "    matches = {}\n",
    "    for i, source in enumerate(source_list):\n",
    "        scores = [calculate_similarity(i, j) for j in range(len(target_list))]\n",
    "\n",
    "        if not scores or max(scores) < 60:\n",
    "            matches[source] = None\n",
    "        else:\n",
    "            best_idx = np.argmax(scores)\n",
    "            confidence = scores[best_idx]\n",
    "\n",
    "            if confidence >= 60:\n",
    "                matches[source] = target_list[best_idx]\n",
    "            else:\n",
    "                matches[source] = None\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_com_matches = fuzzy_strings(author_locations['name'].unique().tolist(), leg_committees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_locations['name'] = author_locations['name'].map(author_com_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sponsors = authors.loc[authors['bill_id'].map(bill_id_codes).isin(bill_ids)]\n",
    "sponsors['term'] = sponsors['bill_id'].apply(lambda x: f\"{x[:4]}-{int(x[:4]) + 1}\" if int(x[:4]) % 2 == 1 else f\"{int(x[:4]) - 1}-{x[:4]}\" if int(x[:4]) % 2 == 0 and int(x[:4]) < 2009 else f\"{x[:4]}-{int(x[:4]) + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob = lobbying.loc[lobbying['clean_beneficiary'].notna(), ['FIRM', 'EXPN_DSCR', 'clean_beneficiary', 'EXPN_DATE', 'BENE_AMT']]\n",
    "lob['EXPN_DATE'] = pd.to_datetime(lob['EXPN_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term(date):\n",
    "    if not isinstance(date, pd.Timestamp):\n",
    "        return None\n",
    "    year = date.year\n",
    "    if year % 2 != 1:\n",
    "        if date.month < 12:\n",
    "            return f\"{year-1}-{year}\"\n",
    "        else:\n",
    "            return f\"{year+1}-{year+2}\"\n",
    "    else:\n",
    "        return f\"{year}-{year+1}\"\n",
    "\n",
    "lob['term'] = lob['EXPN_DATE'].apply(get_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in politicians.loc[politicians['full_name'].apply(lambda x: isinstance(x, float)), ['Term', 'Last', 'chamber']].drop_duplicates().iterrows():\n",
    "    term, last = row['Term'], row['Last']\n",
    "    a = politicians.loc[(politicians['Last'] == last) & (politicians['Term'] == term) & (politicians['full_name'].apply(lambda x: isinstance(x, str)))]\n",
    "    if len(a) > 0:\n",
    "        politicians.loc[(politicians['Term'] == term) & (politicians['Last'] == last) & (politicians['chamber'] == row['chamber']), 'full_name'] = a['full_name'].values[0]\n",
    "        continue\n",
    "    else:\n",
    "        a = politicians.loc[(politicians['Last'] == last) & (politicians['full_name'].apply(lambda x: isinstance(x, str)))]\n",
    "    if len(a) > 0:\n",
    "        politicians.loc[(politicians['Term'] == term) & (politicians['Last'] == last) & (politicians['chamber'] == row['chamber']), 'full_name'] = a['full_name'].values[0]\n",
    "    else:\n",
    "        print(last, term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_names_terms = {}\n",
    "for _, row in politicians[['full_name', 'Term', 'chamber']].drop_duplicates().iterrows():\n",
    "    if ',' in row['full_name']:\n",
    "        name = row['full_name'].split(',')[1].strip() + ' ' + row['full_name'].split(',')[0].strip()\n",
    "    else:\n",
    "        name = row['full_name']\n",
    "    pol_names_terms[(row['full_name'].lower(), row['Term'])] = {'chamber': row['chamber'], 'name': name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians['clean_full_name'] = politicians['full_name'].apply(lambda x: x.split(',')[1].strip() + ' ' + x.split(',')[0].strip() if ',' in x else x)\n",
    "name_fix = {}\n",
    "for _, row in politicians[['clean_full_name', 'full_name']].drop_duplicates().iterrows():\n",
    "    name_fix[row['clean_full_name']] = row['full_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "expend_assembly = expend_assembly.rename(columns={'term': 'Term'})\n",
    "expend_assembly['chamber'] = 'assembly'\n",
    "expend_senate = expend_senate.rename(columns={'term': 'Term'})\n",
    "expend_senate['chamber'] = 'senate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_contributions = pd.concat([expend_assembly.loc[expend_assembly['matched_target_name'].notna(), ['Expender', 'Amount', 'matched_target_name', 'Term', 'chamber', 'DateEnd']].drop_duplicates(subset=['Expender', 'Amount', 'matched_target_name', 'DateEnd']), expend_senate.loc[expend_senate['matched_target_name'].notna(), ['Expender', 'Amount', 'matched_target_name', 'Term', 'chamber', 'DateEnd']].drop_duplicates(subset=['Expender', 'Amount', 'matched_target_name', 'DateEnd'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_contributions['DateEnd'] = pd.to_datetime(campaign_contributions['DateEnd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sponsors['bill_ID'] = sponsors['bill_id'].apply(repair_bill_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = history.merge(bill_votes, left_on=['bill_ID', 'Date'], right_on=['bill_id', 'vote_date_time'], how='inner').rename(columns={'bill_id_x': 'bill_version'}).drop('bill_id_y', axis=1)\n",
    "voting['bv_id'] = voting['bill_version'].apply(repair_bill_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_places = {}\n",
    "for i, row in voting.groupby(['motion_id', 'term', 'chamber', 'Date']).agg({'legislator_name': lambda x: list(x)}).iterrows():\n",
    "    motion_id, term, chamber, date = i\n",
    "    g = politicians.loc[(politicians['chamber'] == chamber) & (politicians['Term'] == term) & (politicians['Last'].isin(row['legislator_name']))]\n",
    "    voting_places[(motion_id, term, chamber, date)] = {\n",
    "        'most_common_committee': g.groupby('committee_clean').size().sort_values(ascending=False).head(1).index[0] if len(g) > 0 else None\n",
    "    }\n",
    "voting['voting_place'] = voting.apply(lambda row: voting_places.get((row['motion_id'], row['term'], row['chamber'], row['Date']), {}).get('most_common_committee', None), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='mps')\n",
    "REFRESH_FLAG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(title):\n",
    "    if not isinstance(title, str):\n",
    "        return ''\n",
    "    title = re.sub(r'\\(.*?\\)', '', title)\n",
    "    title = re.sub(r'[^a-zA-Z0-9\\s]', ' ', title)\n",
    "    title = re.sub(r'\\s+', ' ', title).strip()\n",
    "    return title.lower()\n",
    "\n",
    "def batched_embeddings(values, output_dims=384):\n",
    "    vals = [text_clean(v) for v in values if isinstance(v, str)]\n",
    "    vals = list(set([v for v in vals if v != '']))\n",
    "    embeddings = model.encode(vals, batch_size=64, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True, num_workers=4, output_dims=output_dims)\n",
    "    embs = {v: e for v, e in zip(vals, embeddings)}\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = list(set([t for t in versions.loc[versions['bill_id'].str.startswith('2')]['GeneralSubject'].tolist() if (isinstance(t, str) and t is not None)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_original = {text_clean(t): t for t in subjects}\n",
    "if REFRESH_FLAG == True:\n",
    "    save_dict(subjects_original, 'subjects_original.pkl')\n",
    "    subject_embeddings = batched_embeddings(subjects)\n",
    "    torch.save(subject_embeddings, 'subject_embeddings.pt')\n",
    "else:\n",
    "    subject_embeddings = torch.load('subject_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REFRESH_FLAG == True:\n",
    "    titles = [t for t in bill_vers['Title'].unique().tolist() if (isinstance(t, str) and t not in [None, '', np.nan])]\n",
    "    title_embeddings = batched_embeddings(titles, output_dims=64)\n",
    "    torch.save(title_embeddings, 'title_embeddings.pt')\n",
    "else:\n",
    "    title_embeddings = torch.load('title_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobbying['firm'] = lobbying['FIRM'].apply(text_clean)\n",
    "lob['firm'] = lob['FIRM'].apply(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a38f4dcc17d4c148bf39ba76bfd8793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lobbying_firms_embeddings = batched_embeddings(lobbying.loc[(lobbying['clean_beneficiary'].notna()) & (lobbying['firm'].notna()), 'firm'].unique().tolist(), output_dims=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_embeddings = {}\n",
    "for committee in politicians['committee_clean'].unique().tolist():\n",
    "    co = re.sub(r'assembly|senate|committee|subcommittee', '', committee.lower())\n",
    "    committee_embeddings[committee.lower()] = model.encode(co,  convert_to_tensor=True, normalize_embeddings=True, truncate_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [00:20<00:00, 38.04it/s]\n"
     ]
    }
   ],
   "source": [
    "donor_embeddings = {}\n",
    "\n",
    "for donor in tqdm(donor_names):\n",
    "    donor_embeddings[donor] = model.encode(donor, convert_to_tensor=True, normalize_embeddings=True, truncate_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 58.57it/s]\n"
     ]
    }
   ],
   "source": [
    "motion_embeddings = {}\n",
    "for motion in tqdm([t for t in pd.DataFrame.from_dict(motion_codes, orient='index').reset_index().rename({'index': 'motion_id', 0: 'motion_text'}, axis=1)['motion_text'].drop_duplicates().tolist() if t is not None]):\n",
    "    motion_embeddings[motion] = model.encode(motion, convert_to_tensor=True, normalize_embeddings=True, truncate_dim=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_embeddings = torch.load('digests.pt')\n",
    "if (len([a for a in (list(set([t if (isinstance(t, str) and t is not None) else '' for t in digests.loc[digests['bill_id'].str.startswith('2')]['DigestText'].tolist()]))) if text_clean(a) not in digest_embeddings.keys()]) == 0) & (REFRESH_FLAG == False):\n",
    "    pass\n",
    "else:\n",
    "    for digest in tqdm(list(set([t if (isinstance(t, str) and t is not None) else '' for t in digests.loc[digests['bill_id'].str.startswith('2')]['DigestText'].tolist()]))):\n",
    "        if text_clean(digest) not in digest_embeddings.keys():\n",
    "            digest_embeddings[text_clean(digest)] = model.encode(digest, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    torch.save(digest_embeddings, 'digests.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "hear = hearings[['bill_id', 'location_code']].merge(locations[['committee_code', 'committee_clean']], left_on='location_code', right_on='committee_code', how='left')[['bill_id', 'committee_clean']].drop_duplicates()\n",
    "hear['year'] = hear['bill_id'].apply(lambda x: int(x[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = {p: 'Democratic Alternate' if re.search(r'Democratic\\s*Alternate', p) is not None else 'Vice Chair' if re.search(r'V\\s*i\\s*c\\s*e\\s*-*\\s*C\\s*h\\s*a\\s*i\\s*r\\s*', p) is not None else 'Co-Chair' if re.search(r'Co\\s*-\\s*Chair', p) is not None else 'Chair' if re.search(r'Cha\\s*i\\s*r', p) is not None else 'Republican Alternate' if re.search(r'\\s*Republican\\s*Alternate', p) is not None else p for p in politicians['position'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "vnums = bill_vers.set_index('ID')['VersionNum'].to_dict()\n",
    "vid_map = {v: k for k, val in version_id_mapping.items() for v in val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sponsors['chamber'] = sponsors['House'].apply(lambda x: x.lower() if isinstance(x, str) else None)\n",
    "sponsors = sponsors.merge(politicians[['Term', 'Last', 'chamber', 'full_name']].drop_duplicates(), left_on=['chamber', 'Name', 'term'], right_on=['chamber', 'Last', 'Term'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \" \".join(text.split(',')[::-1])\n",
    "    text = unidecode(text.lower().strip())\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "pol_names_terms2 = {}\n",
    "for k, v in pol_names_terms.items():\n",
    "    pol_names_terms2[(clean_text(k[0]), k[1])] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOP Graph-Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.data import HeteroData\n",
    "import numpy as np\n",
    "\n",
    "author_type_map = {\n",
    "    'LEAD_AUTHOR': 'LEAD_AUTHOR',\n",
    "    'PRINCIPAL_COAUTHOR': 'PRINCIPAL_COAUTHOR',\n",
    "    'COAUTHOR': 'COAUTHOR',\n",
    "    'data=\"COAUTHOR\"': 'COAUTHOR',\n",
    "    'data=\"LEAD_AUTHOR\"': 'LEAD_AUTHOR',\n",
    "    'data=\"PRINCIPAL_COAUTHOR\"': 'PRINCIPAL_COAUTHOR',\n",
    "    'nan': 'AUTHOR'\n",
    "}\n",
    "author_levels = {\n",
    "    'AUTHOR': 1,\n",
    "    'COAUTHOR': 1,\n",
    "    'PRINCIPAL_COAUTHOR': 2,\n",
    "    'LEAD_AUTHOR': 3\n",
    "}\n",
    "\n",
    "def features_vote_required(vr):\n",
    "    if not isinstance(vr, str):\n",
    "        return 'MAJORITY'\n",
    "    if \"FOUR_FIFTHS\" in vr:\n",
    "        return '80_PCT'\n",
    "    elif \"THREE_FOURTHS\" in vr:\n",
    "        return '75_PCT'\n",
    "    elif \"SEVENTY_PERCENT\" in vr or \"70%\" in vr:\n",
    "        return '70_PCT'\n",
    "    elif \"TWO_THIRDS\" in vr:\n",
    "        return '66-67_PCT'\n",
    "    elif \"55%\" in vr:\n",
    "        return '55_PCT'\n",
    "    else:\n",
    "        return 'MAJORITY'\n",
    "\n",
    "vote_required_codes = {\n",
    "    'MAJORITY': 0,\n",
    "    '55_PCT': 1,\n",
    "    '66-67_PCT': 2,\n",
    "    '70_PCT': 3,\n",
    "    '75_PCT': 4,\n",
    "    '80_PCT': 5\n",
    "}\n",
    "\n",
    "def bool_correction(val):\n",
    "    if not isinstance(val, str):\n",
    "        return 0\n",
    "    if 'YES' in val:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "measure_types = bill_vers['MeasureType'].unique()\n",
    "parties = politicians['Party'].unique()\n",
    "chambers = politicians['chamber'].unique()\n",
    "outcome_mapping = {'CHAPTERED': 1, 'VETOED': 0, 'FAILED': -1, 'ENROLLED': 1}\n",
    "measure_encoder = LabelEncoder()\n",
    "measure_encoder.fit(measure_types)\n",
    "party_encoder = LabelEncoder()\n",
    "party_encoder.fit(parties)\n",
    "chamber_encoder = LabelEncoder()\n",
    "chamber_encoder.fit(chambers)\n",
    "pos = list(positions.values()) + ['member']\n",
    "pos_encoder = LabelEncoder()\n",
    "pos_encoder.fit(pos)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, id, type, features=None):\n",
    "        self.id = id\n",
    "        self.type = type\n",
    "        self.features = features or {}\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self, source, target, relation, attributes=None):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.relation = relation\n",
    "        self.attributes = attributes or {}\n",
    "\n",
    "class Bill(Node):\n",
    "    def __init__(self, bill_id, title, subject, measure_type):\n",
    "        measure_type = measure_encoder.transform([measure_type])[0] if measure_type in measure_encoder.classes_ else -1\n",
    "        features = {\n",
    "            'title': title,\n",
    "            'subject': subject,\n",
    "            'measure_type': measure_type,\n",
    "            'date': None,\n",
    "        }\n",
    "        super().__init__(bill_id, \"bill\", features)\n",
    "        self.actions = None\n",
    "        self.order_df = None\n",
    "        self.outcome = None\n",
    "\n",
    "    def add_actions(self, actions):\n",
    "        if self.actions is None:\n",
    "            self.actions = actions\n",
    "        else:\n",
    "            self.actions = pd.concat([self.actions, actions], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    def add_date(self, date):\n",
    "        self.features['date'] = date\n",
    "\n",
    "    def add_order_df(self, order_df):\n",
    "        if self.order_df is None:\n",
    "            self.order_df = order_df\n",
    "        else:\n",
    "            self.order_df = pd.concat([self.order_df, order_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    def add_outcome(self, outcome):\n",
    "        outcome = outcome_mapping.get(outcome, -1)\n",
    "        self.outcome = outcome\n",
    "\n",
    "    def align_actions_versions(self, bill, versions_, dates):\n",
    "        dates = pd.Series(dates).sort_values(ascending=True).drop_duplicates().tolist()\n",
    "        actions = [i for i in introduction_dates.get(bill, {}).get('Actions', []) if i != 'FILED']\n",
    "        if len(actions) > len(dates):\n",
    "            if actions[-2:] == ['ENROLLED', 'CHAPTERED'] or actions[-2:] == ['APPROVED', 'CHAPTERED'] or len(actions) <= 4 and actions[-1] == 'ENROLLED' or abs(len(dates) - len(actions)) >= 2 and actions[-1] == 'ENROLLED' or actions == ['INTRODUCED', 'ENROLLED', 'AMENDED_SENATE'] or actions[-1] == 'APPROVED' or len(actions) == 3 and all(a.startswith('PASSED_') for a in actions[-2:]) or actions == ['ENROLLED', 'INTRODUCED'] or actions == ['INTRODUCED', 'ENROLLED'] or actions == ['INTRODUCED', 'REVISED'] or len(actions) > 3 and actions[-3:] == ['ENROLLED', 'CORRECTED', 'CHAPTERED'] or len(actions) > 3 and actions[-3:] == ['PASSED_SENATE', 'PASSED_ASSEMBLY', 'AMENDED_SENATE'] or list(set(['INTRODUCED', 'PASSED_SENATE', 'PASSED_ASSEMBLY', 'AMENDED_SENATE'])) == list(set(actions)) or list(set(['INTRODUCED', 'PASSED_SENATE', 'PASSED_ASSEMBLY', 'AMENDED_ASSEMBLY'])) == list(set(actions)) or actions[-2] == 'ENROLLED' and actions[-1].startswith('PASSED_') or len(actions) > 5 and actions[-4] == 'CHAPTERED' and actions[-1].startswith('PASSED_') or actions[-2:] == ['PASSED_SENATE', 'PASSED_ASSEMBLY'] or actions == ['INTRODUCED', 'AMENDED_SENATE', 'PASSED_SENATE', 'PASSED_ASSEMBLY', 'AMENDED_ASSEMBLY'] or len(actions) == 9 and actions[-6:] == ['APPROVED', 'CHAPTERED', 'ENROLLED', 'AMENDED_SENATE', 'PASSED_ASSEMBLY', 'PASSED_SENATE'] or len(actions) > 5 and actions[-4:] == ['ENROLLED', 'PASSED_SENATE', 'APPROVED', 'PASSED_ASSEMBLY'] or dates == [pd.Timestamp('2008-12-08 00:00:00'), pd.Timestamp('2008-12-18 00:00:00')]:\n",
    "                dates.append(dates[-1])\n",
    "                if len(actions) > len(dates):\n",
    "                    dates.append(dates[-1])\n",
    "                    if len(actions) > len(dates):\n",
    "                        dates.append(dates[-1])\n",
    "                        if len(actions) > len(dates):\n",
    "                            dates.append(dates[-1])\n",
    "                            if len(actions) > len(dates):\n",
    "                                dates.append(dates[-1])\n",
    "            if len(dates) == 1 and len(actions) > 1:\n",
    "                for _ in range(len(actions) - len(dates)):\n",
    "                    dates.append(dates[0])\n",
    "            if actions[-2:] == ['INTRODUCED', 'PASSED_ASSEMBLY']:\n",
    "                dates = [dates[0]] + dates\n",
    "            if len(actions) >= 6 and actions[:3] == ['INTRODUCED', 'AMENDED_ASSEMBLY', 'ENROLLED'] and actions[-3:] == ['CHAPTERED', 'APPROVED', 'CORRECTED']:\n",
    "                dates = dates[:2] + [dates[2]] + dates[2:-2] + [dates[-2]] + dates[-2:]\n",
    "            elif ('PASSED_ASSEMBLY' in actions and 'AMENDED_ASSEMBLY' in actions) or ('PASSED_SENATE' in actions and 'AMENDED_SENATE' in actions):\n",
    "                if len(dates) == 3:\n",
    "                    dates = dates[:1] + [dates[1]] + dates[1:]\n",
    "                    if all(a for a in ['PASSED_ASSEMBLY', 'AMENDED_ASSEMBLY', 'PASSED_SENATE', 'AMENDED_SENATE'] if a in actions):\n",
    "                        dates = dates[:1] + [dates[1]] + dates[1:]\n",
    "                elif 'PROPOSED_CONFERENCE_REPORT_1' in actions and len(actions) - len(dates) == 2:\n",
    "                    dates = dates[:2] + [dates[2]] + [dates[2]] + dates[2:]\n",
    "                elif len(dates) > 3 and len(actions) - len(dates) > 0 and not (actions[-4:] == ['PASSED_ASSEMBLY', 'ENROLLED', 'PASSED_SENATE', 'CHAPTERED'] and 'AMENDED_SENATE' in actions):\n",
    "                    dates = dates[:2] + [dates[2]] + dates[2:]\n",
    "                if len(actions) > 4 and actions[-3:] == ['ENROLLED', 'PASSED_SENATE', 'CHAPTERED']:\n",
    "                    dates = dates[:-2] + [dates[-2]] + dates[-2:]\n",
    "\n",
    "            if len(actions) - len(dates) == 1:\n",
    "                if 'CORRECTED' in actions:\n",
    "                    actions.remove('CORRECTED')\n",
    "                elif 'RESCIND' in actions:\n",
    "                    actions.remove('RESCIND')\n",
    "\n",
    "            if actions[-1] == 'CORRECTED' and len(actions) - len(dates) == 2:\n",
    "                if len(dates) >= 5:\n",
    "                    dates = dates[:3] + [dates[3]] + [dates[3]] + [dates[4]] + dates[4:]\n",
    "                elif len(dates) == 2:\n",
    "                    dates = [dates[0]] + [dates[0]] + dates[0:]\n",
    "                else:\n",
    "                    dates = dates[:2] + [dates[2]] + [dates[2]] + dates[2:]\n",
    "            if actions[-1] == 'CHAPTERED' and len(actions) - len(dates) == 3:\n",
    "                dates = dates + [dates[-1]] + [dates[-1]] + [dates[-1]]\n",
    "            if actions[-2:] == ['ENROLLED', 'VETOED'] and len(actions) - len(dates) > 0 :\n",
    "                dates = dates[:-4] + [dates[-4]]  + [dates[-3]] + dates[-3:]\n",
    "\n",
    "            if len(dates) < len(actions) and'ENROLLED' in actions and actions.index('ENROLLED') < len(actions) - 1:\n",
    "                for i in range(len(actions) - actions.index('ENROLLED')):\n",
    "                    dates = dates + [dates[-1]]\n",
    "        if len(actions) + 1 == len(dates):\n",
    "            dates = dates[:-1]\n",
    "        try:\n",
    "            action_df = pd.DataFrame({'date': dates, 'action': actions})\n",
    "        except:\n",
    "            return None, None, None\n",
    "        action_df['date'] = pd.to_datetime(action_df['date'], errors='coerce')\n",
    "        order_df = action_df.loc[~action_df['action'].isin(['FILED', 'PASSED_ASSEMBLY', 'PASSED_SENATE', 'APPROVED'])]\n",
    "        repair_flag = False\n",
    "        if order_df.shape[0] > len(versions_):\n",
    "            version_ends = [re.search(r'INT|AMD|ENR|CHP|PRO', v).group() for v in versions_]\n",
    "            if 'ENR' in version_ends:\n",
    "                v_enr = version_ends.index('ENR')\n",
    "                extension = [versions_[v_enr - 1] if v_enr - 1 != 0 else versions_[v_enr] for _ in range(len(order_df) - len(versions_))]\n",
    "                versions_ = versions_[:v_enr] + extension + versions_[v_enr:]\n",
    "            else:\n",
    "                repair_flag = True\n",
    "        vr = pd.DataFrame({'version': versions_})\n",
    "        if vr.shape[0] == 0:\n",
    "            return None, None, None\n",
    "        try:\n",
    "            vr['v_num'] = vr['version'].apply(lambda x: re.search(r'\\d{2}(?=INT|AMD|ENR|CHP|PRO)', x).group()).astype(int)\n",
    "        except:\n",
    "            return None, None, None\n",
    "        vr = vr.sort_values('v_num', ascending=False).reset_index(drop=True)\n",
    "        if repair_flag:\n",
    "            last_v = vr.loc[vr['version'].notna()].iloc[-1]['version']\n",
    "            last_v_num = float(re.search(r'\\d{2}(?=INT|AMD|ENR|CHP|PRO)', last_v).group())\n",
    "            for i in range(len(order_df) - len(versions_)):\n",
    "                vr.loc[len(vr) + i, 'version'] = last_v\n",
    "                vr.loc[len(vr) + i, 'v_num'] = last_v_num\n",
    "\n",
    "        order_df['version'] = vr['version']\n",
    "        order_df['order'] = range(1, len(order_df) + 1)\n",
    "        outcomes = order_df['action'].tolist()\n",
    "        if 'CHAPTERED' in outcomes or 'FILED' in outcomes:\n",
    "            if 'VETOED' in outcomes:\n",
    "                outcome = 'VETOED'\n",
    "            else:\n",
    "                outcome = 'CHAPTERED'\n",
    "        else:\n",
    "            outcome = 'FAILED'\n",
    "        return action_df, order_df, outcome\n",
    "\n",
    "class BillVersion(Node):\n",
    "    def __init__(self, bill_id, version_id, digest, vote_required, local_program, fiscal_com, tax_levy, urgency):\n",
    "        vote_required = vote_required_codes.get(features_vote_required(vote_required), 0)\n",
    "        local_program = bool_correction(local_program)\n",
    "        fiscal_com = bool_correction(fiscal_com)\n",
    "        tax_levy = bool_correction(tax_levy)\n",
    "        urgency = bool_correction(urgency)\n",
    "        features = {\n",
    "            'digest': digest,\n",
    "            'VoteRequired': vote_required,\n",
    "            'LocalProgram': local_program,\n",
    "            'FiscalCommittee': fiscal_com,\n",
    "            'TaxLevy': tax_levy,\n",
    "            'Urgency': urgency,\n",
    "            'date': None\n",
    "        }\n",
    "        super().__init__(version_id, \"bill_version\", features)\n",
    "        self.bill_id = bill_id\n",
    "        self.actions = {}\n",
    "\n",
    "    def add_actions(self, location, date):\n",
    "        if location not in self.actions:\n",
    "            self.actions[location] = []\n",
    "        self.actions[location].append(date)\n",
    "\n",
    "    def add_date(self, date):\n",
    "        self.features['date'] = date\n",
    "\n",
    "class Legislator(Node):\n",
    "    def __init__(self, legislator_id, party):\n",
    "        party = party_encoder.transform([party])[0] if party in party_encoder.classes_ else -1\n",
    "        features = {\n",
    "            'party': party,\n",
    "        }\n",
    "        super().__init__(legislator_id, \"legislator\", features)\n",
    "        self.terms = []\n",
    "\n",
    "class LegislatorTerm(Node):\n",
    "    def __init__(self, term, legislator_id, chamber, district):\n",
    "        chamber = chamber_encoder.transform([chamber])[0] if chamber in chamber_encoder.classes_ else -1\n",
    "        features = {\n",
    "            'chamber': chamber,\n",
    "            'district': district,\n",
    "            'term': term\n",
    "        }\n",
    "        node_id = f\"{legislator_id}_{term}_{chamber}\"\n",
    "        super().__init__(node_id, \"legislator_term\", features)\n",
    "        self.committees = []\n",
    "        self.committee_positions = []\n",
    "\n",
    "    def add_committee(self, committee_id):\n",
    "        self.committees.append(committee_id)\n",
    "\n",
    "    def add_committee_position(self, committee_id, position):\n",
    "        self.committee_positions.append((committee_id, position))\n",
    "\n",
    "class Committee(Node):\n",
    "    def __init__(self, committee_id, name, chamber, term):\n",
    "        chamber = chamber_encoder.transform([chamber])[0] if chamber in chamber_encoder.classes_ else -1\n",
    "        features = {\n",
    "            'name': name,\n",
    "            'chamber': chamber\n",
    "        }\n",
    "        term_ = term.split('-')[0]\n",
    "        id = f\"{committee_id}_{term_}\"\n",
    "        super().__init__(id, \"committee\", features)\n",
    "        self.members = []\n",
    "\n",
    "    def add_member(self, legislator_id):\n",
    "        self.members.append(legislator_id)\n",
    "\n",
    "class LobbyFirm(Node):\n",
    "    def __init__(self, firm_id, name):\n",
    "        features = {\n",
    "            'name': name\n",
    "        }\n",
    "        super().__init__(firm_id, \"lobby_firm\", features)\n",
    "        self.total_donations = 0\n",
    "\n",
    "    def add_donation(self, amount):\n",
    "        self.total_donations += amount\n",
    "\n",
    "class Donor(Node):\n",
    "    def __init__(self, donor_id, name):\n",
    "        features = {\n",
    "            'name': name\n",
    "        }\n",
    "        super().__init__(donor_id, \"donor\", features)\n",
    "        self.total_donations = 0\n",
    "\n",
    "    def add_donation(self, amount):\n",
    "        self.total_donations += amount\n",
    "\n",
    "class Vote(Edge):\n",
    "    def __init__(self, legislator, bill_version, vote, motion, date, direction):\n",
    "        attributes = {\n",
    "            'vote': 1 if vote == 'AYE' else -1 if (vote == 'NOE' or vote == 'NO') else 0,\n",
    "            'motion': motion,\n",
    "            'date': date\n",
    "        }\n",
    "        if direction == 1:\n",
    "            super().__init__(legislator, bill_version, 'voted_on', attributes)\n",
    "        else:\n",
    "            super().__init__(bill_version, legislator, 'vote_from', attributes)\n",
    "\n",
    "class CommitteeMembership(Edge):\n",
    "    def __init__(self, legislator, committee, position, direction):\n",
    "        position = pos_encoder.transform([position])[0] if position in pos_encoder.classes_ else pos_encoder.transform(['member'])[0]\n",
    "        attributes = {\n",
    "            'position': position\n",
    "        }\n",
    "        if direction == 1:\n",
    "            super().__init__(legislator, committee, 'member_of', attributes)\n",
    "        else:\n",
    "            super().__init__(committee, legislator, 'has_member', attributes)\n",
    "        committee.add_member(legislator)\n",
    "\n",
    "class Sponsorship(Edge):\n",
    "    def __init__(self, legislator, bill_version, author_type, direction):\n",
    "        author_type = author_levels.get(author_type_map.get(author_type, 'AUTHOR'))\n",
    "        attributes = {\n",
    "            'author_type': author_type\n",
    "        }\n",
    "        if direction == 1:\n",
    "            super().__init__(legislator, bill_version, 'wrote', attributes)\n",
    "        else:\n",
    "            super().__init__(bill_version, legislator, 'written_by', attributes)\n",
    "\n",
    "class Reading(Edge):\n",
    "    def __init__(self, bill, committee, date, direction):\n",
    "        attributes = {\n",
    "            'date': date\n",
    "        }\n",
    "        if direction == 1:\n",
    "            super().__init__(committee, bill, 'read', attributes)\n",
    "        else:\n",
    "            super().__init__(bill, committee, 'read_by', attributes)\n",
    "\n",
    "class Donation(Edge):\n",
    "    def __init__(self, donor, recipient, amount, date, type, direction=1):\n",
    "        attributes = {\n",
    "            'amount': amount,\n",
    "            'date': date\n",
    "        }\n",
    "        if type == 'CampaignContribution':\n",
    "            if direction == 1:\n",
    "                super().__init__(donor, recipient, 'donated_to', attributes)\n",
    "            else:\n",
    "                super().__init__(recipient, donor, 'has_donation', attributes)\n",
    "        else:\n",
    "            if direction == 1:\n",
    "                super().__init__(donor, recipient, 'lobbied', attributes)\n",
    "            else:\n",
    "                super().__init__(recipient, donor, 'has_lobbying', attributes)\n",
    "\n",
    "class Version(Edge):\n",
    "    def __init__(self, bill_version, bill, direction):\n",
    "        if direction == 1:\n",
    "            super().__init__(bill_version, bill, 'is_version')\n",
    "        else:\n",
    "            super().__init__(bill, bill_version, 'has_version')\n",
    "\n",
    "class siblingVersion(Edge):\n",
    "    def __init__(self, version1, version2, direction):\n",
    "        if direction == 1:\n",
    "            super().__init__(version1, version2, 'priorVersion')\n",
    "        else:\n",
    "            super().__init__(version2, version1, 'nextVersion')\n",
    "\n",
    "class samePerson(Edge):\n",
    "    def __init__(self, node1, node2):\n",
    "        super().__init__(node1, node2, 'samePerson')\n",
    "\n",
    "class Expenditure(Edge):\n",
    "    def __init__(self, donor, bill, amount, date, stance):\n",
    "        attributes = {\n",
    "            \"amount\": amount,\n",
    "            \"stance\": stance,\n",
    "            \"date\": date\n",
    "        }\n",
    "        super().__init__(donor, bill, 'expenditure', attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphBuilder:\n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "        self.edges = []\n",
    "        self.versions = []\n",
    "        self.nodes_by_type = defaultdict(list)\n",
    "        self.edges_by_type = defaultdict(list)\n",
    "        self._type_counters = defaultdict(int)\n",
    "\n",
    "    def add_version(self, version):\n",
    "        self.versions.append(version)\n",
    "\n",
    "    def add_node(self, node):\n",
    "        key = (node.type, node.id)\n",
    "        if key not in self.nodes:\n",
    "            self.nodes[key] = node\n",
    "        idx = self._type_counters[node.type]\n",
    "        self._type_counters[node.type] += 1\n",
    "        self.nodes_by_type[node.type].append(node)\n",
    "\n",
    "    def get_node(self, type_, id_):\n",
    "        return self.nodes.get((type_, id_))\n",
    "\n",
    "    def add_edge(self, edge):\n",
    "        self.edges.append(edge)\n",
    "        etype = (edge.source.type, edge.relation, edge.target.type)\n",
    "        self.edges_by_type[etype].append(edge)\n",
    "\n",
    "    def build(self):\n",
    "        return {\n",
    "            \"nodes\": list(self.nodes.values()),\n",
    "            \"edges\": self.edges,\n",
    "            \"nodes_by_type\": self.nodes_by_type,\n",
    "            \"edges_by_type\": self.edges_by_type\n",
    "        }\n",
    "\n",
    "    def add_bills(self, bill_ids, titles, subjects, titles_embs, subjects_embs, features):\n",
    "        def process_single_bill(bill):\n",
    "            try:\n",
    "                title = text_clean(titles.get(bill, ''))\n",
    "                subject = text_clean(subjects.get(bill, ''))\n",
    "                title_emb = titles_embs.get(title, None)\n",
    "                subject_emb = subjects_embs.get(subject, None)\n",
    "                measure_type = re.search(r'[A-Za-z]+', bill).group()\n",
    "                bill_node = Bill(bill, title_emb, subject_emb, measure_type)\n",
    "\n",
    "                versions = version_id_mapping.get(bill, [])\n",
    "                versions_ = []\n",
    "                dates = introduction_dates.get(bill, {}).get('Dates', [])\n",
    "                try:\n",
    "                    fd = sorted(list(set(dates)))[0]\n",
    "                except IndexError:\n",
    "                    y = int(bill[:4])\n",
    "                    fd = pd.Timestamp(year=y, month=2, day=1)\n",
    "                bill_node.add_date(fd)\n",
    "\n",
    "                if not versions:\n",
    "                    return\n",
    "\n",
    "                self.add_node(bill_node)\n",
    "\n",
    "                for version in versions:\n",
    "                    digest = features[version]['digest']\n",
    "                    if str(digest) == 'nan' or version.endswith('VETO'):\n",
    "                        continue\n",
    "                    digest = text_clean(digest)\n",
    "                    digest_emb = digest_embeddings.get(digest, None)\n",
    "                    if digest_emb is None:\n",
    "                        continue\n",
    "\n",
    "                    version_node = BillVersion(\n",
    "                        bill, version, digest_emb,\n",
    "                        features[version]['VoteRequired'],\n",
    "                        features[version]['LocalProgram'],\n",
    "                        features[version]['FiscalCommittee'],\n",
    "                        features[version]['TaxLevy'],\n",
    "                        features[version]['Urgency']\n",
    "                    )\n",
    "\n",
    "                    self.add_node(version_node)\n",
    "                    if version not in self.versions:\n",
    "                        self.versions.append(version)\n",
    "                    versions_.append(version)\n",
    "\n",
    "\n",
    "                orders = [vnums.get(v) for v in versions]\n",
    "                sorted_versions = [s for _, s in sorted(zip(orders, versions))]\n",
    "                for i, s in enumerate(sorted_versions):\n",
    "                    v = self.get_node('bill_version', s)\n",
    "                    if v is None or bill_node is None:\n",
    "                        continue\n",
    "                    self.add_edge(Version(v, bill_node, 1))\n",
    "                    if i > 0:\n",
    "                        prev_v = self.get_node('bill_version', sorted_versions[i - 1])\n",
    "                        if prev_v is not None:\n",
    "                            self.add_edge(siblingVersion(prev_v, v, 1))\n",
    "                o = outcome.get(bill, 0)\n",
    "                bill_node.add_outcome(o)\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error processing bill {bill}: {e}\")\n",
    "\n",
    "        for bill in tqdm(bill_ids):\n",
    "            process_single_bill(bill)\n",
    "\n",
    "    def add_legislators(self, legislators_):\n",
    "        for legislator in tqdm(legislators_):\n",
    "            leg_name = legislators[legislator]\n",
    "            party = leg_parties.get(leg_name)\n",
    "            legislator_node = Legislator(legislator, party)\n",
    "            self.add_node(legislator_node)\n",
    "            terms = politicians.loc[politicians['full_name'] == leg_name, ['Term', 'District No.', 'chamber']].drop_duplicates()\n",
    "            for _, term in terms.iterrows():\n",
    "                term_node = LegislatorTerm(term['Term'], legislator, term['chamber'], term['District No.'])\n",
    "                self.add_node(term_node)\n",
    "                self.add_edge(samePerson(term_node, legislator_node))\n",
    "\n",
    "    def add_committees(self, committees_df):\n",
    "        for _, row in tqdm(committees_df[['committee_clean', 'Term', 'chamber']].drop_duplicates().iterrows(), total=committees_df[['committee_clean', 'Term', 'chamber']].drop_duplicates().shape[0]):\n",
    "            committee_name = committee_embeddings.get(row['committee_clean'].lower(), None)\n",
    "            committee_id = committee_codes.get(row['committee_clean'].lower(), None)\n",
    "            chamber = chamber_encoder.transform([row['chamber']])[0] if row['chamber'] in chamber_encoder.classes_ else -1\n",
    "            committee_node = Committee(committee_id, committee_name, chamber, row['Term'])\n",
    "            self.add_node(committee_node)\n",
    "            term = row['Term']\n",
    "            members = politicians.loc[(politicians['committee_clean'] == row['committee_clean']) & (politicians['Term'] == row['Term']), ['position', 'full_name', 'chamber']].drop_duplicates()\n",
    "            for _, member in members.iterrows():\n",
    "                leg_id = legislator_codes[member['full_name']]\n",
    "                chamber = chamber_encoder.transform([member['chamber']])[0] if member['chamber'] in chamber_encoder.classes_ else -1\n",
    "                leg_node_id = f\"{leg_id}_{term}_{chamber}\"\n",
    "                leg_node = self.get_node('legislator_term', leg_node_id)\n",
    "                if (leg_node is not None) & (committee_node is not None):\n",
    "                    self.add_edge(CommitteeMembership(leg_node, committee_node, member['position'], 1))\n",
    "                    committee_node.add_member(leg_node_id)\n",
    "                    leg_node.add_committee(committee_name)\n",
    "\n",
    "    def add_votes(self):\n",
    "        for _, row in tqdm(voting.loc[voting['bill_ID'].isin(bill_ids)].iterrows(), total=voting.loc[voting['bill_ID'].isin(bill_ids)].shape[0]):\n",
    "            bv_id = row['bv_id']\n",
    "            v_node = self.get_node('bill_version', bv_id)\n",
    "            if v_node is None:\n",
    "                continue\n",
    "            last = row['legislator_name'].strip().lower()\n",
    "            house = chamber_encoder.transform([row['chamber']])[0] if row['chamber'] in chamber_encoder.classes_ else -1\n",
    "            legislator = legislators_last_names.get((row['chamber'].lower(), last, row['term']), None)\n",
    "            if legislator is None:\n",
    "                if len(last.split(' ')) > 1:\n",
    "                    legislator = row['legislator_name']\n",
    "                else:\n",
    "                    continue\n",
    "            legislator_id = legislator_codes.get(legislator, None)\n",
    "            leg_term_node = self.get_node('legislator_term', f\"{legislator_id}_{row['term']}_{house}\")\n",
    "            if leg_term_node is None:\n",
    "                continue\n",
    "            vote = row['vote_code']\n",
    "            motion_id = row['motion_id']\n",
    "            motion_text = motion_codes.get(motion_id, None)\n",
    "            if motion_text is None:\n",
    "                continue\n",
    "            if row['location_code'] not in ['AFLOOR', 'SFLOOR']:\n",
    "                actions = v_node.actions\n",
    "                if row['location_code'] in actions:\n",
    "                    if row['vote_date_time'] not in actions[row['location_code']]:\n",
    "                        actions[row['location_code']].append(row['vote_date_time'])\n",
    "            if motion_text is None:\n",
    "                motion_embedding = ''\n",
    "            else:\n",
    "                motion_embedding = motion_embeddings.get(motion_text, None)\n",
    "            self.add_edge(Vote(leg_term_node, v_node, vote, motion_embedding, row['vote_date_time'], 1))\n",
    "\n",
    "\n",
    "    def add_readings(self):\n",
    "        for _, row in tqdm(hear.loc[hear['bill_id'].isin(bill_ids)].iterrows(), total=hear.loc[hear['bill_id'].isin(bill_ids)].shape[0]):\n",
    "            b_node = self.get_node('bill', row['bill_id'])\n",
    "            if b_node is None:\n",
    "                continue\n",
    "            location = row['committee_clean']\n",
    "            if location is None or location == '':\n",
    "                continue\n",
    "            term = row['year']\n",
    "            committee_id = committee_codes.get(str(location).lower(), None)\n",
    "            if committee_id is None:\n",
    "                continue\n",
    "            committee_node = self.get_node('committee', f\"{committee_id}_{term}\")\n",
    "            if committee_node is None:\n",
    "                continue\n",
    "            self.add_edge(Reading(committee_node, b_node, term, 1))\n",
    "        for _, row in tqdm(voting.loc[(voting['bv_id'].notna()) & (voting['voting_place'].notna()) & (voting['bill_ID'].isin(bill_ids)), ['bv_id', 'Date', 'voting_place']].drop_duplicates().iterrows()):\n",
    "            bv_node = self.get_node('bill_version', row['bv_id'])\n",
    "            if bv_node is None:\n",
    "                continue\n",
    "            location = row['voting_place']\n",
    "            if location is None or location == '':\n",
    "                continue\n",
    "            date = pd.Timestamp(row['Date'])\n",
    "            committee_id = committee_codes.get(str(location).lower(), None)\n",
    "            if committee_id is None:\n",
    "                continue\n",
    "            year = int(date.year)\n",
    "            committee_node = self.get_node('committee', f\"{committee_id}_{year}\")\n",
    "            if committee_node is None:\n",
    "                continue\n",
    "            self.add_edge(Reading(committee_node, bv_node, date, 1))\n",
    "\n",
    "\n",
    "    def add_sponsorships(self, sponsors):\n",
    "        for _, row in tqdm(sponsors.loc[sponsors['Name'].apply(lambda x: isinstance(x, str))].iterrows(), total=sponsors.shape[0]):\n",
    "            version = row['bill_ID']\n",
    "            version_node = self.get_node('bill_version', version)\n",
    "            if version_node is None:\n",
    "                continue\n",
    "            if row['House'] == 'UNKNOWN':\n",
    "                com = author_com_matches.get(row['Name'], None)\n",
    "                year = row['term'].split('-')[0]\n",
    "                com_node = self.get_node('committee', f\"{com}_{year}\")\n",
    "                if com_node is None:\n",
    "                   continue\n",
    "                self.add_edge(Sponsorship(com_node, version_node, row['Contribution'], 1))\n",
    "                continue\n",
    "            else:\n",
    "                if row['Name'].strip() in ['Mark Stone', 'Cristina Garcia', 'John Campbell', 'Bill Campbell', 'Eduardo Garcia']:\n",
    "                    leg_name = row['Name']\n",
    "                else:\n",
    "                    leg_name = row['full_name']\n",
    "                    if leg_name is None or leg_name == '':\n",
    "                        continue\n",
    "                house = chamber_encoder.transform([row['House'].lower()])[0] if row['House'].lower() in chamber_encoder.classes_ else -1\n",
    "                leg_id = legislator_codes.get(leg_name)\n",
    "                leg_node = self.get_node('legislator_term', f\"{leg_id}_{row['term']}_{house}\")\n",
    "                if leg_node is None:\n",
    "                    continue\n",
    "\n",
    "                self.add_edge(Sponsorship(leg_node, version_node, row['Contribution'], 1))\n",
    "\n",
    "\n",
    "    def add_lobbyists(self, lobbyists):\n",
    "        for key in tqdm(lobbyists.keys(), total=len(lobbyists.keys())):\n",
    "            lobbyist = LobbyFirm(text_clean(key), lobbyists[key])\n",
    "            self.add_node(lobbyist)\n",
    "\n",
    "    def add_lobbying(self, donations):\n",
    "        for _, row in tqdm(donations.iterrows(), total=donations.shape[0]):\n",
    "            firm = row['firm']\n",
    "            firm_node = self.get_node('lobby_firm', firm)\n",
    "            if firm_node is None:\n",
    "                continue\n",
    "            if row['clean_beneficiary'] in committee_codes:\n",
    "                com = committee_codes.get(row['clean_beneficiary'])\n",
    "                year = row['term'].split('-')[0]\n",
    "                com_node = self.get_node('committee', f\"{com}_{year}\")\n",
    "                if com_node is None:\n",
    "                    continue\n",
    "                for y in [1, -1]:\n",
    "                    self.add_edge(Donation(firm_node, com_node, row['BENE_AMT'], row['EXPN_DATE'], 'Lobbying', y))\n",
    "                firm_node.add_donation(row['BENE_AMT'])\n",
    "\n",
    "            else:\n",
    "                dicti = pol_names_terms2.get((row['clean_beneficiary'], row['term']), None)\n",
    "                if dicti is None:\n",
    "                    dicti = pol_names_terms.get((row['clean_beneficiary'], row['term']), None)\n",
    "                chamber = dicti['chamber'] if dicti is not None else None\n",
    "                name = dicti['name'] if dicti is not None else None\n",
    "                name = name_fix.get(name, name)\n",
    "                if chamber is None or name is None:\n",
    "                    continue\n",
    "                chamber = chamber_encoder.transform([chamber])[0] if chamber in chamber_encoder.classes_ else -1\n",
    "                leg_id = legislator_codes.get(name)\n",
    "                leg_node = self.get_node('legislator_term', f\"{leg_id}_{row['term']}_{chamber}\")\n",
    "                if leg_node is None:\n",
    "                    continue\n",
    "                for y in [1, -1]:\n",
    "                    self.add_edge(Donation(firm_node, leg_node, row['BENE_AMT'], row['EXPN_DATE'], 'Lobbying', y))\n",
    "                firm_node.add_donation(row['BENE_AMT'])\n",
    "\n",
    "    def add_donors(self, donors):\n",
    "\n",
    "        for donor in tqdm(donors.keys(), total=len(donors.keys())):\n",
    "            donor_embedding = donors[donor]\n",
    "            donor = Donor(donor, donor_embedding)\n",
    "            self.add_node(donor)\n",
    "\n",
    "    def add_contributions(self, contributions):\n",
    "        for _, row in tqdm(contributions.iterrows(), total=contributions.shape[0]):\n",
    "            expender = row['Expender']\n",
    "            expender_node = self.get_node('donor', expender)\n",
    "            if expender_node is None:\n",
    "                continue\n",
    "            recipient = row['matched_target_name']\n",
    "            recipient_id = legislator_codes.get(recipient)\n",
    "            chamber = chamber_encoder.transform([row['chamber']])[0] if row['chamber'] in chamber_encoder.classes_ else -1\n",
    "            recipient_node = self.get_node('legislator_term', f\"{recipient_id}_{row['Term']}_{chamber}\")\n",
    "            if recipient_node is None:\n",
    "                continue\n",
    "            for y in [1, -1]:\n",
    "                self.add_edge(Donation(expender_node, recipient_node, row['Amount'], row['DateEnd'], 'CampaignContribution', y))\n",
    "            expender_node.add_donation(row['Amount'])\n",
    "\n",
    "    def add_expenditures(self):\n",
    "        for _, row in tqdm(ie.iterrows(), total=ie.shape[0]):\n",
    "            expender = row['Expender']\n",
    "            expender_node = self.get_node('donor', expender)\n",
    "            if expender_node is None:\n",
    "                continue\n",
    "            bills = row['bill_matches']\n",
    "            denom = len(bills)\n",
    "            for b in bills:\n",
    "                end = re.search(r'(?<=[A-Z])\\d+$', b).group()\n",
    "                new = re.sub(r'(?<=[A-Z])\\d+$', str(int(end)), b)\n",
    "                bill_node = self.get_node('bill', new)\n",
    "                if bill_node is None:\n",
    "                    continue\n",
    "                self.add_edge(Expenditure(expender_node, bill_node, row['Amount'] / denom, row['DateEnd'], row['ExpenderPosition']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73769/73769 [07:03<00:00, 174.37it/s]\n",
      "100%|██████████| 505/505 [00:01<00:00, 340.54it/s]\n",
      "100%|█████████▉| 736238/736241 [01:15<00:00, 9700.14it/s] \n"
     ]
    }
   ],
   "source": [
    "builder = GraphBuilder()\n",
    "builder.add_bills(bill_ids, bill_titles, bill_subjects, title_embeddings, subject_embeddings, features)\n",
    "builder.add_legislators(legislators)\n",
    "builder.add_sponsorships(sponsors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1699/1699 [00:07<00:00, 218.11it/s]\n",
      "100%|██████████| 1025/1025 [00:00<00:00, 135890.31it/s]\n",
      "100%|██████████| 121019/121019 [00:12<00:00, 9533.79it/s] \n",
      "100%|██████████| 780/780 [00:00<00:00, 222222.33it/s]\n",
      "100%|██████████| 21422/21422 [00:02<00:00, 7746.34it/s] \n",
      "100%|██████████| 1587/1587 [00:00<00:00, 19995.98it/s]\n"
     ]
    }
   ],
   "source": [
    "builder.add_committees(politicians)\n",
    "builder.add_lobbyists(lobbying_firms_embeddings)\n",
    "builder.add_lobbying(lob)\n",
    "builder.add_donors(donor_embeddings)\n",
    "builder.add_contributions(campaign_contributions)\n",
    "builder.add_expenditures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6094629/6094629 [10:27<00:00, 9712.27it/s] \n",
      "100%|██████████| 132400/132400 [00:03<00:00, 42830.49it/s]\n",
      "101741it [00:02, 39864.66it/s]\n"
     ]
    }
   ],
   "source": [
    "builder.add_votes()\n",
    "builder.add_readings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = graph_data['nodes']\n",
    "edges = graph_data['edges']\n",
    "nodes_by_type = graph_data['nodes_by_type']\n",
    "edges_by_type = graph_data['edges_by_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NODES\n",
      "\n",
      "\n",
      "bill num_nodes: 63100\n",
      "bill_version num_nodes: 222412\n",
      "legislator num_nodes: 505\n",
      "legislator_term num_nodes: 1444\n",
      "committee num_nodes: 1699\n",
      "lobby_firm num_nodes: 1025\n",
      "donor num_nodes: 780\n",
      "EDGES\n",
      "\n",
      "\n",
      "('bill_version', 'is_version', 'bill') num_edges: 222412\n",
      "('bill_version', 'priorVersion', 'bill_version') num_edges: 160768\n",
      "('legislator_term', 'samePerson', 'legislator') num_edges: 1444\n",
      "('legislator_term', 'wrote', 'bill_version') num_edges: 610356\n",
      "('legislator_term', 'member_of', 'committee') num_edges: 17565\n",
      "('lobby_firm', 'lobbied', 'legislator_term') num_edges: 82617\n",
      "('legislator_term', 'has_lobbying', 'lobby_firm') num_edges: 82617\n",
      "('lobby_firm', 'lobbied', 'committee') num_edges: 3982\n",
      "('committee', 'has_lobbying', 'lobby_firm') num_edges: 3982\n",
      "('donor', 'donated_to', 'legislator_term') num_edges: 6498\n",
      "('legislator_term', 'has_donation', 'donor') num_edges: 6498\n",
      "('donor', 'expenditure', 'bill') num_edges: 549\n",
      "('legislator_term', 'voted_on', 'bill_version') num_edges: 5052552\n",
      "('bill_version', 'read', 'committee') num_edges: 51041\n"
     ]
    }
   ],
   "source": [
    "print('NODES')\n",
    "print('\\n')\n",
    "for n, v in nodes_by_type.items():\n",
    "    print(n, f\"num_nodes: {len(v)}\")\n",
    "\n",
    "print(\"EDGES\")\n",
    "print(\"\\n\")\n",
    "for e, v in edges_by_type.items():\n",
    "    print(e, f\"num_edges: {len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_subject_embeddings = {v: k for k, v in subject_embeddings.items()}\n",
    "bill_subjects = {}\n",
    "\n",
    "for node in nodes_by_type['bill']:\n",
    "    if node.features['subject'] in rev_subject_embeddings:\n",
    "        subj = rev_subject_embeddings[node.features['subject']]\n",
    "        bill_subjects[node.id] = subj\n",
    "\n",
    "with open('bill_subjects.json', 'w') as f:\n",
    "    json.dump(bill_subjects, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/jkjxx5pj447bytbqv8pkw9nh0000gn/T/ipykernel_93740/1650919632.py:87: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return sec, int(datetime.datetime.utcfromtimestamp(sec).year)\n"
     ]
    }
   ],
   "source": [
    "TIME_FIELDS_NODES = {\n",
    "    'bill': {'date': 'datetime'},\n",
    "    'bill_version': {'date': 'datetime'},\n",
    "    'legislator_term': {'term': 'year'},\n",
    "}\n",
    "\n",
    "TIME_FIELDS_EDGES = {\n",
    "    ('legislator_term', 'voted_on', 'bill_version'): {'date': 'datetime'},\n",
    "    ('donor', 'donated_to', 'legislator_term'): {'date': 'datetime'},\n",
    "    ('lobby_firm', 'lobbied', 'legislator_term'): {'date': 'datetime'},\n",
    "    ('lobby_firm', 'lobbied', 'committee'): {'date': 'datetime'},\n",
    "    ('committee', 'read', 'bill'): {'date': 'year_or_datetime'},\n",
    "    ('donor', 'expenditure', 'bill'): {'date': 'datetime'}\n",
    "}\n",
    "\n",
    "\n",
    "def _stable_hash_to_int(s):\n",
    "    return int(hashlib.blake2s(s.encode('utf-8'), digest_size=4).hexdigest(), 16)\n",
    "\n",
    "def _looks_like_datetime_string(s):\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    if re.search(r'\\d{4}-\\d{1,2}-\\d{1,2}', s):\n",
    "        return True\n",
    "    if re.search(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', s):\n",
    "        return True\n",
    "    if re.search(r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)', s.lower()) and re.search(r'\\d{4}', s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _parse_year_from_term(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, (int, np.integer)) and 1800 <= int(v) <= 2200:\n",
    "        return int(v)\n",
    "    if isinstance(v, float) and 1800 <= int(v) <= 2200:\n",
    "        return int(v)\n",
    "    if isinstance(v, str):\n",
    "        m = re.search(r'\\b(18|19|20)\\d{2}\\b', v)\n",
    "        if m:\n",
    "            return int(m.group(0))\n",
    "    return None\n",
    "\n",
    "def _to_epoch_seconds_from_datetime_like(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(v, (pd.Timestamp, np.datetime64)):\n",
    "            ts = pd.to_datetime(v, errors='coerce')\n",
    "            return None if pd.isna(ts) else float(ts.value) / 1e9\n",
    "    except Exception:\n",
    "        pass\n",
    "    if isinstance(v, datetime.datetime):\n",
    "        return float(v.timestamp())\n",
    "    if isinstance(v, str):\n",
    "        if _looks_like_datetime_string(v):\n",
    "            ts = pd.to_datetime(v, errors='coerce', utc=False)\n",
    "            return None if pd.isna(ts) else float(ts.value) / 1e9\n",
    "        try:\n",
    "            v_num = float(v)\n",
    "        except Exception:\n",
    "            return None\n",
    "        v = v_num\n",
    "    if isinstance(v, (int, float, np.integer, np.floating)):\n",
    "        x = float(v)\n",
    "        if x > 1e11:\n",
    "            return x / 1000.0\n",
    "        if 1e9 <= x <= 2e10:\n",
    "            return x\n",
    "    return None\n",
    "\n",
    "def _midyear_epoch_seconds(year):\n",
    "    dt = datetime.datetime(year=int(year), month=6, day=30, hour=12, minute=0, second=0)\n",
    "    return float(dt.timestamp())\n",
    "\n",
    "def _normalize_time_value(v, kind):\n",
    "    if kind == 'year':\n",
    "        y = _parse_year_from_term(v)\n",
    "        if y is None:\n",
    "            return None, None\n",
    "        return _midyear_epoch_seconds(y), int(y)\n",
    "    if kind == 'datetime':\n",
    "        sec = _to_epoch_seconds_from_datetime_like(v)\n",
    "        if sec is None:\n",
    "            return None, None\n",
    "        return sec, int(datetime.datetime.utcfromtimestamp(sec).year)\n",
    "    if kind == 'year_or_datetime':\n",
    "        y = _parse_year_from_term(v)\n",
    "        if y is not None:\n",
    "            return _midyear_epoch_seconds(y), int(y)\n",
    "        sec = _to_epoch_seconds_from_datetime_like(v)\n",
    "        if sec is not None:\n",
    "            return sec, int(datetime.datetime.utcfromtimestamp(sec).year)\n",
    "        return None, None\n",
    "    return None, None\n",
    "\n",
    "\n",
    "_VECTOR_DICT_KEYS = ('embedding', 'vector', 'values', 'data', 'array')\n",
    "\n",
    "def _maybe_unwrap_dict_container(x):\n",
    "    if isinstance(x, dict):\n",
    "        for k in _VECTOR_DICT_KEYS:\n",
    "            if k in x:\n",
    "                return x[k]\n",
    "    return x\n",
    "\n",
    "def _parse_numeric_string_vector(s):\n",
    "    s2 = s.strip()\n",
    "    if not s2:\n",
    "        return None\n",
    "    if s2.startswith('[') and s2.endswith(']'):\n",
    "        try:\n",
    "            arr = json.loads(s2)\n",
    "            return _to_float_array(arr)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if (',' in s2) or (re.search(r'\\s', s2) and len(s2.split()) > 1):\n",
    "        toks = [t for t in re.split(r'[,\\s]+', s2) if t]\n",
    "        try:\n",
    "            vals = [float(t) for t in toks]\n",
    "            return np.asarray(vals, dtype=np.float32)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def _to_float_array(x):\n",
    "    x = _maybe_unwrap_dict_container(x)\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        try:\n",
    "            arr = x.detach().cpu().numpy().astype(np.float32).reshape(-1)\n",
    "            return arr\n",
    "        except Exception:\n",
    "            return None\n",
    "    if isinstance(x, pd.Series):\n",
    "        try:\n",
    "            return np.asarray(x.to_numpy(), dtype=np.float32).reshape(-1)\n",
    "        except Exception:\n",
    "            return None\n",
    "    if isinstance(x, np.ndarray):\n",
    "        if x.dtype.kind in 'iuf':\n",
    "            return x.astype(np.float32).reshape(-1)\n",
    "        try:\n",
    "            flat = x.reshape(-1)\n",
    "            vals = [float(v) for v in flat]\n",
    "            return np.asarray(vals, dtype=np.float32)\n",
    "        except Exception:\n",
    "            return None\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        try:\n",
    "            arr = np.asarray(x, dtype=np.float32)\n",
    "            if arr.dtype.kind in 'iuf':\n",
    "                return arr.reshape(-1)\n",
    "            flat = np.array([float(v) for v in _flatten_once(x)], dtype=np.float32)\n",
    "            return flat.reshape(-1)\n",
    "        except Exception:\n",
    "            try:\n",
    "                vals = [float(v) for v in _flatten_once(x)]\n",
    "                return np.asarray(vals, dtype=np.float32).reshape(-1)\n",
    "            except Exception:\n",
    "                return None\n",
    "    if isinstance(x, (int, float, np.integer, np.floating)) and not isinstance(x, bool):\n",
    "        return np.asarray([float(x)], dtype=np.float32)\n",
    "    if isinstance(x, str):\n",
    "        vec = _parse_numeric_string_vector(x)\n",
    "        if vec is not None:\n",
    "            return vec.reshape(-1)\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def _flatten_once(seq):\n",
    "    for el in seq:\n",
    "        if isinstance(el, (list, tuple, np.ndarray)):\n",
    "            for sub in (el if isinstance(el, (list, tuple)) else el.tolist()):\n",
    "                yield sub\n",
    "        else:\n",
    "            yield el\n",
    "\n",
    "def _as1d_numeric(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, bool):\n",
    "        return np.array([1.0 if x else 0.0], dtype=np.float32)\n",
    "    if isinstance(x, (pd.Timestamp, np.datetime64, datetime.datetime, str)):\n",
    "        return None\n",
    "    return _to_float_array(x)\n",
    "\n",
    "def _collect_keys_allow_all(dicts):\n",
    "    keys = set()\n",
    "    for d in dicts:\n",
    "        if not d:\n",
    "            continue\n",
    "        keys.update(d.keys())\n",
    "    return sorted(keys)\n",
    "\n",
    "def _key_dims_with_default(dicts, keys, default_dim=1):\n",
    "    dims = {}\n",
    "    for k in keys:\n",
    "        m = 0\n",
    "        for d in dicts:\n",
    "            v = d.get(k, None)\n",
    "            arr = _as1d_numeric(v)\n",
    "            if arr is not None:\n",
    "                m = max(m, int(arr.shape[0]))\n",
    "        dims[k] = m if m > 0 else default_dim\n",
    "    return dims\n",
    "\n",
    "def _fit_categorical_maps(dicts, keys, dims, max_card_per_key=50000):\n",
    "    cat_maps = {}\n",
    "    for k in keys:\n",
    "        if dims[k] != 1:\n",
    "            continue\n",
    "        seen = {}\n",
    "        nid = 1  # 0 = missing\n",
    "        for d in dicts:\n",
    "            v = d.get(k, None)\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, str) and not _looks_like_datetime_string(v) and _parse_numeric_string_vector(v) is None:\n",
    "                if v not in seen:\n",
    "                    if len(seen) < max_card_per_key:\n",
    "                        seen[v] = nid\n",
    "                        nid += 1\n",
    "        if seen:\n",
    "            cat_maps[k] = seen\n",
    "    return cat_maps\n",
    "\n",
    "def _encode_scalar_from_category(v, k, cat_maps):\n",
    "    if v is None:\n",
    "        return 0.0\n",
    "    if isinstance(v, bool):\n",
    "        return 1.0 if v else 0.0\n",
    "    if isinstance(v, (int, float, np.integer, np.floating)) and not isinstance(v, bool):\n",
    "        return float(v)\n",
    "    if isinstance(v, str) and not _looks_like_datetime_string(v) and _parse_numeric_string_vector(v) is None:\n",
    "        mapping = cat_maps.get(k, None)\n",
    "        if mapping is not None:\n",
    "            return float(mapping.get(v, 0))\n",
    "        return float((_stable_hash_to_int(v) % 65535) + 1)\n",
    "    return 0.0\n",
    "\n",
    "def _stack_with_categories(dicts, keys, dims, cat_maps):\n",
    "    if not keys:\n",
    "        return torch.empty((len(dicts), 0), dtype=torch.float32)\n",
    "    rows = []\n",
    "    for d in dicts:\n",
    "        parts = []\n",
    "        for k in keys:\n",
    "            v = d.get(k, None)\n",
    "            arr = _as1d_numeric(v)\n",
    "            if arr is None:\n",
    "                if dims[k] == 1:\n",
    "                    parts.append(np.array([_encode_scalar_from_category(v, k, cat_maps)], dtype=np.float32))\n",
    "                else:\n",
    "                    parts.append(np.zeros(dims[k], dtype=np.float32))\n",
    "            else:\n",
    "                if arr.shape[0] < dims[k]:\n",
    "                    pad = np.zeros(dims[k], dtype=np.float32)\n",
    "                    pad[:arr.shape[0]] = arr\n",
    "                    arr = pad\n",
    "                elif arr.shape[0] > dims[k]:\n",
    "                    arr = arr[:dims[k]]\n",
    "                parts.append(arr.astype(np.float32))\n",
    "        rows.append(np.concatenate(parts, axis=0))\n",
    "    mat = np.vstack(rows).astype(np.float32)\n",
    "    mat = np.nan_to_num(mat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return torch.from_numpy(mat)\n",
    "\n",
    "\n",
    "def _prep_node_dicts(ntype, nlist):\n",
    "    out = []\n",
    "    time_spec = TIME_FIELDS_NODES.get(ntype, {})\n",
    "    for n in nlist:\n",
    "        d = dict((n.features or {}))\n",
    "        for field, kind in time_spec.items():\n",
    "            if field in d:\n",
    "                sec, yr = _normalize_time_value(d.get(field, None), kind)\n",
    "                d.pop(field, None)\n",
    "                d[f'{field}_sec'] = sec if sec is not None else 0.0\n",
    "                d[f'{field}_year'] = float(yr) if yr is not None else 0.0\n",
    "        out.append(d)\n",
    "    return out\n",
    "\n",
    "def _prep_edge_dicts_filtered(etype, elist, id_map_src, id_map_dst):\n",
    "    src_idx = []\n",
    "    dst_idx = []\n",
    "    dicts = []\n",
    "    time_spec = TIME_FIELDS_EDGES.get(etype, {})\n",
    "    for e in elist:\n",
    "        si = id_map_src.get(e.source.id, None)\n",
    "        di = id_map_dst.get(e.target.id, None)\n",
    "        if si is None or di is None:\n",
    "            continue\n",
    "        d = dict((e.attributes or {}))\n",
    "        for field, kind in time_spec.items():\n",
    "            if field in d:\n",
    "                sec, yr = _normalize_time_value(d.get(field, None), kind)\n",
    "                d.pop(field, None)\n",
    "                d[f'{field}_sec'] = sec if sec is not None else 0.0\n",
    "                d[f'{field}_year'] = float(yr) if yr is not None else 0.0\n",
    "        src_idx.append(si)\n",
    "        dst_idx.append(di)\n",
    "        dicts.append(d)\n",
    "    return src_idx, dst_idx, dicts\n",
    "\n",
    "def _load_bill_labels(path: str):\n",
    "    with open(path, 'r') as f:\n",
    "        raw = json.load(f)\n",
    "    vals = list(raw.values())\n",
    "    if all(isinstance(v, (int, np.integer)) for v in vals):\n",
    "        label_map = {}\n",
    "        inv = None\n",
    "    else:\n",
    "        uniq = sorted(set(map(str, vals)))\n",
    "        label_map = {s: i for i, s in enumerate(uniq)}\n",
    "        inv = {i: s for s, i in label_map.items()}\n",
    "    return raw, label_map, inv\n",
    "\n",
    "def build_heterodata_compact_with_time_and_labels(gb, labels_json_path: str):\n",
    "    data = HeteroData()\n",
    "\n",
    "\n",
    "    id_maps = {\n",
    "        ntype: {n.id: i for i, n in enumerate(nlist)}\n",
    "        for ntype, nlist in gb.nodes_by_type.items()\n",
    "    }\n",
    "\n",
    "    # Nodes\n",
    "    for ntype, nlist in gb.nodes_by_type.items():\n",
    "        if not nlist:\n",
    "            continue\n",
    "        dicts = _prep_node_dicts(ntype, nlist)\n",
    "        keys = _collect_keys_allow_all(dicts)\n",
    "        dims = _key_dims_with_default(dicts, keys, default_dim=1)\n",
    "        cat_maps = _fit_categorical_maps(dicts, keys, dims)\n",
    "        x = _stack_with_categories(dicts, keys, dims, cat_maps)\n",
    "        if x.numel() == 0:\n",
    "            data[ntype].num_nodes = len(nlist)\n",
    "        else:\n",
    "            data[ntype].x = x\n",
    "\n",
    "    # Edges\n",
    "    for etype, elist in gb.edges_by_type.items():\n",
    "        st, rel, dt = etype\n",
    "        if not elist or st not in id_maps or dt not in id_maps:\n",
    "            continue\n",
    "        id_map_src = id_maps[st]\n",
    "        id_map_dst = id_maps[dt]\n",
    "        src, dst, dicts = _prep_edge_dicts_filtered(etype, elist, id_map_src, id_map_dst)\n",
    "        if not src:\n",
    "            continue\n",
    "        edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "        keys = _collect_keys_allow_all(dicts)\n",
    "        dims = _key_dims_with_default(dicts, keys, default_dim=1)\n",
    "        cat_maps = _fit_categorical_maps(dicts, keys, dims)\n",
    "        edge_attr = _stack_with_categories(dicts, keys, dims, cat_maps)\n",
    "        data[etype].edge_index = edge_index\n",
    "        if edge_attr.numel() > 0:\n",
    "            data[etype].edge_attr = edge_attr\n",
    "\n",
    "    # Bill labels/outcomes\n",
    "    if 'bill' in gb.nodes_by_type and gb.nodes_by_type['bill']:\n",
    "        raw_labels, label_map, _ = _load_bill_labels(labels_json_path)\n",
    "        bills = gb.nodes_by_type['bill']\n",
    "        clusters = []\n",
    "        ys = []\n",
    "        for n in bills:\n",
    "            bid = n.id\n",
    "            v = raw_labels.get(bid, None)\n",
    "            if v is None:\n",
    "                clusters.append(-1)\n",
    "            else:\n",
    "                if label_map:\n",
    "                    clusters.append(label_map.get(str(v), -1))\n",
    "                else:\n",
    "                    try:\n",
    "                        clusters.append(int(v))\n",
    "                    except:\n",
    "                        clusters.append(-1)\n",
    "            yv = getattr(n, 'outcome', None)\n",
    "            ys.append(-1 if yv is None else int(yv))\n",
    "        data['bill'].cluster = torch.tensor(clusters, dtype=torch.long)\n",
    "        data['bill'].y = torch.tensor(ys, dtype=torch.long)\n",
    "\n",
    "    # Final numeric cleanup\n",
    "    for store in (list(data.node_stores) + list(data.edge_stores)):\n",
    "        for k, v in list(store.items()):\n",
    "            if isinstance(v, torch.Tensor) and v.dtype.is_floating_point:\n",
    "                torch.nan_to_num_(v, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    return data, id_maps\n",
    "\n",
    "# Build\n",
    "data, id_maps = build_heterodata_compact_with_time_and_labels(builder, 'bill_labels.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_outcomes = pickle.load(open(\"bill_outcomes.pkl\", \"rb\"))\n",
    "bill_id_map = id_maps['bill']\n",
    "\n",
    "\n",
    "outcomes_series = pd.DataFrame.from_dict(bill_id_map, orient='index').reset_index().merge(pd.DataFrame.from_dict(bill_outcomes, orient='index').reset_index(), on='index', how='left')\n",
    "outcomes_series.loc[outcomes_series['0_y'].isna(), '0_y'] = [1, 1, 1, 1, 1]\n",
    "outcomes_fixed = outcomes_series.sort_values('0_x')['0_y'].values\n",
    "# switching failed and vetoed values\n",
    "o = torch.where(torch.from_numpy(outcomes_fixed) == 0, -2, torch.from_numpy(outcomes_fixed))\n",
    "o = torch.where(o == -1, 0, o)\n",
    "o = torch.where(o == -2, -1, o)\n",
    "data['bill'].y = o.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, 'data5.pt', _use_new_zipfile_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('node_id_map.json', 'w') as f:\n",
    "    json.dump(id_maps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node type 'bill': 63100 nodes, feature shape: torch.Size([63100, 771])\n",
      "Node type 'bill_version': 222412 nodes, feature shape: torch.Size([222412, 391])\n",
      "Node type 'legislator': 505 nodes, feature shape: torch.Size([505, 1])\n",
      "Node type 'legislator_term': 1444 nodes, feature shape: torch.Size([1444, 4])\n",
      "Node type 'committee': 1699 nodes, feature shape: torch.Size([1699, 65])\n",
      "Node type 'lobby_firm': 1025 nodes, feature shape: torch.Size([1025, 384])\n",
      "Node type 'donor': 780 nodes, feature shape: torch.Size([780, 64])\n",
      "Edge type '('bill_version', 'is_version', 'bill')': 222412 edges, feature shape: N/A\n",
      "Edge type '('bill_version', 'priorVersion', 'bill_version')': 160768 edges, feature shape: N/A\n",
      "Edge type '('legislator_term', 'samePerson', 'legislator')': 1444 edges, feature shape: N/A\n",
      "Edge type '('legislator_term', 'wrote', 'bill_version')': 610356 edges, feature shape: torch.Size([610356, 1])\n",
      "Edge type '('legislator_term', 'member_of', 'committee')': 17565 edges, feature shape: torch.Size([17565, 1])\n",
      "Edge type '('lobby_firm', 'lobbied', 'legislator_term')': 82617 edges, feature shape: torch.Size([82617, 3])\n",
      "Edge type '('legislator_term', 'has_lobbying', 'lobby_firm')': 82617 edges, feature shape: torch.Size([82617, 2])\n",
      "Edge type '('lobby_firm', 'lobbied', 'committee')': 3982 edges, feature shape: torch.Size([3982, 3])\n",
      "Edge type '('committee', 'has_lobbying', 'lobby_firm')': 3982 edges, feature shape: torch.Size([3982, 2])\n",
      "Edge type '('donor', 'donated_to', 'legislator_term')': 6498 edges, feature shape: torch.Size([6498, 3])\n",
      "Edge type '('legislator_term', 'has_donation', 'donor')': 6498 edges, feature shape: torch.Size([6498, 2])\n",
      "Edge type '('donor', 'expenditure', 'bill')': 549 edges, feature shape: torch.Size([549, 4])\n",
      "Edge type '('legislator_term', 'voted_on', 'bill_version')': 5052552 edges, feature shape: torch.Size([5052552, 19])\n",
      "Edge type '('bill_version', 'read', 'committee')': 51041 edges, feature shape: torch.Size([51041, 1])\n"
     ]
    }
   ],
   "source": [
    "for ntype in data.node_types:\n",
    "    print(f\"Node type '{ntype}': {data[ntype].num_nodes} nodes, feature shape: {data[ntype].x.shape if 'x' in data[ntype] else 'N/A'}\")\n",
    "for etype in data.edge_types:\n",
    "    print(f\"Edge type '{etype}': {data[etype].edge_index.shape[1]} edges, feature shape: {data[etype].edge_attr.shape if 'edge_attr' in data[etype] else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## various repairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open('node_id_map.json', 'r') as f:\n",
    "    id_maps = json.load(f)\n",
    "\n",
    "data = torch.load('data5.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_remove_nodes(data, node_type, merge_map):\n",
    "    node = data[node_type]\n",
    "    N = int(node.num_nodes)\n",
    "\n",
    "    drops = torch.tensor(sorted(merge_map.keys()), dtype=torch.long)\n",
    "    if drops.numel() == 0:\n",
    "        return data\n",
    "\n",
    "    redirect = torch.arange(N, dtype=torch.long)\n",
    "    for drop, keep in merge_map.items():\n",
    "        redirect[drop] = keep\n",
    "\n",
    "    for _ in range(3):\n",
    "        redirect = redirect[redirect]\n",
    "\n",
    "    for (src, rel, dst) in list(data.edge_types):\n",
    "        store = data[(src, rel, dst)]\n",
    "        ei = store.edge_index\n",
    "\n",
    "        changed = False\n",
    "        if src == node_type:\n",
    "            ei0 = redirect[ei[0]]\n",
    "            ei = torch.stack([ei0, ei[1]], dim=0)\n",
    "            changed = True\n",
    "\n",
    "        if dst == node_type:\n",
    "            ei1 = redirect[ei[1]]\n",
    "            ei = torch.stack([ei[0], ei1], dim=0)\n",
    "            changed = True\n",
    "\n",
    "        if changed:\n",
    "            store.edge_index = ei\n",
    "\n",
    "    keep_mask = torch.ones(N, dtype=torch.bool)\n",
    "    keep_mask[drops] = False\n",
    "\n",
    "    reindex = torch.full((N,), -1, dtype=torch.long)\n",
    "    reindex[keep_mask] = torch.arange(int(keep_mask.sum()), dtype=torch.long)\n",
    "\n",
    "    for (src, rel, dst) in list(data.edge_types):\n",
    "        store = data[(src, rel, dst)]\n",
    "        ei = store.edge_index\n",
    "\n",
    "        if src == node_type:\n",
    "            new0 = reindex[ei[0]]\n",
    "            if (new0 < 0).any():\n",
    "                m = new0 >= 0\n",
    "                ei = ei[:, m]\n",
    "                new0 = new0[m]\n",
    "                if \"edge_attr\" in store and store.edge_attr is not None:\n",
    "                    store.edge_attr = store.edge_attr[m]\n",
    "            ei[0] = new0\n",
    "\n",
    "        if dst == node_type:\n",
    "            new1 = reindex[ei[1]]\n",
    "            if (new1 < 0).any():\n",
    "                m = new1 >= 0\n",
    "                ei = ei[:, m]\n",
    "                new1 = new1[m]\n",
    "                if \"edge_attr\" in store and store.edge_attr is not None:\n",
    "                    store.edge_attr = store.edge_attr[m]\n",
    "            ei[1] = new1\n",
    "\n",
    "        store.edge_index = ei\n",
    "\n",
    "    for key, val in list(node.items()):\n",
    "        if torch.is_tensor(val) and val.size(0) == N:\n",
    "            node[key] = val[keep_mask]\n",
    "\n",
    "    node.num_nodes = int(keep_mask.sum())\n",
    "\n",
    "    return data, redirect, reindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merge_map = {\n",
    "    591: 175,\n",
    "    1399: 1039,\n",
    "    1139: 1039,\n",
    "    1301: 1176,\n",
    "    1397: 1135\n",
    "}\n",
    "\n",
    "data, redirect, reindex = merge_and_remove_nodes(data, \"legislator_term\", merge_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_i = id_maps['bill']\n",
    "bvi = id_maps['bill_version']\n",
    "leg_i = id_maps['legislator']\n",
    "lti = id_maps['legislator_term']\n",
    "com_i = id_maps['committee']\n",
    "lob_i = id_maps['lobby_firm']\n",
    "don_i = id_maps['donor']\n",
    "\n",
    "repaired = {}\n",
    "\n",
    "def check_order(dict):\n",
    "    df = pd.DataFrame.from_dict(dict, orient='index').reset_index()\n",
    "    df.columns = ['id', 'loc']\n",
    "    df['index'] = df.index\n",
    "    fixed = {int(row['index']): str(row['id']) for _, row in df.iterrows()}\n",
    "    return fixed\n",
    "\n",
    "for j, k in zip(id_maps.keys(), [bill_i, bvi, leg_i, lti, com_i, lob_i, don_i]):\n",
    "    repaired[j] = check_order(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('id_maps2.json', 'w') as f:\n",
    "    json.dump(repaired, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('subject_key.json', 'r') as f:\n",
    "    subject_key = json.load(f)\n",
    "rev_subject_key = {v: k for k, v in subject_key.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bill_labels.json', 'r') as f:\n",
    "    bill_labels = json.load(f)\n",
    "\n",
    "bill_id_map = id_maps['bill']\n",
    "bill_clusters = {}\n",
    "\n",
    "for bill_id, idx in bill_id_map.items():\n",
    "    if bill_id not in bill_labels:\n",
    "        bill_clusters[idx] = -1\n",
    "    else:\n",
    "        bill_clusters[idx] = int(rev_subject_key.get(bill_labels[bill_id]))\n",
    "bc = torch.tensor([bill_clusters[i] for i in range(len(bill_id_map))], dtype=torch.long)\n",
    "data['bill'].cluster = bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, 'data5.pt', _use_new_zipfile_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_outcomes = pickle.load(open(\"bill_outcomes.pkl\", \"rb\"))\n",
    "\n",
    "# data = torch.load('data5.pt', weights_only=False)\n",
    "\n",
    "\n",
    "outcomes_fixed = pd.DataFrame.from_dict(id_maps['bill'], orient='index').reset_index().merge(pd.DataFrame.from_dict(bill_outcomes, orient='index').reset_index(), on='index').sort_values('0_x')['0_y'].values\n",
    "data['bill'].y = torch.from_numpy(outcomes_fixed).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
