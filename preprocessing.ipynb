{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_votes = pd.read_csv('ca_leg/legislation_data/bill_summary_vote_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_history = pd.read_csv('ca_leg/legislation_data/bill_history_tbl.csv', dtype={'action_status': str, 'primary_location': str, 'secondary_location': str, 'end_status': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('ca_leg/legislation_data/authors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv('ca_leg/legislation_data/history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions = pd.read_csv('ca_leg/legislation_data/bill_versions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes = pd.read_csv('ca_leg/legislation_data/bill_detail_vote_tbl.csv', parse_dates=['session_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_summary = pd.read_csv('ca_leg/legislation_data/bill_summary_vote_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_motions = pd.read_csv('ca_leg/legislation_data/bill_motion_tbl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv('ca_leg/legislation_data/committee_codes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians = pd.read_csv('ca_leg/legislation_data/politicians.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobbying = pd.read_csv('calaccess/lobbying_clean2.csv', dtype={'PAYEE_NAMS': str, 'BAKREF_TID': str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "expend_assembly = pd.read_csv('calaccess/expend_assembly_matched.csv', dtype={'TargetPropositionName': str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "expend_senate = pd.read_csv('calaccess/expend_senate_matched.csv', dtype={'TargetPropositionName': str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "digests = pd.read_csv('ca_leg/legislation_data/digest.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hearings = pd.read_csv('ca_leg/legislation_data/committee_hearing_tbl.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes['vote_date_time'] = pd.to_datetime(bill_votes['vote_date_time']).apply(lambda x: x.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes['legislator_name'] = bill_votes['legislator_name'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_KEYWORDS = [\n",
    "    \"Assembly Third Reading\", \"Assembly 3rd reading\", 'senate 3rd reading', \"Senate Third Reading\",\"Concurrence - Urgency Added\", \"Concurrence in Senate Amendments\", \"Do pass as amended, and re-refer\", \"Do pass as amended, but re-refer\", \"Do pass as amended\", \"Do pass and be re-referred\",\n",
    "    \"Concurrence\", \"Consent Calendar\", \"Urgency Clause\", \"Special Consent\",\n",
    "    \"Motion to Reconsider\", \"Do pass\", \"Reconsideration\", \"Committee amendments\",\n",
    "    \"W/O REF. TO FILE\", \"Be re-referred to the Committee\",\n",
    "    \"Lay on the Table\", \"Amend by\", \"Unfinished Business\", \"Placed on Appropriations Suspense File\",\n",
    "]\n",
    "\n",
    "def extract_action(motion_text):\n",
    "    if not isinstance(motion_text, str) or motion_text is None:\n",
    "        return None\n",
    "    motion = motion_text.upper()\n",
    "\n",
    "    action = next((act for act in ACTION_KEYWORDS if act.upper() in motion), None)\n",
    "    if action != 'Reconsideration' and 'RECONSIDER' in motion:\n",
    "        if action is not None:\n",
    "            action += ' Reconsideration'\n",
    "        else:\n",
    "            action = 'Reconsideration'\n",
    "\n",
    "\n",
    "    return action if action else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_motions['simplified_motion'] = bill_motions['motion_text'].apply(extract_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_coms = {}\n",
    "for i, row in locations.iterrows():\n",
    "    if row['committee_code'].startswith('CZ'):\n",
    "        continue\n",
    "    name = row['committee_name']\n",
    "    if row['committee_code'].startswith('CS'):\n",
    "        if name.startswith('Sen.'):\n",
    "            cname = re.sub(r'Sen. ', 'senate ', name).lower()\n",
    "        elif name.startswith('Senate '):\n",
    "            cname = name.lower()\n",
    "        else:\n",
    "            cname = 'senate ' + name.lower()\n",
    "    elif row['committee_code'].startswith('CX'):\n",
    "        if name.lower().startswith('assembly'):\n",
    "            cname = name.lower()\n",
    "        else:\n",
    "            cname = 'assembly ' + name.lower()\n",
    "    if re.search(r'x\\d$', cname) is not None:\n",
    "        cname = re.sub(r'x(?=\\d$)', 'no. ', cname)\n",
    "    clean_coms[row['committee_code']] = cname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "leg_committees = [f\"{row['chamber']} {row['committee_clean']}\".lower() for _, row in politicians[['committee_clean', 'chamber']].drop_duplicates().iterrows()]\n",
    "\n",
    "def match_committees(_names, clean_coms, threshold=92):\n",
    "    clean_c = list(clean_coms.values())\n",
    "    clean_codes = list(clean_coms.keys())\n",
    "    name_mapping = {}\n",
    "    for i, clean in enumerate(clean_c):\n",
    "        code = clean_codes[i]\n",
    "        matches = []\n",
    "        matches.append(process.extractOne(\n",
    "            clean,\n",
    "            _names,\n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            score_cutoff=threshold\n",
    "        ))\n",
    "        matches.append(process.extractOne(\n",
    "            clean,\n",
    "            _names,\n",
    "            scorer=fuzz.partial_ratio,\n",
    "            score_cutoff=threshold\n",
    "        ))\n",
    "        valid_matches = [m for m in matches if m is not None]\n",
    "        if len(valid_matches) > 0:\n",
    "            best_match = max(valid_matches, key=lambda x: x[1])\n",
    "            name_mapping[code] = best_match[0]\n",
    "        else:\n",
    "            fall_back = process.extractOne(\n",
    "                clean,\n",
    "                _names,\n",
    "                scorer=fuzz.token_sort_ratio,\n",
    "                score_cutoff=threshold - 8\n",
    "            )\n",
    "            if fall_back is not None:\n",
    "                name_mapping[code] = fall_back[0]\n",
    "            else:\n",
    "                name_mapping[code] = None\n",
    "    return name_mapping\n",
    "\n",
    "committee_matches = match_committees(leg_committees, clean_coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations['committee_clean'] = locations['committee_code'].map(committee_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.loc[locations['committee_name'] == 'EDUCATION X5', 'committee_clean'] = 'Budget and Fiscal Review: Education'\n",
    "locations.loc[locations['committee_code'] == 'CX12', 'committee_clean'] = 'Budget No. 1 on Health and Human Services'\n",
    "locations.loc[locations['committee_code'] == 'CS68', 'committee_clean'] = 'Budget No. 3 - Health and Human Services'\n",
    "locations.loc[locations['committee_code'] == 'CS66', 'committee_clean'] = 'Senate Veterans Affairs'\n",
    "locations.loc[locations['committee_code'] == 'CS56', 'committee_clean'] = 'Senate Public Employment and Retirement'\n",
    "locations.loc[locations['committee_code'] == 'CS62', 'committee_clean'] = 'Senate Budget and Fiscal Review'\n",
    "locations.loc[locations['committee_code'] == 'CX23', 'committee_clean'] = 'Assembly Utilities and Commerce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_codes = {\n",
    "    row['motion_id']: row['simplified_motion']\n",
    "    for _, row in bill_motions.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_votes['motion_text'] = summary_votes['motion_id'].map(motion_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_bill_id(id):\n",
    "    front, end = id[:4], id[4:]\n",
    "    if re.search(r'\\d{4}$', front):\n",
    "        return f\"{front}{int(front) + 1}{end}\"\n",
    "    else:\n",
    "        return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions['ID'] = versions['bill_id'].apply(lambda x: repair_bill_id(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_vers = versions.loc[versions['bill_id'].str.startswith('2')]\n",
    "for i, row in bill_vers.iterrows():\n",
    "    tail = f\"{row['VersionNum']}{row['MeasureState']}\"\n",
    "    repaired = repair_bill_id(re.sub(tail, '', row['bill_id']))\n",
    "    end = int(repaired[-4:])\n",
    "\n",
    "    bill_vers.loc[i, 'bill_ID'] = f\"{repaired[:-4]}{end}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislators = {i: pol for i, pol in enumerate(politicians['full_name'].unique().tolist())}\n",
    "\n",
    "leg_parties = {row['full_name']: row['Party'] for _, row in politicians[['full_name', 'Party']].drop_duplicates().iterrows()}\n",
    "leg_occupations = {row['full_name']: row['Occupation'] for _, row in politicians[['full_name', 'Occupation']].drop_duplicates().iterrows()}\n",
    "committees = {i: com for i, com in enumerate(politicians['committee_clean'].unique().tolist())}\n",
    "lobby_firms = {i: firm for i, firm in enumerate(lobbying['FIRM_NAME'].unique().tolist())}\n",
    "\n",
    "donor_names = list(set(expend_assembly['ExpenderName'].unique().tolist() + expend_senate['ExpenderName'].unique().tolist()))\n",
    "donors = {i: donor for i, donor in enumerate(donor_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dict(d, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, d in zip(\n",
    "    ['legislators', 'committees', 'lobby_firms', 'donors'],\n",
    "    [legislators, committees, lobby_firms, donors]\n",
    "):\n",
    "    save_dict(d, f'{name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_titles = {row['bill_ID']: row['Title'] for _, row in bill_vers[['bill_ID', 'Title']].drop_duplicates().iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_subjects = {row['bill_ID']: row['GeneralSubject'] for _, row in bill_vers.loc[bill_vers['GeneralSubject'].apply(lambda x: x is not None and isinstance(x, str)), ['bill_ID', 'GeneralSubject']].drop_duplicates().iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_ids = list(set(bill_votes.loc[bill_votes['bill_id'].str.startswith('2'), 'bill_id'].unique().tolist() + summary_votes.loc[summary_votes['bill_id'].str.startswith('2'), 'bill_id'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_id_codes = {row['bill_id']: row['bill_ID'] for _, row in bill_vers.drop_duplicates(subset=['bill_id', 'bill_ID']).iterrows()}\n",
    "history['bill_ID'] = history['bill_id'].map(bill_id_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['Date'] = pd.to_datetime(history['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_dates = {}\n",
    "for v, group in history.loc[history['bill_ID'].isin(bill_ids)].groupby('bill_ID'):\n",
    "    introduction_dates[v] = {'Dates': group['Date'].unique().tolist(), 'Actions': group.sort_values('Date', ascending=True).drop_duplicates(subset=['Action', 'Date'])['Action'].tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_id_mapping = {i: list(group.values) for i, group in bill_vers.groupby('bill_ID')['ID']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict(version_id_mapping, 'version_id_mapping.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_id_mapping2 = {i: list(group.values) for i, group in bill_vers.groupby('bill_ID')['bill_id']}\n",
    "bv2b = {v: k for k, val in version_id_mapping2.items() for v in val}\n",
    "history['bill_ID'] = history['bill_id'].map(bv2b)\n",
    "save_dict(bv2b, 'bill_id_mapping.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ranges = {}\n",
    "\n",
    "for k, v in introduction_dates.items():\n",
    "    first, last = min(v['Dates']), max(v['Dates'])\n",
    "    date_ranges[k] = {'First_action': first, 'Last_action': last}\n",
    "\n",
    "save_dict(date_ranges, 'bill_dates_map.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = history.loc[history['bill_ID'].notna()].sort_values('Date', ascending=False).groupby('bill_ID').first().reset_index()[['bill_ID', 'Action']]\n",
    "outcomes.loc[outcomes['Action'].isin(['CHAPTERED', 'ENROLLED', 'FILED', 'APPROVED']), 'Outcome'] = 1\n",
    "outcomes.loc[outcomes['Action'] == 'VETOED', 'Outcome'] = -1\n",
    "outcomes.loc[outcomes['Outcome'].isna(), 'Outcome'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = outcomes.set_index('bill_ID')['Outcome'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict(outcome, 'bill_outcomes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_bill_ids = {}\n",
    "for i in summary_votes.loc[summary_votes['bill_id'].isin(bill_ids)].groupby(['year', 'motion_id'])['bill_id'].value_counts().index:\n",
    "    year, motion_id, bill_id = i\n",
    "    if (year, motion_id) not in vote_bill_ids.keys():\n",
    "        vote_bill_ids[(year, motion_id)] = [bill_id]\n",
    "    else:\n",
    "        vote_bill_ids[(year, motion_id)].append(bill_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_vers_dig = bill_vers.merge(digests, on='bill_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislators_last_names = {}\n",
    "for _, row in politicians[['chamber', 'Last', 'Term', 'full_name']].drop_duplicates().iterrows():\n",
    "    legislators_last_names[(row['chamber'], row['Last'].lower(), row['Term'])] = row['full_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {row['ID']: {\n",
    "    'digest': row['DigestText'],\n",
    "    'MeasureState': row['MeasureState'],\n",
    "    'VoteRequired': row['VoteRequired'] if row['VoteRequired'] is not None else 'No',\n",
    "    'VersionNum': row['VersionNum'] if row['VersionNum'] is not None else 'No',\n",
    "    'LocalProgram': row['LocalProgram'] if row['LocalProgram'] is not None else 'No',\n",
    "    'FiscalCommittee': row['FiscalCommittee'] if row['FiscalCommittee'] is not None else 'No',\n",
    "    'TaxLevy': row['TaxLevy'] if row['TaxLevy'] is not None else 'No',\n",
    "    'Urgency': row['Urgency'] if row['Urgency'] is not None else 'No'} for _, row in bill_vers_dig.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislator_codes = {v: k for k, v in legislators.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_codes = {v.lower(): k for k, v in committees.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes['chamber'] = bill_votes['location_code'].apply(lambda x: 'assembly' if x == 'AFLOOR' or x.startswith('CX') else 'senate' if x == 'SFLOOR' or x.startswith('CS') else 'full')\n",
    "bill_votes['vote_date_time'] = pd.to_datetime(bill_votes['vote_date_time'])\n",
    "bill_votes['term'] = bill_votes['vote_date_time'].apply(lambda x: f\"{x.year}-{x.year + 1}\" if x.year % 2 == 1  else f\"{x.year - 1}-{x.year}\" if x.year % 2 == 0 and x < pd.Timestamp(year=x.year, month=11, day=2) else f\"{x.year + 1}-{x.year + 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_locations = authors.loc[(authors['House'] == 'UNKNOWN') & (authors['bill_id'].map(bill_id_codes).isin(bill_ids)), ['bill_id', 'Name']].drop_duplicates()\n",
    "for i, row in author_locations.iterrows():\n",
    "    if 'AB' in row['bill_id']:\n",
    "        author_locations.loc[i, 'name'] = 'Assembly ' + row['Name']\n",
    "    elif 'SB' in row['bill_id']:\n",
    "        author_locations.loc[i, 'name'] = 'Senate ' + row['Name']\n",
    "    else:\n",
    "        author_locations.loc[i, 'name'] = 'Joint ' + row['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fuzzy_strings(source_list, target_list):\n",
    "    def preprocess_name(name):\n",
    "        if not isinstance(name, str):\n",
    "            return \"\"\n",
    "        name = name.lower()\n",
    "        name = re.sub(r'\\(.*?\\)', '', name)\n",
    "        name = re.sub(r'committee on', '', name)\n",
    "        name = re.sub(r'[^a-z\\s]', ' ', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        return name\n",
    "\n",
    "    clean_source = [preprocess_name(c) for c in source_list]\n",
    "    clean_target = [preprocess_name(c) for c in target_list]\n",
    "\n",
    "    keywords = [\"education\", \"health\", \"finance\", \"budget\", \"transportation\",\n",
    "                \"judiciary\", \"environment\", \"agriculture\", \"energy\", \"labor\",\n",
    "                \"housing\", \"veterans affairs\", \"public safety\", \"insurance\", \"banking\", \"public health\", \"small business\", \"redistricting\",\n",
    "                \"public utilities\", \"natural resources\", \"water\",\n",
    "                \"technology\", \"communications\", \"elections\", \"government\",\n",
    "                \"appropriations\", \"rules\", \"ethics\", 'criminal justice', \"environmental protection\", \"college and university\", \"human services\", \"reproductive health\", \"mental health\", \"technology\", \"aggriculture\", \"urban development\", \"renewable energy\", \"gun violence\", \"commerce\", \"privacy\", \"cybersecurity\", \"infrastructure\", \"disaster preparedness\", \"prisons\", \"aging\"]\n",
    "\n",
    "    def get_committee_keywords(name):\n",
    "        return set(kw for kw in keywords if kw in name)\n",
    "\n",
    "    target_keywords = [get_committee_keywords(name) for name in clean_target]\n",
    "\n",
    "    def calculate_similarity(source_idx, target_idx):\n",
    "        source = clean_source[source_idx]\n",
    "        target = clean_target[target_idx]\n",
    "\n",
    "        if not source or not target:\n",
    "            return 0\n",
    "\n",
    "        if source == target:\n",
    "            return 100\n",
    "\n",
    "        token_sort = fuzz.token_sort_ratio(source, target)\n",
    "        token_set = fuzz.token_set_ratio(source, target)\n",
    "        partial = fuzz.partial_ratio(source, target)\n",
    "\n",
    "        source_kw = get_committee_keywords(source)\n",
    "        keyword_overlap = len(source_kw.intersection(target_keywords[target_idx]))\n",
    "        keyword_bonus = min(20, keyword_overlap * 10)\n",
    "        weighted_score = (token_sort * 0.3) + (token_set * 0.5) + (partial * 0.2) + keyword_bonus\n",
    "\n",
    "        return weighted_score\n",
    "\n",
    "    matches = {}\n",
    "    for i, source in enumerate(source_list):\n",
    "        scores = [calculate_similarity(i, j) for j in range(len(target_list))]\n",
    "\n",
    "        if not scores or max(scores) < 60:\n",
    "            matches[source] = None\n",
    "        else:\n",
    "            best_idx = np.argmax(scores)\n",
    "            confidence = scores[best_idx]\n",
    "\n",
    "            if confidence >= 60:\n",
    "                matches[source] = target_list[best_idx]\n",
    "            else:\n",
    "                matches[source] = None\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_com_matches = fuzzy_strings(author_locations['name'].unique().tolist(), leg_committees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_locations['name'] = author_locations['name'].map(author_com_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sponsors = authors.loc[authors['bill_id'].map(bill_id_codes).isin(bill_ids)]\n",
    "sponsors['term'] = sponsors['bill_id'].apply(lambda x: f\"{x[:4]}-{int(x[:4]) + 1}\" if int(x[:4]) % 2 == 1 else f\"{int(x[:4]) - 1}-{x[:4]}\" if int(x[:4]) % 2 == 0 and int(x[:4]) < 2009 else f\"{x[:4]}-{int(x[:4]) + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob = lobbying.loc[lobbying['clean_beneficiary'].notna(), ['FIRM_NAME', 'EXPN_DSCR', 'clean_beneficiary', 'EXPN_DATE', 'BENE_AMT']]\n",
    "lob['EXPN_DATE'] = pd.to_datetime(lob['EXPN_DATE'])\n",
    "lob['term'] = lob['EXPN_DATE'].apply(lambda x: f\"{x.year}-{x.year + 1}\" if x.year % 2 == 1  else f\"{x.year - 1}-{x.year}\" if x.year % 2 == 0 and x < pd.Timestamp(year=x.year, month=11, day=2) else f\"{x.year}-{x.year + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in politicians.loc[politicians['full_name'].apply(lambda x: isinstance(x, float)), ['Term', 'Last', 'chamber']].drop_duplicates().iterrows():\n",
    "    term, last = row['Term'], row['Last']\n",
    "    a = politicians.loc[(politicians['Last'] == last) & (politicians['Term'] == term) & (politicians['full_name'].apply(lambda x: isinstance(x, str)))]\n",
    "    if len(a) > 0:\n",
    "        politicians.loc[(politicians['Term'] == term) & (politicians['Last'] == last) & (politicians['chamber'] == row['chamber']), 'full_name'] = a['full_name'].values[0]\n",
    "        continue\n",
    "    else:\n",
    "        a = politicians.loc[(politicians['Last'] == last) & (politicians['full_name'].apply(lambda x: isinstance(x, str)))]\n",
    "    if len(a) > 0:\n",
    "        politicians.loc[(politicians['Term'] == term) & (politicians['Last'] == last) & (politicians['chamber'] == row['chamber']), 'full_name'] = a['full_name'].values[0]\n",
    "    else:\n",
    "        print(last, term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_names_terms = {}\n",
    "for _, row in politicians[['full_name', 'Term', 'chamber']].drop_duplicates().iterrows():\n",
    "    if ',' in row['full_name']:\n",
    "        name = row['full_name'].split(',')[1].strip() + ' ' + row['full_name'].split(',')[0].strip()\n",
    "    else:\n",
    "        name = row['full_name']\n",
    "    pol_names_terms[(row['full_name'].lower(), row['Term'])] = {'chamber': row['chamber'], 'name': name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "expend_assembly = expend_assembly.rename(columns={'term': 'Term'})\n",
    "expend_assembly['chamber'] = 'assembly'\n",
    "expend_senate = expend_senate.rename(columns={'term': 'Term'})\n",
    "expend_senate['chamber'] = 'senate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_contributions = pd.concat([expend_assembly.loc[expend_assembly['matched_target_name'].notna(), ['ExpenderName', 'Amount', 'matched_target_name', 'Term', 'chamber', 'DateEnd']].drop_duplicates(subset=['ExpenderName', 'Amount', 'matched_target_name', 'DateEnd']), expend_senate.loc[expend_senate['matched_target_name'].notna(), ['ExpenderName', 'Amount', 'matched_target_name', 'Term', 'chamber', 'DateEnd']].drop_duplicates(subset=['ExpenderName', 'Amount', 'matched_target_name', 'DateEnd'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_contributions['DateEnd'] = pd.to_datetime(campaign_contributions['DateEnd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sponsors['bill_ID'] = sponsors['bill_id'].apply(repair_bill_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = history.merge(bill_votes, left_on=['bill_ID', 'Date'], right_on=['bill_id', 'vote_date_time'], how='inner').rename(columns={'bill_id_x': 'bill_version'}).drop('bill_id_y', axis=1)\n",
    "voting['bv_id'] = voting['bill_version'].apply(repair_bill_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(title):\n",
    "    if not isinstance(title, str):\n",
    "        return ''\n",
    "    title = re.sub(r'\\(.*?\\)', '', title)\n",
    "    title = re.sub(r'[^a-zA-Z0-9\\s]', ' ', title)\n",
    "    title = re.sub(r'\\s+', ' ', title).strip()\n",
    "    return title.lower()\n",
    "\n",
    "def batched_embeddings(values, output_dims=384):\n",
    "    vals = [text_clean(v) for v in values if isinstance(v, str)]\n",
    "    vals = list(set([v for v in vals if v != '']))\n",
    "    embeddings = model.encode(vals, batch_size=64, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True, num_workers=4, output_dims=output_dims)\n",
    "    embs = {v: e for v, e in zip(vals, embeddings)}\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = list(set([t for t in versions.loc[versions['bill_id'].str.startswith('2')]['GeneralSubject'].tolist() if (isinstance(t, str) and t is not None)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_original = {text_clean(t): t for t in subjects}\n",
    "save_dict(subjects_original, 'subjects_original.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf54a5f6d794b8195c517e669927748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/913 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subject_embeddings = batched_embeddings(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(subject_embeddings, 'subject_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_embeddings = torch.load('subject_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_embeddings = {}\n",
    "for occ in list(set(list(leg_occupations.values()))):\n",
    "    if isinstance(occ, str) and len(occ) > 0 and text_clean(occ) != '':\n",
    "        occ_embeddings[occ] = model.encode(\n",
    "            text_clean(occ),\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True,\n",
    "            output_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c53bbd8d0c8495f83aa211f8bdb43d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titles = [t for t in bill_vers['Title'].unique().tolist() if (isinstance(t, str) and t not in [None, '', np.nan])]\n",
    "title_embeddings = batched_embeddings(titles, output_dims=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9dfa020f454d2ab6ccf6604a444393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lobbying_firms_embeddings = batched_embeddings([firm for firm in lobbying.loc[(lobbying['clean_beneficiary'].notna()) & (lobbying['FIRM_NAME'])]['FIRM_NAME'].unique().tolist() if isinstance(firm, str)], output_dims=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_embeddings = {}\n",
    "for committee in politicians['committee_clean'].unique().tolist():\n",
    "    co = re.sub(r'assembly|senate|committee|subcommittee', '', committee.lower())\n",
    "    committee_embeddings[committee.lower()] = model.encode(co,  convert_to_tensor=True, normalize_embeddings=True, truncate_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [00:38<00:00, 13.31it/s]\n"
     ]
    }
   ],
   "source": [
    "donor_embeddings = {}\n",
    "\n",
    "for donor in tqdm(donor_names):\n",
    "    donor_embeddings[donor] = model.encode(donor, convert_to_tensor=True, normalize_embeddings=True, truncate_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 27.17it/s]\n"
     ]
    }
   ],
   "source": [
    "motion_embeddings = {}\n",
    "for motion in tqdm([t for t in pd.DataFrame.from_dict(motion_codes, orient='index').reset_index().rename({'index': 'motion_id', 0: 'motion_text'}, axis=1)['motion_text'].drop_duplicates().tolist() if t is not None]):\n",
    "    motion_embeddings[motion] = model.encode(motion, convert_to_tensor=True, normalize_embeddings=True, truncate_dim=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182472/182472 [02:39<00:00, 1146.63it/s]\n"
     ]
    }
   ],
   "source": [
    "digest_embeddings = torch.load('digests.pt')\n",
    "for digest in tqdm(list(set([t if (isinstance(t, str) and t is not None) else '' for t in digests.loc[digests['bill_id'].str.startswith('2')]['DigestText'].tolist()]))):\n",
    "    if digest not in digest_embeddings.keys():\n",
    "        digest_embeddings[digest] = model.encode(digest, convert_to_tensor=True, normalize_embeddings=True, truncate_dim=64)\n",
    "torch.save(digest_embeddings, 'digests.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "hear = hearings[['bill_id', 'location_code']].merge(locations[['committee_code', 'committee_clean']], left_on='location_code', right_on='committee_code', how='left')[['bill_id', 'committee_clean']].drop_duplicates()\n",
    "hear['year'] = hear['bill_id'].apply(lambda x: int(x[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = {p: 'Democratic Alternate' if re.search(r'Democratic\\s*Alternate', p) is not None else 'Vice Chair' if re.search(r'V\\s*i\\s*c\\s*e\\s*-*\\s*C\\s*h\\s*a\\s*i\\s*r\\s*', p) is not None else 'Co-Chair' if re.search(r'Co\\s*-\\s*Chair', p) is not None else 'Chair' if re.search(r'Cha\\s*i\\s*r', p) is not None else 'Republican Alternate' if re.search(r'\\s*Republican\\s*Alternate', p) is not None else p for p in politicians['position'].unique()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOP Graph-Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, id, type, features=None):\n",
    "        self.id = id\n",
    "        self.type = type\n",
    "        self.features = features or {}\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self, source, target, relation, attributes=None):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.relation = relation\n",
    "        self.attributes = attributes or {}\n",
    "\n",
    "class Bill(Node):\n",
    "    def __init__(self, bill_id, title, subject, measure_type):\n",
    "        features = {\n",
    "            'title': title,\n",
    "            'subject': subject,\n",
    "            'measure_type': measure_type,\n",
    "            'date': None,\n",
    "        }\n",
    "        super().__init__(bill_id, \"bill\", features)\n",
    "        self.actions = None\n",
    "        self.order_df = None\n",
    "        self.outcome = None\n",
    "\n",
    "    def add_actions(self, actions):\n",
    "        if self.actions is None:\n",
    "            self.actions = actions\n",
    "        else:\n",
    "            self.actions = pd.concat([self.actions, actions], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    def add_date(self, date):\n",
    "        self.features['date'] = date\n",
    "\n",
    "    def add_order_df(self, order_df):\n",
    "        if self.order_df is None:\n",
    "            self.order_df = order_df\n",
    "        else:\n",
    "            self.order_df = pd.concat([self.order_df, order_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    def add_outcome(self, outcome):\n",
    "        self.outcome = outcome\n",
    "\n",
    "    def align_actions_versions(self, bill, versions_, dates):\n",
    "        dates = pd.Series(dates).sort_values(ascending=True).drop_duplicates().tolist()\n",
    "        actions = [i for i in introduction_dates.get(bill, {}).get('Actions', []) if i != 'FILED']\n",
    "        if len(actions) > len(dates):\n",
    "            if actions[-2:] == ['ENROLLED', 'CHAPTERED'] or actions[-2:] == ['APPROVED', 'CHAPTERED'] or len(actions) <= 4 and actions[-1] == 'ENROLLED' or abs(len(dates) - len(actions)) >= 2 and actions[-1] == 'ENROLLED' or actions == ['INTRODUCED', 'ENROLLED', 'AMENDED_SENATE'] or actions[-1] == 'APPROVED' or len(actions) == 3 and all(a.startswith('PASSED_') for a in actions[-2:]) or actions == ['ENROLLED', 'INTRODUCED'] or actions == ['INTRODUCED', 'ENROLLED'] or actions == ['INTRODUCED', 'REVISED'] or len(actions) > 3 and actions[-3:] == ['ENROLLED', 'CORRECTED', 'CHAPTERED'] or len(actions) > 3 and actions[-3:] == ['PASSED_SENATE', 'PASSED_ASSEMBLY', 'AMENDED_SENATE'] or list(set(['INTRODUCED', 'PASSED_SENATE', 'PASSED_ASSEMBLY', 'AMENDED_SENATE'])) == list(set(actions)) or list(set(['INTRODUCED', 'PASSED_SENATE', 'PASSED_ASSEMBLY', 'AMENDED_ASSEMBLY'])) == list(set(actions)) or actions[-2] == 'ENROLLED' and actions[-1].startswith('PASSED_') or len(actions) > 5 and actions[-4] == 'CHAPTERED' and actions[-1].startswith('PASSED_') or actions[-2:] == ['PASSED_SENATE', 'PASSED_ASSEMBLY'] or actions == ['INTRODUCED', 'AMENDED_SENATE', 'PASSED_SENATE', 'PASSED_ASSEMBLY', 'AMENDED_ASSEMBLY'] or len(actions) == 9 and actions[-6:] == ['APPROVED', 'CHAPTERED', 'ENROLLED', 'AMENDED_SENATE', 'PASSED_ASSEMBLY', 'PASSED_SENATE'] or len(actions) > 5 and actions[-4:] == ['ENROLLED', 'PASSED_SENATE', 'APPROVED', 'PASSED_ASSEMBLY'] or dates == [pd.Timestamp('2008-12-08 00:00:00'), pd.Timestamp('2008-12-18 00:00:00')]:\n",
    "                dates.append(dates[-1])\n",
    "                if len(actions) > len(dates):\n",
    "                    dates.append(dates[-1])\n",
    "                    if len(actions) > len(dates):\n",
    "                        dates.append(dates[-1])\n",
    "                        if len(actions) > len(dates):\n",
    "                            dates.append(dates[-1])\n",
    "                            if len(actions) > len(dates):\n",
    "                                dates.append(dates[-1])\n",
    "            if len(dates) == 1 and len(actions) > 1:\n",
    "                for _ in range(len(actions) - len(dates)):\n",
    "                    dates.append(dates[0])\n",
    "            if actions[-2:] == ['INTRODUCED', 'PASSED_ASSEMBLY']:\n",
    "                dates = [dates[0]] + dates\n",
    "            if len(actions) >= 6 and actions[:3] == ['INTRODUCED', 'AMENDED_ASSEMBLY', 'ENROLLED'] and actions[-3:] == ['CHAPTERED', 'APPROVED', 'CORRECTED']:\n",
    "                dates = dates[:2] + [dates[2]] + dates[2:-2] + [dates[-2]] + dates[-2:]\n",
    "            elif ('PASSED_ASSEMBLY' in actions and 'AMENDED_ASSEMBLY' in actions) or ('PASSED_SENATE' in actions and 'AMENDED_SENATE' in actions):\n",
    "                if len(dates) == 3:\n",
    "                    dates = dates[:1] + [dates[1]] + dates[1:]\n",
    "                    if all(a for a in ['PASSED_ASSEMBLY', 'AMENDED_ASSEMBLY', 'PASSED_SENATE', 'AMENDED_SENATE'] if a in actions):\n",
    "                        dates = dates[:1] + [dates[1]] + dates[1:]\n",
    "                elif 'PROPOSED_CONFERENCE_REPORT_1' in actions and len(actions) - len(dates) == 2:\n",
    "                    dates = dates[:2] + [dates[2]] + [dates[2]] + dates[2:]\n",
    "                elif len(dates) > 3 and len(actions) - len(dates) > 0 and not (actions[-4:] == ['PASSED_ASSEMBLY', 'ENROLLED', 'PASSED_SENATE', 'CHAPTERED'] and 'AMENDED_SENATE' in actions):\n",
    "                    dates = dates[:2] + [dates[2]] + dates[2:]\n",
    "                if len(actions) > 4 and actions[-3:] == ['ENROLLED', 'PASSED_SENATE', 'CHAPTERED']:\n",
    "                    dates = dates[:-2] + [dates[-2]] + dates[-2:]\n",
    "\n",
    "            if len(actions) - len(dates) == 1:\n",
    "                if 'CORRECTED' in actions:\n",
    "                    actions.remove('CORRECTED')\n",
    "                elif 'RESCIND' in actions:\n",
    "                    actions.remove('RESCIND')\n",
    "\n",
    "            if actions[-1] == 'CORRECTED' and len(actions) - len(dates) == 2:\n",
    "                if len(dates) >= 5:\n",
    "                    dates = dates[:3] + [dates[3]] + [dates[3]] + [dates[4]] + dates[4:]\n",
    "                elif len(dates) == 2:\n",
    "                    dates = [dates[0]] + [dates[0]] + dates[0:]\n",
    "                else:\n",
    "                    dates = dates[:2] + [dates[2]] + [dates[2]] + dates[2:]\n",
    "            if actions[-1] == 'CHAPTERED' and len(actions) - len(dates) == 3:\n",
    "                dates = dates + [dates[-1]] + [dates[-1]] + [dates[-1]]\n",
    "            if actions[-2:] == ['ENROLLED', 'VETOED'] and len(actions) - len(dates) > 0 :\n",
    "                dates = dates[:-4] + [dates[-4]]  + [dates[-3]] + dates[-3:]\n",
    "\n",
    "            if len(dates) < len(actions) and'ENROLLED' in actions and actions.index('ENROLLED') < len(actions) - 1:\n",
    "                for i in range(len(actions) - actions.index('ENROLLED')):\n",
    "                    dates = dates + [dates[-1]]\n",
    "        if len(actions) + 1 == len(dates):\n",
    "            dates = dates[:-1]\n",
    "        try:\n",
    "            action_df = pd.DataFrame({'date': dates, 'action': actions})\n",
    "        except:\n",
    "            return None, None, None\n",
    "        action_df['date'] = pd.to_datetime(action_df['date'], errors='coerce')\n",
    "        order_df = action_df.loc[~action_df['action'].isin(['FILED', 'PASSED_ASSEMBLY', 'PASSED_SENATE', 'APPROVED'])]\n",
    "        repair_flag = False\n",
    "        if order_df.shape[0] > len(versions_):\n",
    "            version_ends = [re.search(r'INT|AMD|ENR|CHP|PRO', v).group() for v in versions_]\n",
    "            if 'ENR' in version_ends:\n",
    "                v_enr = version_ends.index('ENR')\n",
    "                extension = [versions_[v_enr - 1] if v_enr - 1 != 0 else versions_[v_enr] for _ in range(len(order_df) - len(versions_))]\n",
    "                versions_ = versions_[:v_enr] + extension + versions_[v_enr:]\n",
    "            else:\n",
    "                repair_flag = True\n",
    "        vr = pd.DataFrame({'version': versions_})\n",
    "        if vr.shape[0] == 0:\n",
    "            return None, None, None\n",
    "        try:\n",
    "            vr['v_num'] = vr['version'].apply(lambda x: re.search(r'\\d{2}(?=INT|AMD|ENR|CHP|PRO)', x).group()).astype(int)\n",
    "        except:\n",
    "            return None, None, None\n",
    "        vr = vr.sort_values('v_num', ascending=False).reset_index(drop=True)\n",
    "        if repair_flag:\n",
    "            last_v = vr.loc[vr['version'].notna()].iloc[-1]['version']\n",
    "            last_v_num = float(re.search(r'\\d{2}(?=INT|AMD|ENR|CHP|PRO)', last_v).group())\n",
    "            for i in range(len(order_df) - len(versions_)):\n",
    "                vr.loc[len(vr) + i, 'version'] = last_v\n",
    "                vr.loc[len(vr) + i, 'v_num'] = last_v_num\n",
    "\n",
    "        order_df['version'] = vr['version']\n",
    "        order_df['order'] = range(1, len(order_df) + 1)\n",
    "        outcomes = order_df['action'].tolist()\n",
    "        if 'CHAPTERED' in outcomes or 'FILED' in outcomes:\n",
    "            if 'VETOED' in outcomes:\n",
    "                outcome = 'VETOED'\n",
    "            else:\n",
    "                outcome = 'CHAPTERED'\n",
    "        else:\n",
    "            outcome = 'FAILED'\n",
    "        return action_df, order_df, outcome\n",
    "\n",
    "class BillVersion(Node):\n",
    "    def __init__(self, bill_id, version_id, digest, vote_required, local_program, fiscal_com, tax_levy, urgency):\n",
    "        features = {\n",
    "            'digest': digest,\n",
    "            'VoteRequired': vote_required,\n",
    "            'LocalProgram': local_program,\n",
    "            'FiscalCommittee': fiscal_com,\n",
    "            'TaxLevy': tax_levy,\n",
    "            'Urgency': urgency,\n",
    "            'date': None\n",
    "        }\n",
    "        super().__init__(version_id, \"bill_version\", features)\n",
    "        self.bill_id = bill_id\n",
    "        self.actions = {}\n",
    "\n",
    "    def add_actions(self, location, date):\n",
    "        if location not in self.actions:\n",
    "            self.actions[location] = []\n",
    "        self.actions[location].append(date)\n",
    "\n",
    "    def add_date(self, date):\n",
    "        self.features['date'] = date\n",
    "\n",
    "class Legislator(Node):\n",
    "    def __init__(self, legislator_id, party, occupation):\n",
    "        features = {\n",
    "            'party': party,\n",
    "            'occupation': occupation\n",
    "        }\n",
    "        super().__init__(legislator_id, \"legislator\", features)\n",
    "        self.terms = []\n",
    "\n",
    "class LegislatorTerm(Node):\n",
    "    def __init__(self, term, legislator_id, chamber, district):\n",
    "        features = {\n",
    "            'chamber': chamber,\n",
    "            'district': district,\n",
    "            'term': term\n",
    "        }\n",
    "        node_id = f\"{legislator_id}_{term}_{chamber}\"\n",
    "        super().__init__(node_id, \"legislator_term\", features)\n",
    "        self.committees = []\n",
    "        self.committee_positions = []\n",
    "\n",
    "    def add_committee(self, committee_id):\n",
    "        self.committees.append(committee_id)\n",
    "\n",
    "    def add_committee_position(self, committee_id, position):\n",
    "        self.committee_positions.append((committee_id, position))\n",
    "\n",
    "class Committee(Node):\n",
    "    def __init__(self, committee_id, name, chamber, term):\n",
    "        features = {\n",
    "            'name': name,\n",
    "            'chamber': chamber\n",
    "        }\n",
    "        term_ = term.split('-')[0]\n",
    "        id = f\"{committee_id}_{term_}\"\n",
    "        super().__init__(id, \"committee\", features)\n",
    "        self.members = []\n",
    "\n",
    "    def add_member(self, legislator_id):\n",
    "        self.members.append(legislator_id)\n",
    "\n",
    "class LobbyFirm(Node):\n",
    "    def __init__(self, firm_id, name):\n",
    "        features = {\n",
    "            'name': name\n",
    "        }\n",
    "        super().__init__(firm_id, \"lobby_firm\", features)\n",
    "        self.total_donations = 0\n",
    "\n",
    "    def add_donation(self, amount):\n",
    "        self.total_donations += amount\n",
    "\n",
    "class Donor(Node):\n",
    "    def __init__(self, donor_id, name):\n",
    "        features = {\n",
    "            'name': name\n",
    "        }\n",
    "        super().__init__(donor_id, \"donor\", features)\n",
    "        self.total_donations = 0\n",
    "\n",
    "    def add_donation(self, amount):\n",
    "        self.total_donations += amount\n",
    "\n",
    "class Vote(Edge):\n",
    "    def __init__(self, legislator, bill_version, vote, motion, date, direction):\n",
    "        attributes = {\n",
    "            'vote': 1 if vote == 'AYE' else -1 if vote == 'NOE' else 0,\n",
    "            'motion': motion,\n",
    "            'date': date\n",
    "        }\n",
    "        if direction == 1:\n",
    "            super().__init__(legislator, bill_version, 'voted_on', attributes)\n",
    "        else:\n",
    "            super().__init__(bill_version, legislator, 'vote_from', attributes)\n",
    "\n",
    "class CommitteeMembership(Edge):\n",
    "    def __init__(self, legislator, committee, position, direction):\n",
    "        attributes = {\n",
    "            'position': position\n",
    "        }\n",
    "        if direction == 1:\n",
    "            super().__init__(legislator, committee, 'member_of', attributes)\n",
    "        else:\n",
    "            super().__init__(committee, legislator, 'has_member', attributes)\n",
    "        committee.add_member(legislator)\n",
    "\n",
    "class Sponsorship(Edge):\n",
    "    def __init__(self, legislator, bill_version, author_type, direction):\n",
    "        attributes = {\n",
    "            'author_type': author_type\n",
    "        }\n",
    "        if direction == 1:\n",
    "            super().__init__(legislator, bill_version, 'wrote', attributes)\n",
    "        else:\n",
    "            super().__init__(bill_version, legislator, 'written_by', attributes)\n",
    "\n",
    "class Reading(Edge):\n",
    "    def __init__(self, bill, committee, direction):\n",
    "        if direction == 1:\n",
    "            super().__init__(committee, bill, 'read')\n",
    "        else:\n",
    "            super().__init__(bill, committee, 'read_by')\n",
    "\n",
    "class Donation(Edge):\n",
    "    def __init__(self, donor, recipient, amount, date, type, direction=1):\n",
    "        attributes = {\n",
    "            'amount': amount,\n",
    "            'date': date\n",
    "        }\n",
    "        if type == 'CampaignContribution':\n",
    "            if direction == 1:\n",
    "                super().__init__(donor, recipient, 'donated_to', attributes)\n",
    "            else:\n",
    "                super().__init__(recipient, donor, 'has_donation', attributes)\n",
    "        else:\n",
    "            if direction == 1:\n",
    "                super().__init__(donor, recipient, 'lobbied', attributes)\n",
    "            else:\n",
    "                super().__init__(recipient, donor, 'has_lobbying', attributes)\n",
    "\n",
    "class Version(Edge):\n",
    "    def __init__(self, bill_version, bill, direction):\n",
    "        if direction == 1:\n",
    "            super().__init__(bill_version, bill, 'is_version')\n",
    "        else:\n",
    "            super().__init__(bill, bill_version, 'has_version')\n",
    "\n",
    "class siblingVersion(Edge):\n",
    "    def __init__(self, version1, version2, direction):\n",
    "        if direction == 1:\n",
    "            super().__init__(version1, version2, 'priorVersion')\n",
    "        else:\n",
    "            super().__init__(version2, version1, 'nextVersion')\n",
    "\n",
    "class samePerson(Edge):\n",
    "    def __init__(self, node1, node2):\n",
    "        super().__init__(node1, node2, 'samePerson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "vnums = bill_vers.set_index('ID')['VersionNum'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_map = {v: k for k, val in version_id_mapping.items() for v in val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class GraphBuilder:\n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "        self.edges = []\n",
    "        self.versions = []\n",
    "        self.nodes_by_type = defaultdict(list)\n",
    "        self.edges_by_type = defaultdict(list)\n",
    "        self._type_counters = defaultdict(int)\n",
    "\n",
    "    def add_version(self, version):\n",
    "        self.versions.append(version)\n",
    "\n",
    "    def add_node(self, node):\n",
    "        key = (node.type, node.id)\n",
    "        if key not in self.nodes:\n",
    "            self.nodes[key] = node\n",
    "        idx = self._type_counters[node.type]\n",
    "        self._type_counters[node.type] += 1\n",
    "        self.nodes_by_type[node.type].append(node)\n",
    "\n",
    "    def get_node(self, type_, id_):\n",
    "        return self.nodes.get((type_, id_))\n",
    "\n",
    "    def add_edge(self, edge):\n",
    "        self.edges.append(edge)\n",
    "        etype = (edge.source.type, edge.relation, edge.target.type)\n",
    "        self.edges_by_type[etype].append(edge)\n",
    "\n",
    "    def build(self):\n",
    "        return {\n",
    "            \"nodes\": list(self.nodes.values()),\n",
    "            \"edges\": self.edges,\n",
    "            \"nodes_by_type\": self.nodes_by_type,\n",
    "            \"edges_by_type\": self.edges_by_type\n",
    "        }\n",
    "\n",
    "    def add_bills(self, bill_ids, titles, subjects, titles_embs, subjects_embs, features):\n",
    "        def process_single_bill(bill):\n",
    "            try:\n",
    "                title = text_clean(titles.get(bill, ''))\n",
    "                subject = text_clean(subjects.get(bill, ''))\n",
    "                title_emb = titles_embs.get(title, None)\n",
    "                subject_emb = subjects_embs.get(subject, None)\n",
    "                measure_type = re.search(r'[A-Za-z]+', bill).group()\n",
    "                bill_node = Bill(bill, title_emb, subject_emb, measure_type)\n",
    "\n",
    "                versions = version_id_mapping.get(bill, [])\n",
    "                versions_ = []\n",
    "                dates = introduction_dates.get(bill, {}).get('Dates', [])\n",
    "                try:\n",
    "                    fd = sorted(list(set(dates)))[0]\n",
    "                except IndexError:\n",
    "                    y = int(bill[:4])\n",
    "                    fd = pd.Timestamp(year=y, month=2, day=1)\n",
    "                bill_node.add_date(fd)\n",
    "\n",
    "                if not versions:\n",
    "                    return\n",
    "\n",
    "                self.add_node(bill_node)\n",
    "\n",
    "                for version in versions:\n",
    "                    digest = features[version]['digest']\n",
    "                    if str(digest) == 'nan' or version.endswith('VETO'):\n",
    "                        continue\n",
    "\n",
    "                    digest_emb = digest_embeddings.get(digest, None)\n",
    "                    if digest_emb is None:\n",
    "                        continue\n",
    "\n",
    "                    version_node = BillVersion(\n",
    "                        bill, version, digest_emb,\n",
    "                        features[version]['VoteRequired'],\n",
    "                        features[version]['LocalProgram'],\n",
    "                        features[version]['FiscalCommittee'],\n",
    "                        features[version]['TaxLevy'],\n",
    "                        features[version]['Urgency']\n",
    "                    )\n",
    "\n",
    "                    self.add_node(version_node)\n",
    "                    if version not in self.versions:\n",
    "                        self.versions.append(version)\n",
    "                    versions_.append(version)\n",
    "\n",
    "\n",
    "                orders = [vnums.get(v) for v in versions]\n",
    "                sorted_versions = [s for _, s in sorted(zip(orders, versions))]\n",
    "                for i, s in enumerate(sorted_versions):\n",
    "                    v = self.get_node('bill_version', s)\n",
    "                    if v is None or bill_node is None:\n",
    "                        continue\n",
    "                    self.add_edge(Version(v, bill_node, 1))\n",
    "                    if i > 0:\n",
    "                        prev_v = self.get_node('bill_version', sorted_versions[i - 1])\n",
    "                        if prev_v is not None:\n",
    "                            self.add_edge(siblingVersion(prev_v, v, 1))\n",
    "                o = outcome.get(bill, 0)\n",
    "                bill_node.add_outcome(o)\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error processing bill {bill}: {e}\")\n",
    "\n",
    "        for bill in tqdm(bill_ids):\n",
    "            process_single_bill(bill)\n",
    "\n",
    "    def add_legislators(self, legislators_):\n",
    "        for legislator in tqdm(legislators_):\n",
    "            leg_name = legislators[legislator]\n",
    "            party = leg_parties.get(leg_name)\n",
    "            occupation = leg_occupations.get(leg_name)\n",
    "            occ_embedding = occ_embeddings.get(occupation, None)\n",
    "            legislator_node = Legislator(legislator, party, occ_embedding)\n",
    "            self.add_node(legislator_node)\n",
    "            terms = politicians.loc[politicians['full_name'] == leg_name, ['Term', 'District No.', 'chamber']].drop_duplicates()\n",
    "            for _, term in terms.iterrows():\n",
    "                term_node = LegislatorTerm(term['Term'], legislator, term['chamber'], term['District No.'])\n",
    "                self.add_node(term_node)\n",
    "                self.add_edge(samePerson(legislator_node, term_node))\n",
    "\n",
    "    def add_committees(self, committees_df):\n",
    "        for _, row in tqdm(committees_df[['committee_clean', 'Term', 'chamber']].drop_duplicates().iterrows(), total=committees_df[['committee_clean', 'Term', 'chamber']].drop_duplicates().shape[0]):\n",
    "            committee_name = committee_embeddings.get(row['committee_clean'].lower(), None)\n",
    "            committee_id = committee_codes.get(row['committee_clean'].lower(), None)\n",
    "            committee_node = Committee(committee_id, committee_name, row['chamber'], row['Term'])\n",
    "            self.add_node(committee_node)\n",
    "            term = row['Term']\n",
    "            members = politicians.loc[(politicians['committee_clean'] == row['committee_clean']) & (politicians['Term'] == row['Term']), ['position', 'full_name', 'chamber']].drop_duplicates()\n",
    "            for _, member in members.iterrows():\n",
    "                leg_id = legislator_codes[member['full_name']]\n",
    "                leg_node_id = f\"{leg_id}_{term}_{member['chamber']}\"\n",
    "                leg_node = self.get_node('legislator_term', leg_node_id)\n",
    "                self.add_edge(CommitteeMembership(leg_node, committee_node, member['position'], 1))\n",
    "                committee_node.add_member(leg_node_id)\n",
    "                leg_node.add_committee(committee_name)\n",
    "\n",
    "    def add_votes(self):\n",
    "        for _, row in tqdm(voting.loc[voting['bill_ID'].isin(bill_ids)].iterrows(), total=voting.loc[voting['bill_ID'].isin(bill_ids)].shape[0]):\n",
    "            bv_id = row['bv_id']\n",
    "            v_node = self.get_node('bill_version', bv_id)\n",
    "            if v_node is None:\n",
    "                continue\n",
    "            last = row['legislator_name'].strip().lower()\n",
    "            legislator = legislators_last_names.get((row['chamber'], last, row['term']), None)\n",
    "            if legislator is None:\n",
    "                if len(last.split(' ')) > 1:\n",
    "                    legislator = row['legislator_name']\n",
    "                else:\n",
    "                    continue\n",
    "            legislator_id = legislator_codes.get(legislator, None)\n",
    "            leg_term_node = self.get_node('legislator_term', f\"{legislator_id}_{row['term']}_{row['chamber']}\")\n",
    "            if leg_term_node is None:\n",
    "                continue\n",
    "            vote = row['vote_code']\n",
    "            motion_id = row['motion_id']\n",
    "            motion_text = motion_codes.get(motion_id, None)\n",
    "            if motion_text is None:\n",
    "                continue\n",
    "            if row['location_code'] not in ['AFLOOR', 'SFLOOR']:\n",
    "                actions = v_node.actions\n",
    "                if row['location_code'] in actions:\n",
    "                    if row['vote_date_time'] not in actions[row['location_code']]:\n",
    "                        actions[row['location_code']].append(row['vote_date_time'])\n",
    "            if motion_text is None:\n",
    "                motion_embedding = ''\n",
    "            else:\n",
    "                motion_embedding = motion_embeddings.get(motion_text, None)\n",
    "            self.add_edge(Vote(leg_term_node, v_node, vote, motion_embedding, row['vote_date_time'], 1))\n",
    "\n",
    "\n",
    "    def add_readings(self):\n",
    "        for _, row in tqdm(hear.loc[hear['bill_id'].isin(bill_ids)].iterrows(), total=hear.loc[hear['bill_id'].isin(bill_ids)].shape[0]):\n",
    "            b_node = self.get_node('bill', row['bill_id'])\n",
    "            if b_node is None:\n",
    "                continue\n",
    "            location = row['committee_clean']\n",
    "            if location is None or location == '':\n",
    "                continue\n",
    "            term = row['year']\n",
    "            committee_id = committee_codes.get(str(location).lower(), None)\n",
    "            if committee_id is None:\n",
    "                continue\n",
    "            committee_node = self.get_node('committee', f\"{committee_id}_{term}\")\n",
    "            if committee_node is None:\n",
    "                continue\n",
    "            self.add_edge(Reading(committee_node, b_node, 1))\n",
    "\n",
    "\n",
    "    def add_sponsorships(self, sponsors):\n",
    "        for _, row in tqdm(sponsors.iterrows(), total=sponsors.shape[0]):\n",
    "            version = row['bill_ID']\n",
    "            version_node = self.get_node('bill_version', version)\n",
    "            if version_node is None:\n",
    "                continue\n",
    "            if row['House'] == 'UNKNOWN':\n",
    "                com = author_com_matches.get(row['Name'], None)\n",
    "                year = row['term'].split('-')[0]\n",
    "                com_node = self.get_node('committee', f\"{com}_{year}\")\n",
    "                if com_node is None:\n",
    "                   continue\n",
    "                self.add_edge(Sponsorship(com_node, version_node, row['Contribution'], 1))\n",
    "            else:\n",
    "                if row['Name'] in ['Mark Stone', 'Cristina Garcia', 'John Campbell', 'Bill Campbell', 'Eduardo Garcia']:\n",
    "                    leg_id = legislator_codes.get(row['Name'])\n",
    "                    leg_node = self.get_node('legislator_term', f\"{leg_id}_{row['term']}_{row['House'].lower()}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        name = re.sub(r'\\'', '', re.sub(r'-', ' ', row['Name'])).lower().strip()\n",
    "                    except:\n",
    "                        continue\n",
    "                    leg_name = legislators_last_names.get((row['House'].lower(), name, row['term']), None)\n",
    "                    if leg_name is None:\n",
    "                        continue\n",
    "                    leg_id = legislator_codes.get(leg_name)\n",
    "                    leg_node = self.get_node('legislator_term', f\"{leg_id}_{row['term']}_{row['House'].lower()}\")\n",
    "                if leg_node is None:\n",
    "                        continue\n",
    "                self.add_edge(Sponsorship(leg_node, version_node, row['Contribution'], 1))\n",
    "\n",
    "\n",
    "    def add_lobbyists(self, lobbyists):\n",
    "        for key in tqdm(lobbyists.keys(), total=len(lobbyists.keys())):\n",
    "            lobbyist = LobbyFirm(text_clean(key), lobbyists[key])\n",
    "            self.add_node(lobbyist)\n",
    "\n",
    "    def add_donations(self, donations):\n",
    "        for _, row in tqdm(donations.iterrows(), total=donations.shape[0]):\n",
    "            firm = row['FIRM_NAME']\n",
    "            firm_node = self.get_node('lobby_firm', text_clean(firm))\n",
    "            if firm_node is None:\n",
    "                continue\n",
    "            if row['clean_beneficiary'] in committee_codes:\n",
    "                com = committee_codes.get(row['clean_beneficiary'])\n",
    "                year = row['term'].split('-')[0]\n",
    "                com_node = self.get_node('committee', f\"{com}_{year}\")\n",
    "                if com_node is None:\n",
    "                    continue\n",
    "                self.add_edge(Donation(firm_node, com_node, row['BENE_AMT'], row['EXPN_DATE'], 'Lobbying', 1))\n",
    "                firm_node.add_donation(row['BENE_AMT'])\n",
    "\n",
    "            else:\n",
    "                dicti = pol_names_terms.get((row['clean_beneficiary'], row['term']), None)\n",
    "                chamber = dicti['chamber'] if dicti is not None else None\n",
    "                name = dicti['name'] if dicti is not None else None\n",
    "                if chamber is None or name is None:\n",
    "                    continue\n",
    "                leg_id = legislator_codes.get(name)\n",
    "                leg_node = self.get_node('legislator_term', f\"{leg_id}_{row['term']}_{chamber}\")\n",
    "                if leg_node is None:\n",
    "                    continue\n",
    "                self.add_edge(Donation(firm_node, leg_node, row['BENE_AMT'], row['EXPN_DATE'], 'Lobbying', 1))\n",
    "                firm_node.add_donation(row['BENE_AMT'])\n",
    "\n",
    "    def add_donors(self, donors):\n",
    "\n",
    "        for donor in tqdm(donors.keys(), total=len(donors.keys())):\n",
    "            donor_embedding = donors[donor]\n",
    "            donor = Donor(donor, donor_embedding)\n",
    "            self.add_node(donor)\n",
    "\n",
    "    def add_contributions(self, contributions):\n",
    "        for _, row in tqdm(contributions.iterrows(), total=contributions.shape[0]):\n",
    "            expender = row['ExpenderName']\n",
    "            expender_node = self.get_node('donor', expender)\n",
    "            if expender_node is None:\n",
    "                continue\n",
    "            recipient = row['matched_target_name']\n",
    "            recipient_id = legislator_codes.get(recipient)\n",
    "            recipient_node = self.get_node('legislator_term', f\"{recipient_id}_{row['Term']}_{row['chamber']}\")\n",
    "            if recipient_node is None:\n",
    "                continue\n",
    "            self.add_edge(Donation(expender_node, recipient_node, row['Amount'], row['DateEnd'], 'CampaignContribution', 1))\n",
    "            expender_node.add_donation(row['Amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47669/47669 [06:29<00:00, 122.51it/s]\n",
      "100%|██████████| 506/506 [00:01<00:00, 439.40it/s]\n",
      "100%|██████████| 702148/702148 [00:28<00:00, 24435.52it/s]\n"
     ]
    }
   ],
   "source": [
    "builder = GraphBuilder()\n",
    "builder.add_bills(bill_ids, bill_titles, bill_subjects, title_embeddings, subject_embeddings, features)\n",
    "builder.add_legislators(legislators)\n",
    "builder.add_sponsorships(sponsors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1699/1699 [00:05<00:00, 310.66it/s]\n",
      "100%|██████████| 1165/1165 [00:00<00:00, 214464.72it/s]\n",
      "100%|██████████| 122317/122317 [00:03<00:00, 30812.34it/s]\n",
      "100%|██████████| 506/506 [00:00<00:00, 257375.43it/s]\n",
      "100%|██████████| 9151/9151 [00:00<00:00, 24186.48it/s]\n"
     ]
    }
   ],
   "source": [
    "builder.add_committees(politicians)\n",
    "builder.add_lobbyists(lobbying_firms_embeddings)\n",
    "builder.add_donations(lob)\n",
    "builder.add_donors(donor_embeddings)\n",
    "builder.add_contributions(campaign_contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5965170/5965170 [04:47<00:00, 20737.70it/s]\n",
      "100%|██████████| 120596/120596 [00:03<00:00, 38543.59it/s]\n"
     ]
    }
   ],
   "source": [
    "builder.add_votes()\n",
    "builder.add_readings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = graph_data['nodes']\n",
    "edges = graph_data['edges']\n",
    "nodes_by_type = graph_data['nodes_by_type']\n",
    "edges_by_type = graph_data['edges_by_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "rev_subject_embeddings = {v: k for k, v in subject_embeddings.items()}\n",
    "bill_subjects = {}\n",
    "\n",
    "for node in nodes_by_type['bill']:\n",
    "    if node.features['subject'] in rev_subject_embeddings:\n",
    "        subj = rev_subject_embeddings[node.features['subject']]\n",
    "        bill_subjects[node.id] = subj\n",
    "\n",
    "with open('bill_subjects.json', 'w') as f:\n",
    "    json.dump(bill_subjects, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.data import HeteroData\n",
    "import numpy as np\n",
    "\n",
    "author_type_map = {\n",
    "    'LEAD_AUTHOR': 'LEAD_AUTHOR',\n",
    "    'PRINCIPAL_COAUTHOR': 'PRINCIPAL_COAUTHOR',\n",
    "    'COAUTHOR': 'COAUTHOR',\n",
    "    'data=\"COAUTHOR\"': 'COAUTHOR',\n",
    "    'data=\"LEAD_AUTHOR\"': 'LEAD_AUTHOR',\n",
    "    'data=\"PRINCIPAL_COAUTHOR\"': 'PRINCIPAL_COAUTHOR',\n",
    "    'nan': 'AUTHOR'\n",
    "}\n",
    "author_levels = {\n",
    "    'AUTHOR': 1,\n",
    "    'COAUTHOR': 1,\n",
    "    'PRINCIPAL_COAUTHOR': 2,\n",
    "    'LEAD_AUTHOR': 3\n",
    "}\n",
    "\n",
    "measure_types = bill_vers['MeasureType'].unique()\n",
    "parties = politicians['Party'].unique()\n",
    "chambers = politicians['chamber'].unique()\n",
    "outcome_mapping = {'CHAPTERED': 1, 'VETOED': 0, 'FAILED': -1, 'ENROLLED': 1}\n",
    "measure_encoder = LabelEncoder()\n",
    "measure_encoder.fit(measure_types)\n",
    "party_encoder = LabelEncoder()\n",
    "party_encoder.fit(parties)\n",
    "chamber_encoder = LabelEncoder()\n",
    "chamber_encoder.fit(chambers)\n",
    "pos = list(positions.values()) + ['member']\n",
    "pos_encoder = LabelEncoder()\n",
    "pos_encoder.fit(pos)\n",
    "\n",
    "feature_encoders = {\n",
    "    'measure_type': measure_encoder,\n",
    "    'party': party_encoder,\n",
    "    'chamber': chamber_encoder,\n",
    "    'position': pos_encoder\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = [g for g in globals() if g not in ['builder', 'graph_data', 'nodes', 'edges', 'nodes_by_type', 'edges_by_type', 'rev_subject_embeddings', 'bill_subjects', 'author_type_map', 'author_levels', 'measure_types', 'parties', 'chambers', 'outcome_mapping', 'measure_encoder', 'party_encoder', 'chamber_encoder', 'pos_encoder', 'feature_encoders', 'positions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in old:\n",
    "    if o in globals():\n",
    "        del globals()[o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.data import HeteroData\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "EMBED_DIM = 384\n",
    "TEXT_KEYS = {'title','subject','digest','occupation','name'}\n",
    "YESNO_KEYS = {'VoteRequired','LocalProgram','FiscalCommittee','TaxLevy','Urgency'}\n",
    "\n",
    "def _to_np1d(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 0:\n",
    "        x = np.array([x], dtype=np.float32)\n",
    "    else:\n",
    "        x = x.reshape(-1).astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def _pad_trunc(a, L):\n",
    "    a = _to_np1d(a)\n",
    "    if a.size == L:\n",
    "        return a\n",
    "    if a.size > L:\n",
    "        return a[:L]\n",
    "    out = np.zeros(L, dtype=np.float32)\n",
    "    out[:a.size] = a\n",
    "    return out\n",
    "\n",
    "def _encode_scalar(x, default=0.0):\n",
    "    if x is None or x == '' or (isinstance(x, float) and np.isnan(x)):\n",
    "        return np.array([default], dtype=np.float32)\n",
    "    try:\n",
    "        return np.array([float(x)], dtype=np.float32)\n",
    "    except:\n",
    "        return np.array([default], dtype=np.float32)\n",
    "\n",
    "def _encode_date(x):\n",
    "    if x is None or x is pd.NaT:\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "    try:\n",
    "        # handles datetime or pandas Timestamp\n",
    "        return np.array([pd.Timestamp(x).timestamp()], dtype=np.float32)\n",
    "    except:\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "\n",
    "def _encode_term(x):\n",
    "    if x is None or x == '':\n",
    "        return np.array([2000.0], dtype=np.float32)\n",
    "    try:\n",
    "        return np.array([float(str(x).split('-')[0])], dtype=np.float32)\n",
    "    except:\n",
    "        return np.array([2000.0], dtype=np.float32)\n",
    "\n",
    "def _encode_district(x):\n",
    "    if x is None:\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "    t = re.sub(r'[^\\d]', '', str(x))\n",
    "    return np.array([float(t) if t != '' else 0.0], dtype=np.float32)\n",
    "\n",
    "def _encode_yesno(x):\n",
    "    if x is None:\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "    return np.array([1.0 if str(x).lower() == 'yes' else 0.0], dtype=np.float32)\n",
    "\n",
    "def _encode_author_type(x, author_levels=None, author_type_map=None):\n",
    "    if author_levels is None or author_type_map is None:\n",
    "        return np.array([1.0], dtype=np.float32)\n",
    "    key = str(x)\n",
    "    if key in author_levels:\n",
    "        return np.array([float(author_levels[key])], dtype=np.float32)\n",
    "    if key in author_type_map and author_type_map[key] in author_levels:\n",
    "        return np.array([float(author_levels[author_type_map[key]])], dtype=np.float32)\n",
    "    return np.array([1.0], dtype=np.float32)\n",
    "\n",
    "def _encode_position(x, positions=None, pos_encoder=None):\n",
    "    if pos_encoder is None:\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "    base = 'member'\n",
    "    if positions is not None:\n",
    "        base = positions.get(x, 'member')\n",
    "    try:\n",
    "        arr = pos_encoder.transform([base])\n",
    "        return _to_np1d(arr)\n",
    "    except:\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "\n",
    "def _encode_categorical(x, enc=None):\n",
    "    if enc is None:\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "    try:\n",
    "        arr = enc.transform([x]) if x is not None else enc.transform(['__UNK__'])\n",
    "        return _to_np1d(arr)\n",
    "    except:\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "\n",
    "def _encode_text_embed(x, dim=EMBED_DIM):\n",
    "    if x is None or x == '':\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "    arr = _to_np1d(x)\n",
    "    return _pad_trunc(arr, dim)\n",
    "\n",
    "def build_feature_schema(nlist, feature_encoders=None, positions=None, pos_encoder=None, author_levels=None, author_type_map=None):\n",
    "    feature_encoders = feature_encoders or {}\n",
    "    keys_union = set(TEXT_KEYS)  # ensure these always exist\n",
    "    array_maxdims = {}\n",
    "\n",
    "    # discover keys and max lengths for array-like fields\n",
    "    for node in nlist:\n",
    "        attrs = getattr(node, 'features', {})\n",
    "        keys_union |= set(attrs.keys())\n",
    "        for k, v in attrs.items():\n",
    "            if isinstance(v, (np.ndarray, torch.Tensor)):\n",
    "                L = int(np.prod(v.shape))\n",
    "                array_maxdims[k] = max(array_maxdims.get(k, 0), L)\n",
    "\n",
    "    # fixed-1D keys\n",
    "    fixed1d = {'date','author_type','term','district'} | YESNO_KEYS\n",
    "    # order: deterministic for reproducibility\n",
    "    order = sorted(keys_union)\n",
    "\n",
    "    # build schema: list of (key, dim, encoder_callable)\n",
    "    schema = []\n",
    "    for k in order:\n",
    "        if k in TEXT_KEYS:\n",
    "            dim = EMBED_DIM\n",
    "            enc = (lambda val, dim=dim: _encode_text_embed(val, dim))\n",
    "        elif k in YESNO_KEYS:\n",
    "            dim = 1\n",
    "            enc = _encode_yesno\n",
    "        elif k == 'date':\n",
    "            dim = 1\n",
    "            enc = _encode_date\n",
    "        elif k == 'term':\n",
    "            dim = 1\n",
    "            enc = _encode_term\n",
    "        elif k == 'district':\n",
    "            dim = 1\n",
    "            enc = _encode_district\n",
    "        elif k == 'author_type':\n",
    "            dim = 1\n",
    "            enc = (lambda val, author_levels=author_levels, author_type_map=author_type_map: _encode_author_type(val, author_levels, author_type_map))\n",
    "        elif k == 'position':\n",
    "            dim = 1  # LabelEncoder-like output\n",
    "            enc = (lambda val, positions=positions, pos_encoder=pos_encoder: _encode_position(val, positions, pos_encoder))\n",
    "        elif k in (feature_encoders.keys() if feature_encoders else []):\n",
    "            dim = 1  # LabelEncoder assumed; if you switch to OneHot, update to its n_features\n",
    "            enc = (lambda val, enc=feature_encoders[k]: _encode_categorical(val, enc))\n",
    "        elif k in array_maxdims:\n",
    "            dim = array_maxdims[k]\n",
    "            enc = (lambda val, dim=dim: (_pad_trunc(val, dim) if val is not None else np.zeros(dim, dtype=np.float32)))\n",
    "        else:\n",
    "            dim = 1\n",
    "            enc = _encode_scalar\n",
    "        schema.append((k, dim, enc))\n",
    "\n",
    "    # pack schema for use\n",
    "    spec = {\n",
    "        'order': [k for k, _, _ in schema],\n",
    "        'dims': {k: dim for k, dim, _ in schema},\n",
    "        'encoders': {k: enc for k, _, enc in schema},\n",
    "    }\n",
    "    return spec\n",
    "\n",
    "def encode_node_with_schema(attrs, spec):\n",
    "    vecs = []\n",
    "    for k in spec['order']:\n",
    "        enc = spec['encoders'][k]\n",
    "        dim = spec['dims'][k]\n",
    "        v = attrs.get(k, None)\n",
    "        out = enc(v)\n",
    "        out = out.reshape(-1).astype(np.float32)\n",
    "        if out.size != dim:\n",
    "            out = _pad_trunc(out, dim)\n",
    "        vecs.append(out)\n",
    "    x = np.concatenate(vecs, axis=0).reshape(1, -1).astype(np.float32)\n",
    "    return x\n",
    "\n",
    "# ----- assembly -----\n",
    "\n",
    "data = HeteroData()\n",
    "node_id_map = {}\n",
    "feature_schemas = {}\n",
    "\n",
    "for ntype, nlist in nodes_by_type.items():\n",
    "    spec = build_feature_schema(\n",
    "        nlist,\n",
    "        feature_encoders=feature_encoders if 'feature_encoders' in globals() else None,\n",
    "        positions=positions if 'positions' in globals() else None,\n",
    "        pos_encoder=pos_encoder if 'pos_encoder' in globals() else None,\n",
    "        author_levels=author_levels if 'author_levels' in globals() else None,\n",
    "        author_type_map=author_type_map if 'author_type_map' in globals() else None,\n",
    "    )\n",
    "    feature_schemas[ntype] = spec\n",
    "\n",
    "    X_list = []\n",
    "    node_ids = []\n",
    "    outcomes = []\n",
    "\n",
    "    for node in nlist:\n",
    "        attrs = getattr(node, 'features', {})\n",
    "        x_vec = encode_node_with_schema(attrs, spec)\n",
    "        X_list.append(x_vec)\n",
    "        node_ids.append(node.id)\n",
    "        if ntype == 'bill':\n",
    "            outcomes.append(getattr(node, 'outcome', 0))\n",
    "\n",
    "    X = np.vstack(X_list).astype(np.float32)\n",
    "    data[ntype].x = torch.from_numpy(X)\n",
    "    data[ntype].n_id = np.array(node_ids, dtype=np.str_)\n",
    "    data[ntype].num_nodes = len(nlist)\n",
    "    if ntype == 'bill':\n",
    "        data[ntype].y = torch.from_numpy(np.array(outcomes, dtype=np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_np1d(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 0:\n",
    "        x = np.array([x], dtype=np.float32)\n",
    "    else:\n",
    "        x = x.reshape(-1).astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def _pad_trunc(a, L):\n",
    "    a = _to_np1d(a)\n",
    "    if a.size == L:\n",
    "        return a\n",
    "    if a.size > L:\n",
    "        return a[:L]\n",
    "    out = np.zeros(L, dtype=np.float32)\n",
    "    out[:a.size] = a\n",
    "    return out\n",
    "\n",
    "def _encode_scalar(x, default=0.0):\n",
    "    if x is None or x == '' or (isinstance(x, float) and np.isnan(x)):\n",
    "        return np.array([default], dtype=np.float32)\n",
    "    try:\n",
    "        return np.array([float(x)], dtype=np.float32)\n",
    "    except:\n",
    "        return np.array([default], dtype=np.float32)\n",
    "\n",
    "def _encode_date(x):\n",
    "    if x is None or x is pd.NaT:\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "    try:\n",
    "        return np.array([pd.Timestamp(x).timestamp()], dtype=np.float32)\n",
    "    except:\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "\n",
    "def build_edge_feature_schema(edge_list, feature_encoders=None):\n",
    "    feature_encoders = feature_encoders or {}\n",
    "    keys_union = set()\n",
    "    array_maxdims = {}\n",
    "\n",
    "    for e in edge_list:\n",
    "        attrs = getattr(e, 'attributes', {}) or {}\n",
    "        keys_union |= set(attrs.keys())\n",
    "        for k, v in attrs.items():\n",
    "            if isinstance(v, (np.ndarray, torch.Tensor)):\n",
    "                L = int(np.prod(v.shape))\n",
    "                array_maxdims[k] = max(array_maxdims.get(k, 0), L)\n",
    "\n",
    "    order = sorted(keys_union)\n",
    "    schema = []\n",
    "    for k in order:\n",
    "        if k == 'date':\n",
    "            dim = 1\n",
    "            enc = _encode_date\n",
    "        elif k in (feature_encoders.keys() if feature_encoders else []):\n",
    "            enc_ = feature_encoders[k]\n",
    "            try:\n",
    "                dim = enc_.transform(['__probe__']).shape[-1] if hasattr(enc_, 'transform') and hasattr(enc_.transform(['__probe__']), 'shape') else 1\n",
    "            except:\n",
    "                dim = 1\n",
    "            def enc(val, enc_=enc_):\n",
    "                try:\n",
    "                    arr = enc_.transform([val if val is not None else '__UNK__'])\n",
    "                    return _to_np1d(arr)\n",
    "                except:\n",
    "                    return np.zeros(dim, dtype=np.float32)\n",
    "        elif k in array_maxdims:\n",
    "            dim = array_maxdims[k]\n",
    "            enc = (lambda val, dim=dim: (_pad_trunc(val, dim) if val is not None else np.zeros(dim, dtype=np.float32)))\n",
    "        else:\n",
    "            dim = 1\n",
    "            enc = _encode_scalar\n",
    "        schema.append((k, dim, enc))\n",
    "\n",
    "    spec = {\n",
    "        'order': [k for k, _, _ in schema],\n",
    "        'dims': {k: dim for k, dim, _ in schema},\n",
    "        'encoders': {k: enc for k, _, enc in schema},\n",
    "    }\n",
    "    return spec\n",
    "\n",
    "def encode_edge_with_schema(attrs, spec):\n",
    "    vecs = []\n",
    "    attrs = attrs or {}\n",
    "    for k in spec['order']:\n",
    "        enc = spec['encoders'][k]\n",
    "        dim = spec['dims'][k]\n",
    "        v = attrs.get(k, None)\n",
    "        out = enc(v)\n",
    "        out = out.reshape(-1).astype(np.float32)\n",
    "        if out.size != dim:\n",
    "            out = _pad_trunc(out, dim)\n",
    "        vecs.append(out)\n",
    "    return np.concatenate(vecs, axis=0).reshape(1, -1).astype(np.float32)\n",
    "\n",
    "edge_feature_schemas = {}\n",
    "\n",
    "for etype, elist in edges_by_type.items():\n",
    "    if not elist:\n",
    "        continue\n",
    "\n",
    "    # Determine the hetero key (src, rel, dst)\n",
    "    if isinstance(etype, tuple) and len(etype) == 3:\n",
    "        src_type, rel, dst_type = etype\n",
    "    else:\n",
    "        # fallback: infer from first edge\n",
    "        src_type = elist[0].source.type\n",
    "        dst_type = elist[0].target.type\n",
    "        rel = str(etype)\n",
    "\n",
    "    hetero_key = (src_type, rel, dst_type)\n",
    "\n",
    "    # Build per-etype schema\n",
    "    spec = build_edge_feature_schema(elist, feature_encoders=feature_encoders if 'feature_encoders' in globals() else None)\n",
    "    edge_feature_schemas[hetero_key] = spec\n",
    "\n",
    "    # Collect indices and attrs\n",
    "    idx_list = []\n",
    "    attr_rows = []\n",
    "\n",
    "    for e in elist:\n",
    "        try:\n",
    "            s = node_id_map[src_type][e.source.id]\n",
    "            d = node_id_map[dst_type][e.target.id]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        idx_list.append([s, d])\n",
    "\n",
    "        x = encode_edge_with_schema(getattr(e, 'attributes', {}) or {}, spec)\n",
    "        attr_rows.append(x)\n",
    "\n",
    "    if not idx_list:\n",
    "        continue\n",
    "\n",
    "    edge_idx = torch.tensor(idx_list, dtype=torch.long).t().contiguous()\n",
    "    data[hetero_key].edge_index = edge_idx\n",
    "\n",
    "    if attr_rows:\n",
    "        EA = np.vstack(attr_rows).astype(np.float32)\n",
    "        data[hetero_key].edge_attr = torch.from_numpy(EA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, 'data3.pt', _use_new_zipfile_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('node_id_map.json', 'w') as f:\n",
    "    json.dump(node_id_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
