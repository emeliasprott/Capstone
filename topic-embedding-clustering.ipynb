{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10e4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, torch, json, pickle, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from community import community_louvain\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd3f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bill_subjects.json', 'r') as f:\n",
    "    bill_subjects = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5afe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_originals = pickle.load(open('subjects_original.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8f7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "so = {k: subject_originals[v] for k, v in bill_subjects.items() if v in subject_originals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684cf49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc2de6888654a798dd09f34e8b75be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def canonical(sub):\n",
    "    txt = sub.lower()\n",
    "    txt = re.sub(r'[^a-z\\s]', ' ', txt)\n",
    "    txt = re.sub(r'(?:california|state|bill|law|act|amendment|proposition|measure|initiative|program|act|code|section|chapter|month|awareness|prevention)', '', txt)\n",
    "    txt = re.sub(r'\\s+', ' ', txt).strip()\n",
    "    txt = ' '.join([w for w in txt.split() if w not in stopwords])\n",
    "    return txt.strip()\n",
    "\n",
    "canonical_subjects = {k: canonical(v) for k, v in so.items()}\n",
    "canonical_set = set(canonical_subjects.values())\n",
    "\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m-qat-q4_0-unquantized\")\n",
    "subs = list(canonical_set)\n",
    "embs = model.encode(subs, show_progress_bar=True, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8b0d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig, leidenalg as la\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "X = normalize(embs.astype(np.float32))\n",
    "nbrs = NearestNeighbors(n_neighbors=min(22, max(10, int(np.sqrt(len(X))))), metric=\"cosine\").fit(X)\n",
    "dist, idx = nbrs.kneighbors(X)\n",
    "sim = 1.0 - dist\n",
    "rows = np.repeat(np.arange(len(X)), idx.shape[1])\n",
    "cols = idx.ravel()\n",
    "weights = sim.ravel()\n",
    "m = np.vstack([rows, cols, weights]).T\n",
    "m = m[rows != cols]\n",
    "G = ig.Graph(n=len(X), edges=list(map(tuple, m[:, :2].astype(int))), directed=False)\n",
    "G.es[\"weight\"] = m[:, 2].astype(float)\n",
    "res_grid = np.linspace(0.2, 6.0, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c4f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = []\n",
    "scores = []\n",
    "for r in res_grid:\n",
    "    part = la.find_partition(G, la.RBConfigurationVertexPartition, weights=\"weight\", resolution_parameter=float(r), seed=42)\n",
    "    labels = np.array(part.membership)\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        try:\n",
    "            s = silhouette_score(X, labels, metric=\"cosine\")\n",
    "        except Exception:\n",
    "            s = -1.0\n",
    "    else:\n",
    "        s = -1.0\n",
    "    parts.append(labels)\n",
    "    scores.append(s)\n",
    "best = int(np.argmax(scores))\n",
    "labels = parts[best]\n",
    "k = len(np.unique(labels))\n",
    "centroids = []\n",
    "for c in range(k):\n",
    "    msk = labels==c\n",
    "    v = X[msk].mean(axis=0)\n",
    "    v = v/np.linalg.norm(v) if np.linalg.norm(v)>0 else v\n",
    "    centroids.append(v)\n",
    "centroids = np.stack(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b4db314",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = X @ centroids.T\n",
    "cluster_sim = sims[np.arange(len(X)), labels]\n",
    "thr = np.percentile(cluster_sim, 10)\n",
    "noise = cluster_sim < max(thr, 0.1)\n",
    "labels_noise = labels.copy()\n",
    "labels_noise[noise] = -1\n",
    "clusters = {}\n",
    "for cid in sorted(set(labels_noise) - {-1}):\n",
    "    idx_c = np.where(labels_noise==cid)[0]\n",
    "    cvec = centroids[cid]\n",
    "    ex_i = idx_c[np.argmax(X[idx_c] @ cvec)]\n",
    "    clusters[int(cid)] = {\n",
    "        \"indices\": idx_c.tolist(),\n",
    "        \"subjects\": [subs[i] for i in idx_c],\n",
    "        \"centroid\": cvec.tolist(),\n",
    "        \"exemplar_index\": int(ex_i),\n",
    "        \"exemplar_subject\": subs[ex_i]\n",
    "    }\n",
    "result = {\n",
    "    \"labels\": labels_noise.tolist(),\n",
    "    \"clusters\": clusters,\n",
    "    \"noise_indices\": np.where(labels_noise==-1)[0].tolist(),\n",
    "    \"resolution\": float(res_grid[best]),\n",
    "    \"silhouette\": float(scores[best])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21875ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [(c['exemplar_subject'], c['subjects'], len(c['subjects'])) for c in result['clusters'].values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f78a5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame({\n",
    "    'bill_id': list(so.keys()),\n",
    "    'subject': list(so.values())\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0540d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_reverse = {sub: cid for cid, c in clusters.items() for sub in c['subjects']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a5bb52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['clean_subject'] = labels['bill_id'].map(canonical_subjects)\n",
    "labels['cluster'] = labels['clean_subject'].map(clust_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1ddba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bill_ids.txt', 'r') as f:\n",
    "    bill_ids = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4b22c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('missed_bills.txt', 'r') as f:\n",
    "    missed_bills = f.read().splitlines()\n",
    "\n",
    "for b in labels.loc[~labels['bill_id'].isin(missed_bills) & (labels['cluster'].isna()), 'bill_id'].unique().tolist():\n",
    "    missed_bills.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d073320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_id_mapping = pickle.load(open('bill_id_mapping.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31c351a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_variations = [k for k, v in bill_id_mapping.items() if v in missed_bills]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70e8059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_labels = {}\n",
    "for _, row in labels.loc[labels['cluster'].notna(), ['bill_id', 'cluster']].drop_duplicates().iterrows():\n",
    "    bill_labels[row['bill_id']] = row['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e06a54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bill_labels.json', 'w') as f:\n",
    "    json.dump(bill_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43e78f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "digests = pd.read_csv('ca_leg/legislation_data/digest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97292f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_embeddings = torch.load('digests.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11a8e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_version(identifier):\n",
    "    nums = re.findall(r'(\\d+)', str(identifier))\n",
    "    return int(nums[-1]) if nums else 0\n",
    "\n",
    "repairs = digests.loc[digests['bill_id'].isin(missing_variations)].copy()\n",
    "repairs['bill'] = repairs['bill_id'].map(bill_id_mapping)\n",
<<<<<<< HEAD
    "repairs['version'] = repairs['bill_id'].apply(lambda x: x[-5:-3] if 'VETO' not in x else re.search(r'\\d{2}(?=VETO)', x).group()).astype(int)\n",
    "repairs = repairs.sort_values('version', ascending=False).groupby('bill').head(1)"
=======
    "repairs['version'] = repairs['bill_id'].apply(extract_version)\n",
    "repairs = (\n",
    "    repairs.sort_values('version', ascending=False)\n",
    "    .groupby('bill')\n",
    "    .head(1)\n",
    "    .reset_index(drop=True)\n",
    ")\n"
>>>>>>> d35b3fc56d8e3818a836cf9c7ba48fa1e07beca5
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79035831",
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_embedding_lookup = {\n",
    "    key: tensor.cpu().numpy()\n",
    "    for key, tensor in digest_embeddings.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10458241",
   "metadata": {},
   "outputs": [],
   "source": [
    "repairs['digest_embedding'] = repairs['DigestText'].map(digest_embedding_lookup)\n",
    "repairs = repairs.loc[repairs['digest_embedding'].notna()].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3ed503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = labels[['bill_id', 'cluster']].drop_duplicates().rename(columns={'bill_id': 'bill'})\n",
    "labeled_variations = [k for k, v in bill_id_mapping.items() if v in labeled['bill'].values]\n",
    "\n",
    "labeled_digests = digests.loc[digests['bill_id'].isin(labeled_variations)].copy()\n",
    "labeled_digests['bill'] = labeled_digests['bill_id'].map(bill_id_mapping)\n",
    "labeled_digests['version'] = labeled_digests['bill_id'].apply(extract_version)\n",
    "labeled_digests = (\n",
    "    labeled_digests.sort_values('version', ascending=False)\n",
    "    .groupby('bill')\n",
    "    .head(1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "labeled_digests['digest_embedding'] = labeled_digests['DigestText'].map(digest_embedding_lookup)\n",
    "labeled_digests = (\n",
    "    labeled_digests.merge(labeled, on='bill', how='inner')\n",
    "    .loc[lambda df: df['digest_embedding'].notna()]\n",
    "    .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03223df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(matrix):\n",
    "    matrix = np.asarray(matrix, dtype=np.float32)\n",
    "    norms = np.linalg.norm(matrix, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    return matrix / norms\n",
    "\n",
    "if labeled_digests.empty or repairs.empty:\n",
    "    train_matrix = np.empty((0, 0))\n",
    "    train_labels = np.array([])\n",
    "    query_matrix = np.empty((0, 0))\n",
    "else:\n",
    "    train_matrix = normalize_embeddings(np.vstack(labeled_digests['digest_embedding'].values))\n",
    "    train_labels = labeled_digests['cluster'].values\n",
    "    query_matrix = normalize_embeddings(np.vstack(repairs['digest_embedding'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f6c14cf",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "X = np.stack(t['digest_embedding'].values)\n",
    "y = t['cluster'].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78420205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emeliasprott/miniconda3/envs/ml/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.2412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': [10, 15],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=1)\n",
    "grid = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=2, verbose=1)\n",
    "grid.fit(X, y)\n",
    "print(f\"Best Score: {grid.best_score_:.4f}\")"
=======
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "if len(train_matrix) and len(query_matrix):\n",
    "    n_neighbors = min(15, len(train_matrix))\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm='brute')\n",
    "    nn.fit(train_matrix)\n",
    "    distances, indices = nn.kneighbors(query_matrix, return_distance=True)\n",
    "\n",
    "    def weighted_vote(neighbor_indices, neighbor_distances):\n",
    "        sims = 1 - neighbor_distances\n",
    "        scores = defaultdict(float)\n",
    "        for idx, sim in zip(neighbor_indices, sims):\n",
    "            scores[train_labels[idx]] += float(sim)\n",
    "        return max(scores.items(), key=lambda item: item[1])[0]\n",
    "\n",
    "    predicted_clusters = [\n",
    "        weighted_vote(ind, dist) for ind, dist in zip(indices, distances)\n",
    "    ]\n",
    "    repairs['label_pred'] = predicted_clusters\n",
    "else:\n",
    "    repairs['label_pred'] = np.nan\n"
>>>>>>> d35b3fc56d8e3818a836cf9c7ba48fa1e07beca5
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73e5e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = {k: v for k, v in repairs[['bill', 'label_pred']].values if pd.notna(v)}\n",
    "bbb = bill_labels.copy()\n",
    "bbb.update(reps)\n",
    "\n",
    "with open('bill_labels_updated.json', 'w') as f:\n",
    "    json.dump(bbb, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff37214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bill_labels_updated.json', 'r') as f:\n",
    "    updated_labels = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
