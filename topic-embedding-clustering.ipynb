{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10e4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "import re, torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd3f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bill_subjects.json', 'r') as f:\n",
    "    bill_subjects = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ddba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bill_ids.txt', 'r') as f:\n",
    "    bill_ids = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa15278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_bill_subject(bill_id):\n",
    "    url = f'https://leginfo.legislature.ca.gov/faces/billStatusClient.xhtml?bill_id={bill_id}'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup.select('.statusCellData #subject')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d8ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1230/26608 [03:50<3:14:11,  2.18it/s]"
     ]
    }
   ],
   "source": [
    "subs2 = []\n",
    "for bill_id in tqdm(bill_ids):\n",
    "    try:\n",
    "        subs2.append(get_bill_subject(bill_id))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ac171",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e874c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(title):\n",
    "    if not isinstance(title, str):\n",
    "        return ''\n",
    "    title = re.sub(r'\\(.*?\\)', '', title)\n",
    "    title = re.sub(r'[^a-zA-Z0-9\\s]', ' ', title)\n",
    "    title = re.sub(r'\\s+', ' ', title).strip()\n",
    "    return title.lower()\n",
    "\n",
    "def batched_embeddings(values, output_dims=384):\n",
    "    vals = [text_clean(v) for v in values if isinstance(v, str)]\n",
    "    vals = list(set([v for v in vals if v != '']))\n",
    "    embeddings = model.encode(vals, batch_size=64, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True, num_workers=4, output_dims=output_dims)\n",
    "    embs = {v: e for v, e in zip(vals, embeddings)}\n",
    "    return embs\n",
    "\n",
    "subjects2 = batched_embeddings(subs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e33c8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = torch.load('subject_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6936b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([subjects[subject].cpu().numpy() for subject in subjects])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d2d1652a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:58<00:00, 58.64s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "clusters",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "silhouette",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "ref": "b139e412-38aa-4285-aced-a134b64ded39",
       "rows": [
        [
         "375",
         "[372  79   4 ... 218  15  35]",
         "0.04409744"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters</th>\n",
       "      <th>silhouette</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>[372, 79, 4, 319, 13, 361, 128, 282, 219, 195,...</td>\n",
       "      <td>0.044097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clusters  silhouette\n",
       "375  [372, 79, 4, 319, 13, 361, 128, 282, 219, 195,...    0.044097"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering = {}\n",
    "for n in tqdm([375]):\n",
    "    clusterer = AgglomerativeClustering(\n",
    "        n_clusters=n,\n",
    "        linkage='ward'\n",
    "    )\n",
    "    clusters = clusterer.fit_predict(X)\n",
    "    silhouette = silhouette_score(X, clusters)\n",
    "    clustering[n] = {'clusters': clusters, 'silhouette': silhouette}\n",
    "pd.DataFrame.from_dict(clustering, orient='index').sort_values('silhouette', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "414bfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = clustering[375]['clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e7031e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes = silhouette_samples(X, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9d32ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = pd.DataFrame({'subject': subjects.keys(), 'label': clusters, 'silhouette': silhouettes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ac48dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {k: v.values[0][0] for k, v in subj.sort_values('silhouette', ascending=False).groupby('label').head(1).reset_index(drop=True).groupby('label')[['subject']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6071bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3e3439a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_ids = np.sort(subj['label'].unique())\n",
    "centroids = np.vstack([X[subj['label'] == leaf_id].mean(axis=0) for leaf_id in leaf_ids])\n",
    "Z = linkage(centroids, method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ac80b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [150, 75, 40, 15]\n",
    "hierarchy = {}\n",
    "for k in targets:\n",
    "    s_labels = fcluster(Z, k, criterion='maxclust') - 1\n",
    "    map_leaf = dict(zip(leaf_ids, s_labels))\n",
    "    hierarchy[k] = subj['label'].map(map_leaf).to_numpy()\n",
    "hierarchy = pd.DataFrame(hierarchy, index=subj['subject']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "61190f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings = subj.merge(hierarchy, on='subject', how='left').rename(columns={150: 'group_150', 75: 'group_75', 40: 'group_40', 15: 'group_15'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3bd18b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings[['subject', 'label']].drop_duplicates().to_csv('subject_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d2881057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "grouped = groupings.groupby('label')['subject'].apply(list)\n",
    "\n",
    "clear_cluster_names = {}\n",
    "\n",
    "for label, subjects in grouped.items():\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 5), max_features=200)\n",
    "    tfidf = vectorizer.fit_transform(subjects)\n",
    "    c_vectorizer = CountVectorizer(stop_words='english', ngram_range=(2, 4), max_features=200)\n",
    "\n",
    "    if tfidf.shape[0] > 1:\n",
    "        svd = TruncatedSVD(n_components=1, random_state=0)\n",
    "        topic_vector = svd.fit(tfidf).components_[0]\n",
    "        c_topic_vector = svd.fit(c_vectorizer.fit_transform(subjects)).components_[0]\n",
    "        tf_top = vectorizer.get_feature_names_out()[topic_vector.argsort()[::-1][:10]]\n",
    "        c_top = c_vectorizer.get_feature_names_out()[c_topic_vector.argsort()[::-1][:10]]\n",
    "\n",
    "    clear_cluster_names[label] = {\n",
    "        'tf_top': tf_top,\n",
    "        'c_top': c_top\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7a4f75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(clear_cluster_names, orient='index').reset_index(names='cluster').to_csv('cluster_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e54ba2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = pd.read_csv('cluster_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "429a4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names.columns = ['cluster', 'tf_top', 'c_top', 'summary_phrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e19383",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = {row['cluster']: row['summary_phrase'] for _, row in cluster_names.iterrows()}\n",
    "groupings['label_name'] = groupings['label'].map(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings.to_csv('groupings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c8f3e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_labels = {row['subject']: row['label'] for _, row in subj.iterrows()}\n",
    "\n",
    "bill_subjects_clean = {}\n",
    "for bill, subject in bill_subjects.items():\n",
    "    bill_subjects_clean[bill] = subj_labels.get(subject, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "beddc84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bill_labels.json', 'w') as f:\n",
    "    json.dump(bill_subjects_clean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247feed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings = pd.read_csv('groupings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "812fdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_labels = groupings.groupby('label')['label_name'].first().loc[leaf_ids].tolist()\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='mps')\n",
    "label_vecs = model.encode(leaf_labels, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_norm = normalize(centroids, norm='l2')\n",
    "a = 0.75\n",
    "hybrid = np.hstack([a*centroids_norm, (1-a)*label_vecs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f18ff445",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = groupings[['group_150', 'group_75', 'group_40', 'group_15', 'label_name']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6972b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.to_csv('groupings_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b7ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1136fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "def cluster_group_doc(level):\n",
    "    g = groupings[[level, 'label_name']].drop_duplicates().groupby(level, sort=True)['label_name'].apply(lambda x: '. '.join(x))\n",
    "    return g\n",
    "\n",
    "qa = pipeline('question-answering', model='deepset/tinyroberta-squad2', device=0)\n",
    "def compress_to_phrase(summary):\n",
    "    question = \"In a single, clear phrase, summarize the subject/s that connect the following phrases: \"\n",
    "    answer = qa(question=question, context=summary, max_length=150, min_length=25)['answer']\n",
    "    return answer\n",
    "\n",
    "def compress_group_doc(level):\n",
    "    group = cluster_group_doc(level)\n",
    "    grouping = {i: '' for i in group.index}\n",
    "    for g, i in tqdm(zip(group, group.index), total=len(group)):\n",
    "        phrase = compress_to_phrase(g)\n",
    "        grouping[i] = phrase.strip()\n",
    "    return grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "163e8e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:07<00:00, 20.73it/s]\n"
     ]
    }
   ],
   "source": [
    "a = compress_group_doc('group_150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56159855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
