{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bill_id', 'location_code', 'vote_date_time', 'motion_id', 'ayes',\n",
       "       'noes', 'abstain', 'vote_result', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_votes = pd.read_csv('ca_leg/legislation_data/bill_summary_vote_tbl.csv')\n",
    "summary_votes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bill_id', 'bill_history_id', 'action_date', 'action_',\n",
       "       'action_sequence', 'action_code', 'action_status', 'primary_location',\n",
       "       'secondary_location', 'end_status', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill_history = pd.read_csv('ca_leg/legislation_data/bill_history_tbl.csv', dtype={'action_status': str, 'primary_location': str, 'secondary_location': str, 'end_status': str})\n",
    "bill_history.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bill_id', 'Contribution', 'House', 'Name'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv('ca_leg/legislation_data/authors.csv')\n",
    "authors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bill_id', 'Action', 'Date'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.read_csv('ca_leg/legislation_data/history.csv')\n",
    "history.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bill_id', 'MeasureType', 'Urgency', 'MeasureNum', 'GeneralSubject',\n",
       "       'VersionNum', 'Appropriation', 'SessionYear', 'SessionNum',\n",
       "       'VoteRequired', 'LocalProgram', 'FiscalCommittee', 'MeasureState',\n",
       "       'TaxLevy', 'Title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "versions = pd.read_csv('ca_leg/legislation_data/bill_versions.csv')\n",
    "versions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bill_id', 'location_code', 'legislator_name', 'vote_date_time',\n",
       "       'vote_date_seq', 'vote_code', 'motion_id', 'member_order',\n",
       "       'session_date', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill_votes = pd.read_csv('ca_leg/legislation_data/bill_detail_vote_tbl.csv', parse_dates=['session_date'])\n",
    "bill_votes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bill_id', 'location_code', 'vote_date_time', 'motion_id', 'ayes',\n",
       "       'noes', 'abstain', 'vote_result', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill_summary = pd.read_csv('ca_leg/legislation_data/bill_summary_vote_tbl.csv')\n",
    "bill_summary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['motion_id', 'motion_text', 'year'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill_motions = pd.read_csv('ca_leg/legislation_data/bill_motion_tbl.csv')\n",
    "bill_motions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['committee_code', 'committee_name'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = pd.read_csv('ca_leg/legislation_data/committee_codes.csv')\n",
    "locations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['committee_clean', 'position', 'Occupation', 'Party', 'District No.',\n",
       "       'Seat No.', 'Term', 'Last', 'full_name', 'chamber'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politicians = pd.read_csv('ca_leg/legislation_data/politicians.csv')\n",
    "politicians.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FILING_ID', 'FIRM_NAME', 'AMEND_ID', 'LINE_ITEM', 'REC_TYPE',\n",
       "       'FORM_TYPE', 'TRAN_ID', 'RECSUBTYPE', 'ENTITY_CD', 'PAYEE_NAML',\n",
       "       'PAYEE_NAMF', 'PAYEE_NAMT', 'PAYEE_NAMS', 'PAYEE_CITY', 'PAYEE_ST',\n",
       "       'PAYEE_ZIP4', 'CREDCARDCO', 'BENE_NAME', 'BENE_POSIT', 'BENE_AMT',\n",
       "       'EXPN_DSCR', 'EXPN_DATE', 'AMOUNT', 'MEMO_CODE', 'MEMO_REFNO',\n",
       "       'BAKREF_TID', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27',\n",
       "       'Unnamed: 28', 'clean_beneficiary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lobbying = pd.read_csv('calaccess/lobbying_clean2.csv', dtype={'PAYEE_NAMS': str, 'BAKREF_TID': str})\n",
    "lobbying.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TargetCandidateName', 'TargetCandidateOffice', 'TargetPropositionName',\n",
       "       'ExpenderPosition', 'ExpenderName', 'ExpenderID', 'Amount',\n",
       "       'ExpenditureDscr', 'PayeeName', 'DateStart', 'DateEnd', 'DateRange',\n",
       "       'year', 'term_x', 'matched_target_name', 'term_y', 'politician',\n",
       "       'committee', 'position', 'committee_clean', 'Occupation', 'Party',\n",
       "       'District No.', 'Seat No.', 'Term', 'First', 'Last', 'Position',\n",
       "       'full_name', 'chamber', 'target_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expend_assembly = pd.read_csv('calaccess/expend_assembly_matched.csv', dtype={'TargetPropositionName': str})\n",
    "expend_assembly.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TargetCandidateName', 'TargetCandidateOffice', 'TargetPropositionName',\n",
       "       'ExpenderPosition', 'ExpenderName', 'ExpenderID', 'Amount',\n",
       "       'ExpenditureDscr', 'PayeeName', 'DateStart', 'DateEnd', 'DateRange',\n",
       "       'matched_target_name', 'term', 'politician', 'committee', 'position',\n",
       "       'committee_clean', 'Name', 'Occupation', 'Party', 'District No.',\n",
       "       'Seat No.', 'Phone', 'Counties', 'pages', 'Last', 'Term', 'full_name',\n",
       "       'chamber'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expend_senate = pd.read_csv('calaccess/expend_senate_matched.csv', dtype={'TargetPropositionName': str})\n",
    "expend_senate.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bill_id', 'DigestText'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digests = pd.read_csv('ca_leg/legislation_data/digest.csv')\n",
    "digests.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes['vote_date_time'] = pd.to_datetime(bill_votes['vote_date_time']).apply(lambda x: x.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes['legislator_name'] = bill_votes['legislator_name'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_KEYWORDS = [\n",
    "    \"Assembly Third Reading\", \"Assembly 3rd reading\", 'senate 3rd reading', \"Senate Third Reading\",\"Concurrence - Urgency Added\", \"Concurrence in Senate Amendments\", \"Do pass as amended, and re-refer\", \"Do pass as amended, but re-refer\", \"Do pass as amended\", \"Do pass and be re-referred\",\n",
    "    \"Concurrence\", \"Consent Calendar\", \"Urgency Clause\", \"Special Consent\",\n",
    "    \"Motion to Reconsider\", \"Do pass\", \"Reconsideration\", \"Committee amendments\",\n",
    "    \"W/O REF. TO FILE\", \"Be re-referred to the Committee\",\n",
    "    \"Lay on the Table\", \"Amend by\", \"Unfinished Business\", \"Placed on Appropriations Suspense File\",\n",
    "]\n",
    "\n",
    "def extract_action(motion_text):\n",
    "    if not isinstance(motion_text, str) or motion_text is None:\n",
    "        return None\n",
    "    motion = motion_text.upper()\n",
    "\n",
    "    action = next((act for act in ACTION_KEYWORDS if act.upper() in motion), None)\n",
    "    if action != 'Reconsideration' and 'RECONSIDER' in motion:\n",
    "        if action is not None:\n",
    "            action += ' Reconsideration'\n",
    "        else:\n",
    "            action = 'Reconsideration'\n",
    "\n",
    "\n",
    "    return action if action else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_motions['simplified_motion'] = bill_motions['motion_text'].apply(extract_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_coms = {}\n",
    "for i, row in locations.iterrows():\n",
    "    if row['committee_code'].startswith('CZ'):\n",
    "        continue\n",
    "    name = row['committee_name']\n",
    "    if row['committee_code'].startswith('CS'):\n",
    "        if name.startswith('Sen.'):\n",
    "            cname = re.sub(r'Sen. ', 'senate ', name).lower()\n",
    "        elif name.startswith('Senate '):\n",
    "            cname = name.lower()\n",
    "        else:\n",
    "            cname = 'senate ' + name.lower()\n",
    "    elif row['committee_code'].startswith('CX'):\n",
    "        if name.lower().startswith('assembly'):\n",
    "            cname = name.lower()\n",
    "        else:\n",
    "            cname = 'assembly ' + name.lower()\n",
    "    if re.search(r'x\\d$', cname) is not None:\n",
    "        cname = re.sub(r'x(?=\\d$)', 'no. ', cname)\n",
    "    clean_coms[row['committee_code']] = cname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "leg_committees = [f\"{row['chamber']} {row['committee_clean']}\".lower() for _, row in politicians[['committee_clean', 'chamber']].drop_duplicates().iterrows()]\n",
    "\n",
    "def match_committees(_names, clean_coms, threshold=92):\n",
    "    clean_c = list(clean_coms.values())\n",
    "    clean_codes = list(clean_coms.keys())\n",
    "    name_mapping = {}\n",
    "    for i, clean in enumerate(clean_c):\n",
    "        code = clean_codes[i]\n",
    "        matches = []\n",
    "        matches.append(process.extractOne(\n",
    "            clean,\n",
    "            _names,\n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            score_cutoff=threshold\n",
    "        ))\n",
    "        matches.append(process.extractOne(\n",
    "            clean,\n",
    "            _names,\n",
    "            scorer=fuzz.partial_ratio,\n",
    "            score_cutoff=threshold\n",
    "        ))\n",
    "        valid_matches = [m for m in matches if m is not None]\n",
    "        if len(valid_matches) > 0:\n",
    "            best_match = max(valid_matches, key=lambda x: x[1])\n",
    "            name_mapping[code] = best_match[0]\n",
    "        else:\n",
    "            fall_back = process.extractOne(\n",
    "                clean,\n",
    "                _names,\n",
    "                scorer=fuzz.token_sort_ratio,\n",
    "                score_cutoff=threshold - 8\n",
    "            )\n",
    "            if fall_back is not None:\n",
    "                name_mapping[code] = fall_back[0]\n",
    "            else:\n",
    "                name_mapping[code] = None\n",
    "    return name_mapping\n",
    "\n",
    "committee_matches = match_committees(leg_committees, clean_coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations['committee_clean'] = locations['committee_code'].map(committee_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.loc[locations['committee_name'] == 'EDUCATION X5', 'committee_clean'] = 'Budget and Fiscal Review: Education'\n",
    "locations.loc[locations['committee_code'] == 'CX12', 'committee_clean'] = 'Budget No. 1 on Health and Human Services'\n",
    "locations.loc[locations['committee_code'] == 'CS68', 'committee_clean'] = 'Budget No. 3 - Health and Human Services'\n",
    "locations.loc[locations['committee_code'] == 'CS66', 'committee_clean'] = 'Senate Veterans Affairs'\n",
    "locations.loc[locations['committee_code'] == 'CS56', 'committee_clean'] = 'Senate Public Employment and Retirement'\n",
    "locations.loc[locations['committee_code'] == 'CS62', 'committee_clean'] = 'Senate Budget and Fiscal Review'\n",
    "locations.loc[locations['committee_code'] == 'CX23', 'committee_clean'] = 'Assembly Utilities and Commerce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_codes = {\n",
    "    row['motion_id']: row['simplified_motion']\n",
    "    for _, row in bill_motions.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_votes['motion_text'] = summary_votes['motion_id'].map(motion_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_bill_id(id):\n",
    "    front, end = id[:4], id[4:]\n",
    "    if re.search(r'\\d{4}$', front):\n",
    "        return f\"{front}{int(front) + 1}{end}\"\n",
    "    else:\n",
    "        return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions['ID'] = versions['bill_id'].apply(lambda x: repair_bill_id(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_vers = versions.loc[versions['bill_id'].str.startswith('2')]\n",
    "for i, row in bill_vers.iterrows():\n",
    "    tail = f\"{row['VersionNum']}{row['MeasureState']}\"\n",
    "    repaired = repair_bill_id(re.sub(tail, '', row['bill_id']))\n",
    "    end = int(repaired[-4:])\n",
    "\n",
    "    bill_vers.loc[i, 'bill_ID'] = f\"{repaired[:-4]}{end}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislators = {i: pol for i, pol in enumerate(politicians['full_name'].unique().tolist())}\n",
    "\n",
    "leg_parties = {row['full_name']: row['Party'] for _, row in politicians[['full_name', 'Party']].drop_duplicates().iterrows()}\n",
    "leg_occupations = {row['full_name']: row['Occupation'] for _, row in politicians[['full_name', 'Occupation']].drop_duplicates().iterrows()}\n",
    "committees = {i: com for i, com in enumerate(politicians['committee_clean'].unique().tolist())}\n",
    "lobby_firms = {i: firm for i, firm in enumerate(lobbying['FIRM_NAME'].unique().tolist())}\n",
    "\n",
    "donor_names = list(set(expend_assembly['ExpenderName'].unique().tolist() + expend_senate['ExpenderName'].unique().tolist()))\n",
    "donors = {i: donor for i, donor in enumerate(donor_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dict(d, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(d, f)\n",
    "\n",
    "for name, d in zip(\n",
    "    ['legislators', 'committees', 'lobby_firms', 'donors'],\n",
    "    [legislators, committees, lobby_firms, donors]\n",
    "):\n",
    "    save_dict(d, f'{name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_titles = {row['bill_ID']: row['Title'] for _, row in bill_vers[['bill_ID', 'Title']].drop_duplicates().iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_subjects = {row['bill_ID']: row['GeneralSubject'] for _, row in bill_vers.loc[bill_vers['GeneralSubject'].apply(lambda x: x is not None and isinstance(x, str)), ['bill_ID', 'GeneralSubject']].drop_duplicates().iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_ids = list(set(bill_votes.loc[bill_votes['bill_id'].str.startswith('2'), 'bill_id'].unique().tolist() + summary_votes.loc[summary_votes['bill_id'].str.startswith('2'), 'bill_id'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_id_codes = {row['bill_id']: row['bill_ID'] for _, row in bill_vers.drop_duplicates(subset=['bill_id', 'bill_ID']).iterrows()}\n",
    "history['bill_ID'] = history['bill_id'].map(bill_id_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['Date'] = pd.to_datetime(history['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_dates = {}\n",
    "for v, group in history.loc[history['bill_ID'].isin(bill_ids)].groupby('bill_ID'):\n",
    "    introduction_dates[v] = {'Dates': group['Date'].unique().tolist(), 'Actions': group.sort_values('Date', ascending=True).drop_duplicates(subset=['Action', 'Date'])['Action'].tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_id_mapping = {i: list(group.values) for i, group in bill_vers.groupby('bill_ID')['ID']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_bill_ids = {}\n",
    "for i in summary_votes.loc[summary_votes['bill_id'].isin(bill_ids)].groupby(['year', 'motion_id'])['bill_id'].value_counts().index:\n",
    "    year, motion_id, bill_id = i\n",
    "    if (year, motion_id) not in vote_bill_ids.keys():\n",
    "        vote_bill_ids[(year, motion_id)] = [bill_id]\n",
    "    else:\n",
    "        vote_bill_ids[(year, motion_id)].append(bill_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_vers_dig = bill_vers.merge(digests, on='bill_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislators_last_names = {}\n",
    "for _, row in politicians[['chamber', 'Last', 'Term', 'full_name']].drop_duplicates().iterrows():\n",
    "    legislators_last_names[(row['chamber'], row['Last'].lower(), row['Term'])] = row['full_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {row['ID']: {\n",
    "    'digest': row['DigestText'],\n",
    "    'MeasureState': row['MeasureState'],\n",
    "    'VoteRequired': row['VoteRequired'] if row['VoteRequired'] is not None else 'No',\n",
    "    'VersionNum': row['VersionNum'] if row['VersionNum'] is not None else 'No',\n",
    "    'LocalProgram': row['LocalProgram'] if row['LocalProgram'] is not None else 'No',\n",
    "    'FiscalCommittee': row['FiscalCommittee'] if row['FiscalCommittee'] is not None else 'No',\n",
    "    'TaxLevy': row['TaxLevy'] if row['TaxLevy'] is not None else 'No',\n",
    "    'Urgency': row['Urgency'] if row['Urgency'] is not None else 'No'} for _, row in bill_vers_dig.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislator_codes = {v: k for k, v in legislators.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_codes = {v.lower(): k for k, v in committees.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_votes['chamber'] = bill_votes['location_code'].apply(lambda x: 'assembly' if x == 'AFLOOR' or x.startswith('CX') else 'senate' if x == 'SFLOOR' or x.startswith('CS') else 'full')\n",
    "bill_votes['vote_date_time'] = pd.to_datetime(bill_votes['vote_date_time'])\n",
    "bill_votes['term'] = bill_votes['vote_date_time'].apply(lambda x: f\"{x.year}-{x.year + 1}\" if x.year % 2 == 1  else f\"{x.year - 1}-{x.year}\" if x.year % 2 == 0 and x < pd.Timestamp(year=x.year, month=11, day=2) else f\"{x.year + 1}-{x.year + 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_locations = authors.loc[(authors['House'] == 'UNKNOWN') & (authors['bill_id'].map(bill_id_codes).isin(bill_ids)), ['bill_id', 'Name']].drop_duplicates()\n",
    "for i, row in author_locations.iterrows():\n",
    "    if 'AB' in row['bill_id']:\n",
    "        author_locations.loc[i, 'name'] = 'Assembly ' + row['Name']\n",
    "    elif 'SB' in row['bill_id']:\n",
    "        author_locations.loc[i, 'name'] = 'Senate ' + row['Name']\n",
    "    else:\n",
    "        author_locations.loc[i, 'name'] = 'Joint ' + row['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fuzzy_strings(source_list, target_list):\n",
    "    def preprocess_name(name):\n",
    "        if not isinstance(name, str):\n",
    "            return \"\"\n",
    "        name = name.lower()\n",
    "        name = re.sub(r'\\(.*?\\)', '', name)\n",
    "        name = re.sub(r'committee on', '', name)\n",
    "        name = re.sub(r'[^a-z\\s]', ' ', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        return name\n",
    "\n",
    "    clean_source = [preprocess_name(c) for c in source_list]\n",
    "    clean_target = [preprocess_name(c) for c in target_list]\n",
    "\n",
    "    keywords = [\"education\", \"health\", \"finance\", \"budget\", \"transportation\",\n",
    "                \"judiciary\", \"environment\", \"agriculture\", \"energy\", \"labor\",\n",
    "                \"housing\", \"veterans affairs\", \"public safety\", \"insurance\", \"banking\", \"public health\", \"small business\", \"redistricting\",\n",
    "                \"public utilities\", \"natural resources\", \"water\",\n",
    "                \"technology\", \"communications\", \"elections\", \"government\",\n",
    "                \"appropriations\", \"rules\", \"ethics\", 'criminal justice', \"environmental protection\", \"college and university\", \"human services\", \"reproductive health\", \"mental health\", \"technology\", \"aggriculture\", \"urban development\", \"renewable energy\", \"gun violence\", \"commerce\", \"privacy\", \"cybersecurity\", \"infrastructure\", \"disaster preparedness\", \"prisons\", \"aging\"]\n",
    "\n",
    "    def get_committee_keywords(name):\n",
    "        return set(kw for kw in keywords if kw in name)\n",
    "\n",
    "    target_keywords = [get_committee_keywords(name) for name in clean_target]\n",
    "\n",
    "    def calculate_similarity(source_idx, target_idx):\n",
    "        source = clean_source[source_idx]\n",
    "        target = clean_target[target_idx]\n",
    "\n",
    "        if not source or not target:\n",
    "            return 0\n",
    "\n",
    "        if source == target:\n",
    "            return 100\n",
    "\n",
    "        token_sort = fuzz.token_sort_ratio(source, target)\n",
    "        token_set = fuzz.token_set_ratio(source, target)\n",
    "        partial = fuzz.partial_ratio(source, target)\n",
    "\n",
    "        source_kw = get_committee_keywords(source)\n",
    "        keyword_overlap = len(source_kw.intersection(target_keywords[target_idx]))\n",
    "        keyword_bonus = min(20, keyword_overlap * 10)\n",
    "        weighted_score = (token_sort * 0.3) + (token_set * 0.5) + (partial * 0.2) + keyword_bonus\n",
    "\n",
    "        return weighted_score\n",
    "\n",
    "    matches = {}\n",
    "    for i, source in enumerate(source_list):\n",
    "        scores = [calculate_similarity(i, j) for j in range(len(target_list))]\n",
    "\n",
    "        if not scores or max(scores) < 60:\n",
    "            matches[source] = None\n",
    "        else:\n",
    "            best_idx = np.argmax(scores)\n",
    "            confidence = scores[best_idx]\n",
    "\n",
    "            if confidence >= 60:\n",
    "                matches[source] = target_list[best_idx]\n",
    "            else:\n",
    "                matches[source] = None\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_com_matches = fuzzy_strings(author_locations['name'].unique().tolist(), leg_committees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_locations['name'] = author_locations['name'].map(author_com_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sponsors = authors.loc[authors['bill_id'].map(bill_id_codes).isin(bill_ids)]\n",
    "sponsors['term'] = sponsors['bill_id'].apply(lambda x: f\"{x[:4]}-{int(x[:4]) + 1}\" if int(x[:4]) % 2 == 1 else f\"{int(x[:4]) - 1}-{x[:4]}\" if int(x[:4]) % 2 == 0 and int(x[:4]) < 2009 else f\"{x[:4]}-{int(x[:4]) + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob = lobbying.loc[lobbying['clean_beneficiary'].notna(), ['FIRM_NAME', 'EXPN_DSCR', 'clean_beneficiary', 'EXPN_DATE', 'BENE_AMT']]\n",
    "lob['EXPN_DATE'] = pd.to_datetime(lob['EXPN_DATE'])\n",
    "lob['term'] = lob['EXPN_DATE'].apply(lambda x: f\"{x.year}-{x.year + 1}\" if x.year % 2 == 1  else f\"{x.year - 1}-{x.year}\" if x.year % 2 == 0 and x < pd.Timestamp(year=x.year, month=11, day=2) else f\"{x.year}-{x.year + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_names_terms = {}\n",
    "for _, row in politicians[['full_name', 'Term', 'chamber']].drop_duplicates().iterrows():\n",
    "    pol_names_terms[(row['full_name'].lower(), row['Term'])] = {'chamber': row['chamber'], 'name': row['full_name']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_contributions = pd.concat([expend_assembly.loc[expend_assembly['full_name'].notna(), ['ExpenderName', 'Amount', 'full_name', 'Term', 'chamber', 'DateEnd']].drop_duplicates(subset=['ExpenderName', 'Amount', 'full_name', 'DateEnd']), expend_senate.loc[expend_senate['full_name'].notna(), ['ExpenderName', 'Amount', 'full_name', 'Term', 'chamber', 'DateEnd']].drop_duplicates(subset=['ExpenderName', 'Amount', 'full_name', 'DateEnd'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_contributions['DateEnd'] = pd.to_datetime(campaign_contributions['DateEnd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sponsors['bill_ID'] = sponsors['bill_id'].apply(repair_bill_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2ad82622c74ec381c5b48bd3a3497f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subjects = list(set([t for t in versions.loc[versions['bill_id'].str.startswith('2')]['GeneralSubject'].tolist() if (isinstance(t, str) and t is not None)]))\n",
    "embeddings = model.encode(\n",
    "    subjects,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    num_workers=4,\n",
    "    normalize_embeddings=True,\n",
    "    truncate_dim=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_embeddings = {subjects[i]: embeddings[i] for i in range(len(subjects))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_embeddings = {}\n",
    "for occ in list(set(list(leg_occupations.values()))):\n",
    "    if isinstance(occ, str) and len(occ) > 0:\n",
    "        occ_embeddings[occ] = model.encode(\n",
    "            occ,\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True,\n",
    "            truncate_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f77dfa078742a7b6534d0b18e30d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1448 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titles = [t for t in bill_vers['Title'].unique().tolist() if (isinstance(t, str) and t is not None and t != '')]\n",
    "title_embeddings = model.encode(\n",
    "    titles,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    num_workers=4,\n",
    "    normalize_embeddings=True,\n",
    "    truncate_dim=64\n",
    ")\n",
    "title_embeddings = {titles[i]: title_embeddings[i] for i in range(len(titles))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobbying_firms_embeddings = {}\n",
    "for firm in [firm for firm in lobbying.loc[(lobbying['clean_beneficiary'].notna()) & (lobbying['FIRM_NAME'])]['FIRM_NAME'].unique().tolist() if isinstance(firm, str)]:\n",
    "    lobbying_firms_embeddings[firm] = model.encode(firm, convert_to_tensor=True, normalize_embeddings=True, truncate_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 2720/5541 [01:22<01:25, 32.93it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m lobbying_expense_embeddings = {}\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m expense \u001b[38;5;129;01min\u001b[39;00m tqdm([expense \u001b[38;5;28;01mfor\u001b[39;00m expense \u001b[38;5;129;01min\u001b[39;00m lobbying.loc[(lobbying[\u001b[33m'\u001b[39m\u001b[33mclean_beneficiary\u001b[39m\u001b[33m'\u001b[39m].notna())][\u001b[33m'\u001b[39m\u001b[33mEXPN_DSCR\u001b[39m\u001b[33m'\u001b[39m].unique().tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expense, \u001b[38;5;28mstr\u001b[39m)]):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     lobbying_expense_embeddings[expense] = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpense\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:685\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    682\u001b[39m features.update(extra_features)\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    687\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:752\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    750\u001b[39m     module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m    751\u001b[39m     module_kwargs = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:442\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m trans_features = {\n\u001b[32m    437\u001b[39m     key: value\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    440\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m output_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m output_tokens = output_states[\u001b[32m0\u001b[39m]\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1144\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1142\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1156\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1157\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    574\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    575\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    583\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    584\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    592\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    594\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    506\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    507\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    514\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    525\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:440\u001b[39m, in \u001b[36mBertSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[32m    436\u001b[39m is_causal = (\n\u001b[32m    437\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    438\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    450\u001b[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m.all_head_size)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "lobbying_expense_embeddings = {}\n",
    "for expense in tqdm([expense for expense in lobbying.loc[(lobbying['clean_beneficiary'].notna())]['EXPN_DSCR'].unique().tolist() if isinstance(expense, str)]):\n",
    "    lobbying_expense_embeddings[expense] = model.encode(expense, convert_to_tensor=True, normalize_embeddings=True, truncate_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_embeddings = {}\n",
    "for committee in politicians['committee_clean'].unique().tolist():\n",
    "    co = re.sub(r'assembly|senate|committee|subcommittee', '', committee.lower())\n",
    "    committee_embeddings[committee.lower()] = model.encode(co,  convert_to_tensor=True, normalize_embeddings=True, truncate_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1136/1136 [00:42<00:00, 26.44it/s]\n"
     ]
    }
   ],
   "source": [
    "donor_embeddings = {}\n",
    "\n",
    "for donor in tqdm(donor_names):\n",
    "    donor_embeddings[donor] = model.encode(donor, convert_to_tensor=True, normalize_embeddings=True, truncate_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 63.04it/s]\n"
     ]
    }
   ],
   "source": [
    "motion_embeddings = {}\n",
    "for motion in tqdm([t for t in pd.DataFrame.from_dict(motion_codes, orient='index').reset_index().rename({'index': 'motion_id', 0: 'motion_text'}, axis=1)['motion_text'].drop_duplicates().tolist() if t is not None]):\n",
    "    motion_embeddings[motion] = model.encode(motion, convert_to_tensor=True, normalize_embeddings=True, truncate_dim=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```# digest_embeddings = {}\n",
    "# for digest in tqdm(list(set([t if (isinstance(t, str) and t is not None) else '' for t in digests.loc[digests['bill_id'].str.startswith('2')]['DigestText'].tolist()]))):\n",
    "    # digest_embeddings[digest] = model.encode(digest, convert_to_tensor=True, normalize_embeddings=True)\n",
    "# torch.save(digest_embeddings, 'digests.pt')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_embeddings = torch.load('digests.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = {p: 'Democratic Alternate' if re.search(r'Democratic\\s*Alternate', p) is not None else 'Vice Chair' if re.search(r'V\\s*i\\s*c\\s*e\\s*-*\\s*C\\s*h\\s*a\\s*i\\s*r\\s*', p) is not None else 'Co-Chair' if re.search(r'Co\\s*-\\s*Chair', p) is not None else 'Chair' if re.search(r'Cha\\s*i\\s*r', p) is not None else 'Republican Alternate' if re.search(r'\\s*Republican\\s*Alternate', p) is not None else p for p in politicians['position'].unique()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOP Graph-Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, id, type, features=None):\n",
    "        self.id = id\n",
    "        self.type = type\n",
    "        self.features = features or {}\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self, source, target, relation, attributes=None):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.relation = relation\n",
    "        self.attributes = attributes or {}\n",
    "\n",
    "class Bill(Node):\n",
    "    def __init__(self, bill_id, title, subject, measure_type):\n",
    "        features = {\n",
    "            'title': title,\n",
    "            'subject': subject,\n",
    "            'measure_type': measure_type,\n",
    "            'date': None,\n",
    "        }\n",
    "        super().__init__(bill_id, \"bill\", features)\n",
    "        self.actions = None\n",
    "        self.order_df = None\n",
    "        self.outcome = None\n",
    "\n",
    "    def add_actions(self, actions):\n",
    "        if self.actions is None:\n",
    "            self.actions = actions\n",
    "        else:\n",
    "            self.actions = pd.concat([self.actions, actions], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    def add_date(self, date):\n",
    "        self.features['date'] = date\n",
    "\n",
    "    def add_order_df(self, order_df):\n",
    "        if self.order_df is None:\n",
    "            self.order_df = order_df\n",
    "        else:\n",
    "            self.order_df = pd.concat([self.order_df, order_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    def add_outcome(self, outcome):\n",
    "        self.outcome = outcome\n",
    "\n",
    "    def align_actions_versions(self, bill, versions_, dates):\n",
    "        dates = pd.Series(dates).sort_values(ascending=True).drop_duplicates().tolist()\n",
    "        actions = [i for i in introduction_dates.get(bill, {}).get('Actions', []) if i != 'FILED']\n",
    "        if len(actions) > len(dates):\n",
    "            if actions[-2:] == ['ENROLLED', 'CHAPTERED'] or actions[-2:] == ['APPROVED', 'CHAPTERED'] or len(actions) <= 4 and actions[-1] == 'ENROLLED' or abs(len(dates) - len(actions)) >= 2 and actions[-1] == 'ENROLLED' or actions == ['INTRODUCED', 'ENROLLED', 'AMENDED_SENATE'] or actions[-1] == 'APPROVED' or len(actions) == 3 and all(a.startswith('PASSED_') for a in actions[-2:]) or actions == ['ENROLLED', 'INTRODUCED'] or actions == ['INTRODUCED', 'ENROLLED'] or actions == ['INTRODUCED', 'REVISED'] or len(actions) > 3 and actions[-3:] == ['ENROLLED', 'CORRECTED', 'CHAPTERED'] or len(actions) > 3 and actions[-3:] == ['PASSED_SENATE', 'PASSED_ASSEMBLY', 'AMENDED_SENATE'] or list(set(['INTRODUCED', 'PASSED_SENATE', 'PASSED_ASSEMBLY', 'AMENDED_SENATE'])) == list(set(actions)) or list(set(['INTRODUCED', 'PASSED_SENATE', 'PASSED_ASSEMBLY', 'AMENDED_ASSEMBLY'])) == list(set(actions)) or actions[-2] == 'ENROLLED' and actions[-1].startswith('PASSED_') or len(actions) > 5 and actions[-4] == 'CHAPTERED' and actions[-1].startswith('PASSED_') or actions[-2:] == ['PASSED_SENATE', 'PASSED_ASSEMBLY'] or actions == ['INTRODUCED', 'AMENDED_SENATE', 'PASSED_SENATE', 'PASSED_ASSEMBLY', 'AMENDED_ASSEMBLY'] or len(actions) == 9 and actions[-6:] == ['APPROVED', 'CHAPTERED', 'ENROLLED', 'AMENDED_SENATE', 'PASSED_ASSEMBLY', 'PASSED_SENATE'] or len(actions) > 5 and actions[-4:] == ['ENROLLED', 'PASSED_SENATE', 'APPROVED', 'PASSED_ASSEMBLY'] or dates == [pd.Timestamp('2008-12-08 00:00:00'), pd.Timestamp('2008-12-18 00:00:00')]:\n",
    "                dates.append(dates[-1])\n",
    "                if len(actions) > len(dates):\n",
    "                    dates.append(dates[-1])\n",
    "                    if len(actions) > len(dates):\n",
    "                        dates.append(dates[-1])\n",
    "                        if len(actions) > len(dates):\n",
    "                            dates.append(dates[-1])\n",
    "                            if len(actions) > len(dates):\n",
    "                                dates.append(dates[-1])\n",
    "            if len(dates) == 1 and len(actions) > 1:\n",
    "                for _ in range(len(actions) - len(dates)):\n",
    "                    dates.append(dates[0])\n",
    "            if actions[-2:] == ['INTRODUCED', 'PASSED_ASSEMBLY']:\n",
    "                dates = [dates[0]] + dates\n",
    "            if len(actions) >= 6 and actions[:3] == ['INTRODUCED', 'AMENDED_ASSEMBLY', 'ENROLLED'] and actions[-3:] == ['CHAPTERED', 'APPROVED', 'CORRECTED']:\n",
    "                dates = dates[:2] + [dates[2]] + dates[2:-2] + [dates[-2]] + dates[-2:]\n",
    "            elif ('PASSED_ASSEMBLY' in actions and 'AMENDED_ASSEMBLY' in actions) or ('PASSED_SENATE' in actions and 'AMENDED_SENATE' in actions):\n",
    "                if len(dates) == 3:\n",
    "                    dates = dates[:1] + [dates[1]] + dates[1:]\n",
    "                    if all(a for a in ['PASSED_ASSEMBLY', 'AMENDED_ASSEMBLY', 'PASSED_SENATE', 'AMENDED_SENATE'] if a in actions):\n",
    "                        dates = dates[:1] + [dates[1]] + dates[1:]\n",
    "                elif 'PROPOSED_CONFERENCE_REPORT_1' in actions and len(actions) - len(dates) == 2:\n",
    "                    dates = dates[:2] + [dates[2]] + [dates[2]] + dates[2:]\n",
    "                elif len(dates) > 3 and len(actions) - len(dates) > 0 and not (actions[-4:] == ['PASSED_ASSEMBLY', 'ENROLLED', 'PASSED_SENATE', 'CHAPTERED'] and 'AMENDED_SENATE' in actions):\n",
    "                    dates = dates[:2] + [dates[2]] + dates[2:]\n",
    "                if len(actions) > 4 and actions[-3:] == ['ENROLLED', 'PASSED_SENATE', 'CHAPTERED']:\n",
    "                    dates = dates[:-2] + [dates[-2]] + dates[-2:]\n",
    "\n",
    "            if len(actions) - len(dates) == 1:\n",
    "                if 'CORRECTED' in actions:\n",
    "                    actions.remove('CORRECTED')\n",
    "                elif 'RESCIND' in actions:\n",
    "                    actions.remove('RESCIND')\n",
    "\n",
    "            if actions[-1] == 'CORRECTED' and len(actions) - len(dates) == 2:\n",
    "                if len(dates) >= 5:\n",
    "                    dates = dates[:3] + [dates[3]] + [dates[3]] + [dates[4]] + dates[4:]\n",
    "                elif len(dates) == 2:\n",
    "                    dates = [dates[0]] + [dates[0]] + dates[0:]\n",
    "                else:\n",
    "                    dates = dates[:2] + [dates[2]] + [dates[2]] + dates[2:]\n",
    "            if actions[-1] == 'CHAPTERED' and len(actions) - len(dates) == 3:\n",
    "                dates = dates + [dates[-1]] + [dates[-1]] + [dates[-1]]\n",
    "            if actions[-2:] == ['ENROLLED', 'VETOED'] and len(actions) - len(dates) > 0 :\n",
    "                dates = dates[:-4] + [dates[-4]]  + [dates[-3]] + dates[-3:]\n",
    "\n",
    "            if len(dates) < len(actions) and'ENROLLED' in actions and actions.index('ENROLLED') < len(actions) - 1:\n",
    "                for i in range(len(actions) - actions.index('ENROLLED')):\n",
    "                    dates = dates + [dates[-1]]\n",
    "        if len(actions) + 1 == len(dates):\n",
    "            dates = dates[:-1]\n",
    "        try:\n",
    "            action_df = pd.DataFrame({'date': dates, 'action': actions})\n",
    "        except:\n",
    "            return None, None, None\n",
    "        action_df['date'] = pd.to_datetime(action_df['date'], errors='coerce')\n",
    "        order_df = action_df.loc[~action_df['action'].isin(['FILED', 'PASSED_ASSEMBLY', 'PASSED_SENATE', 'APPROVED'])]\n",
    "        repair_flag = False\n",
    "        if order_df.shape[0] > len(versions_):\n",
    "            version_ends = [re.search(r'INT|AMD|ENR|CHP|PRO', v).group() for v in versions_]\n",
    "            if 'ENR' in version_ends:\n",
    "                v_enr = version_ends.index('ENR')\n",
    "                extension = [versions_[v_enr - 1] if v_enr - 1 != 0 else versions_[v_enr] for _ in range(len(order_df) - len(versions_))]\n",
    "                versions_ = versions_[:v_enr] + extension + versions_[v_enr:]\n",
    "            else:\n",
    "                repair_flag = True\n",
    "        vr = pd.DataFrame({'version': versions_})\n",
    "        if vr.shape[0] == 0:\n",
    "            return None, None, None\n",
    "        try:\n",
    "            vr['v_num'] = vr['version'].apply(lambda x: re.search(r'\\d{2}(?=INT|AMD|ENR|CHP|PRO)', x).group()).astype(int)\n",
    "        except:\n",
    "            return None, None, None\n",
    "        vr = vr.sort_values('v_num', ascending=False).reset_index(drop=True)\n",
    "        if repair_flag:\n",
    "            last_v = vr.loc[vr['version'].notna()].iloc[-1]['version']\n",
    "            last_v_num = float(re.search(r'\\d{2}(?=INT|AMD|ENR|CHP|PRO)', last_v).group())\n",
    "            for i in range(len(order_df) - len(versions_)):\n",
    "                vr.loc[len(vr) + i, 'version'] = last_v\n",
    "                vr.loc[len(vr) + i, 'v_num'] = last_v_num\n",
    "\n",
    "        order_df['version'] = vr['version']\n",
    "        order_df['order'] = range(1, len(order_df) + 1)\n",
    "        outcomes = order_df['action'].tolist()\n",
    "        if 'CHAPTERED' in outcomes or 'FILED' in outcomes:\n",
    "            if 'VETOED' in outcomes:\n",
    "                outcome = 'VETOED'\n",
    "            else:\n",
    "                outcome = 'CHAPTERED'\n",
    "        else:\n",
    "            outcome = 'FAILED'\n",
    "        return action_df, order_df, outcome\n",
    "\n",
    "class BillVersion(Node):\n",
    "    def __init__(self, bill_id, version_id, digest, vote_required, local_program, fiscal_com, tax_levy, urgency):\n",
    "        features = {\n",
    "            'digest': digest,\n",
    "            'VoteRequired': vote_required,\n",
    "            'LocalProgram': local_program,\n",
    "            'FiscalCommittee': fiscal_com,\n",
    "            'TaxLevy': tax_levy,\n",
    "            'Urgency': urgency,\n",
    "            'date': None\n",
    "        }\n",
    "        super().__init__(version_id, \"bill_version\", features)\n",
    "        self.bill_id = bill_id\n",
    "        self.actions = {}\n",
    "\n",
    "    def add_actions(self, location, date):\n",
    "        if location not in self.actions:\n",
    "            self.actions[location] = []\n",
    "        self.actions[location].append(date)\n",
    "\n",
    "    def add_date(self, date):\n",
    "        self.features['date'] = date\n",
    "\n",
    "class Legislator(Node):\n",
    "    def __init__(self, legislator_id, party, occupation):\n",
    "        features = {\n",
    "            'party': party,\n",
    "            'occupation': occupation\n",
    "        }\n",
    "        super().__init__(legislator_id, \"legislator\", features)\n",
    "        self.terms = []\n",
    "\n",
    "class LegislatorTerm(Node):\n",
    "    def __init__(self, term, legislator_id, chamber, district):\n",
    "        features = {\n",
    "            'chamber': chamber,\n",
    "            'district': district,\n",
    "            'term': term\n",
    "        }\n",
    "        node_id = f\"{legislator_id}_{term}_{chamber}\"\n",
    "        super().__init__(node_id, \"legislator_term\", features)\n",
    "        self.committees = []\n",
    "        self.committee_positions = []\n",
    "\n",
    "    def add_committee(self, committee_id):\n",
    "        self.committees.append(committee_id)\n",
    "\n",
    "    def add_committee_position(self, committee_id, position):\n",
    "        self.committee_positions.append((committee_id, position))\n",
    "\n",
    "class Committee(Node):\n",
    "    def __init__(self, committee_id, name, chamber, term):\n",
    "        features = {\n",
    "            'name': name,\n",
    "            'chamber': chamber\n",
    "        }\n",
    "        term_ = term.split('-')[0]\n",
    "        id = f\"{committee_id}_{term_}\"\n",
    "        super().__init__(id, \"committee\", features)\n",
    "        self.members = []\n",
    "\n",
    "    def add_member(self, legislator_id):\n",
    "        self.members.append(legislator_id)\n",
    "\n",
    "class LobbyFirm(Node):\n",
    "    def __init__(self, firm_id, name):\n",
    "        features = {\n",
    "            'name': name\n",
    "        }\n",
    "        super().__init__(firm_id, \"lobby_firm\", features)\n",
    "        self.total_donations = 0\n",
    "\n",
    "    def add_donation(self, amount):\n",
    "        self.total_donations += amount\n",
    "\n",
    "class Donor(Node):\n",
    "    def __init__(self, donor_id, name):\n",
    "        features = {\n",
    "            'name': name\n",
    "        }\n",
    "        super().__init__(donor_id, \"donor\", features)\n",
    "        self.total_donations = 0\n",
    "\n",
    "    def add_donation(self, amount):\n",
    "        self.total_donations += amount\n",
    "\n",
    "class Vote(Edge):\n",
    "    def __init__(self, legislator, bill_version, vote, motion, date, direction):\n",
    "        attributes = {\n",
    "            'vote': 1 if vote == 'AYE' else -1 if vote == 'NOE' else 0,\n",
    "            'motion': motion,\n",
    "            'date': date\n",
    "        }\n",
    "        if direction == 1:\n",
    "            super().__init__(legislator, bill_version, 'voted_on', attributes)\n",
    "        else:\n",
    "            super().__init__(bill_version, legislator, 'vote_from', attributes)\n",
    "\n",
    "class CommitteeMembership(Edge):\n",
    "    def __init__(self, legislator, committee, position, direction):\n",
    "        attributes = {\n",
    "            'position': position\n",
    "        }\n",
    "        if direction == 1:\n",
    "            super().__init__(legislator, committee, 'member_of', attributes)\n",
    "        else:\n",
    "            super().__init__(committee, legislator, 'has_member', attributes)\n",
    "        committee.add_member(legislator)\n",
    "\n",
    "class Sponsorship(Edge):\n",
    "    def __init__(self, legislator, bill_version, author_type, direction):\n",
    "        attributes = {\n",
    "            'author_type': author_type\n",
    "        }\n",
    "        if direction == 1:\n",
    "            super().__init__(legislator, bill_version, 'wrote', attributes)\n",
    "        else:\n",
    "            super().__init__(bill_version, legislator, 'written_by', attributes)\n",
    "\n",
    "class Reading(Edge):\n",
    "    def __init__(self, bill_version, committee, date, direction):\n",
    "        attributes = {\n",
    "            'date': date\n",
    "        }\n",
    "        if direction == 1:\n",
    "            super().__init__(committee, bill_version, 'read', attributes)\n",
    "        else:\n",
    "            super().__init__(bill_version, committee, 'read_by', attributes)\n",
    "\n",
    "class Donation(Edge):\n",
    "    def __init__(self, donor, recipient, amount, date, type, direction=1):\n",
    "        attributes = {\n",
    "            'amount': amount,\n",
    "            'date': date\n",
    "        }\n",
    "        if type == 'CampaignContribution':\n",
    "            if direction == 1:\n",
    "                super().__init__(donor, recipient, 'donated_to', attributes)\n",
    "            else:\n",
    "                super().__init__(recipient, donor, 'has_donation', attributes)\n",
    "        else:\n",
    "            if direction == 1:\n",
    "                super().__init__(donor, recipient, 'lobbied', attributes)\n",
    "            else:\n",
    "                super().__init__(recipient, donor, 'has_lobbying', attributes)\n",
    "\n",
    "class Version(Edge):\n",
    "    def __init__(self, bill_version, bill, order, direction):\n",
    "        attributes = {\n",
    "            'order': order\n",
    "        }\n",
    "        if direction == 1:\n",
    "            super().__init__(bill_version, bill, 'is_version', attributes)\n",
    "        else:\n",
    "            super().__init__(bill, bill_version, 'has_version', attributes)\n",
    "\n",
    "class siblingVersion(Edge):\n",
    "    def __init__(self, version1, version2, direction):\n",
    "        if direction == 1:\n",
    "            super().__init__(version1, version2, 'priorVersion')\n",
    "        else:\n",
    "            super().__init__(version2, version1, 'nextVersion')\n",
    "\n",
    "class samePerson(Edge):\n",
    "    def __init__(self, node1, node2):\n",
    "        super().__init__(node1, node2, 'samePerson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphBuilder:\n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "        self.edges = []\n",
    "        self.versions = []\n",
    "\n",
    "    def add_version(self, version):\n",
    "        self.versions.append(version)\n",
    "\n",
    "    def add_node(self, node):\n",
    "        key = (node.type, node.id)\n",
    "        if key not in self.nodes:\n",
    "            self.nodes[key] = node\n",
    "\n",
    "    def get_node(self, type_, id_):\n",
    "        return self.nodes.get((type_, id_))\n",
    "\n",
    "    def add_edge(self, edge):\n",
    "        self.edges.append(edge)\n",
    "\n",
    "    def build(self):\n",
    "        return list(self.nodes.values()), self.edges\n",
    "\n",
    "    def add_bills(self, bill_ids, titles, subjects, titles_embs, subjects_embs, features):\n",
    "        def process_single_bill(bill):\n",
    "            try:\n",
    "                title = titles.get(bill, '')\n",
    "                subject = subjects.get(bill, '')\n",
    "                title_emb = titles_embs.get(title, None)\n",
    "                subject_emb = subjects_embs.get(subject, None)\n",
    "                measure_type = re.search(r'[A-Za-z]+', bill).group()\n",
    "                bill_node = Bill(bill, title_emb, subject_emb, measure_type)\n",
    "\n",
    "                versions = version_id_mapping.get(bill, [])\n",
    "                versions_ = []\n",
    "                dates = introduction_dates.get(bill, {}).get('Dates', [])\n",
    "                fd = sorted(list(set(dates)))[0]\n",
    "                bill_node.add_date(fd)\n",
    "\n",
    "                if not versions:\n",
    "                    return\n",
    "\n",
    "                for version in versions:\n",
    "                    digest = features[version]['digest']\n",
    "                    if str(digest) == 'nan' or version.endswith('VETO'):\n",
    "                        continue\n",
    "\n",
    "                    digest_emb = digest_embeddings.get(digest, None)\n",
    "                    if digest_emb is None:\n",
    "                        continue\n",
    "\n",
    "                    version_node = BillVersion(\n",
    "                        bill, version, digest_emb,\n",
    "                        features[version]['VoteRequired'],\n",
    "                        features[version]['LocalProgram'],\n",
    "                        features[version]['FiscalCommittee'],\n",
    "                        features[version]['TaxLevy'],\n",
    "                        features[version]['Urgency']\n",
    "                    )\n",
    "\n",
    "                    self.add_node(version_node)\n",
    "                    if version not in self.versions:\n",
    "                        self.versions.append(version)\n",
    "                    versions_.append(version)\n",
    "\n",
    "                action_df, order_df, outcome = bill_node.align_actions_versions(bill, versions_, dates)\n",
    "                bill_node.add_actions(action_df)\n",
    "                bill_node.add_order_df(order_df)\n",
    "                bill_node.add_outcome(outcome)\n",
    "\n",
    "                self.add_node(bill_node)\n",
    "                for i, row in order_df.drop_duplicates(subset='version').iterrows():\n",
    "                    version_node = self.get_node('bill_version', row['version'])\n",
    "                    version_node.add_date(row['date'])\n",
    "                    if version_node:\n",
    "                        self.add_edge(Version(version_node, bill_node, row['order'], 1))\n",
    "                        if i < len(order_df) - 1:\n",
    "                            v2_node = self.get_node('bill_version', order_df.iloc[i + 1]['version'])\n",
    "                            if v2_node:\n",
    "                                self.add_edge(siblingVersion(version_node, v2_node, 1))\n",
    "            except:\n",
    "                pass\n",
    "        for bill in tqdm(bill_ids):\n",
    "            process_single_bill(bill)\n",
    "\n",
    "    def add_legislators(self, legislators_):\n",
    "        for legislator in tqdm(legislators_):\n",
    "            leg_name = legislators[legislator]\n",
    "            party = leg_parties.get(leg_name)\n",
    "            occupation = leg_occupations.get(leg_name)\n",
    "            occ_embedding = occ_embeddings.get(occupation, None)\n",
    "            legislator_node = Legislator(legislator, party, occ_embedding)\n",
    "            self.add_node(legislator_node)\n",
    "            terms = politicians.loc[politicians['full_name'] == leg_name, ['Term', 'District No.', 'chamber']].drop_duplicates()\n",
    "            for _, term in terms.iterrows():\n",
    "                term_node = LegislatorTerm(term['Term'], legislator, term['chamber'], term['District No.'])\n",
    "                self.add_node(term_node)\n",
    "                self.add_edge(samePerson(legislator_node, term_node))\n",
    "\n",
    "    def add_committees(self, committees_df):\n",
    "        for _, row in tqdm(committees_df[['committee_clean', 'Term', 'chamber']].drop_duplicates().iterrows(), total=committees_df[['committee_clean', 'Term', 'chamber']].drop_duplicates().shape[0]):\n",
    "            committee_name = committee_embeddings.get(row['committee_clean'].lower(), None)\n",
    "            committee_id = committee_codes.get(row['committee_clean'].lower(), None)\n",
    "            committee_node = Committee(committee_id, committee_name, row['chamber'], row['Term'])\n",
    "            self.add_node(committee_node)\n",
    "            term = row['Term']\n",
    "            members = politicians.loc[(politicians['committee_clean'] == row['committee_clean']) & (politicians['Term'] == row['Term']), ['position', 'full_name', 'chamber']].drop_duplicates()\n",
    "            for _, member in members.iterrows():\n",
    "                leg_id = legislator_codes[member['full_name']]\n",
    "                leg_node_id = f\"{leg_id}_{term}_{member['chamber']}\"\n",
    "                leg_node = self.get_node('legislator_term', leg_node_id)\n",
    "                self.add_edge(CommitteeMembership(leg_node, committee_node, member['position'], 1))\n",
    "                committee_node.add_member(leg_node_id)\n",
    "                leg_node.add_committee(committee_name)\n",
    "\n",
    "    def add_votes(self):\n",
    "        for _, row in tqdm(bill_votes.loc[bill_votes['bill_id'].isin(bill_ids)].iterrows(), total=bill_votes.loc[bill_votes['bill_id'].isin(bill_ids)].shape[0]):\n",
    "            bill_node = self.get_node('bill', row['bill_id'])\n",
    "            if bill_node is None:\n",
    "                continue\n",
    "            order_df = bill_node.order_df\n",
    "            if order_df is None:\n",
    "                continue\n",
    "            try:\n",
    "                v = order_df.loc[order_df['date'] <= row['vote_date_time'], 'version'].values[-1]\n",
    "            except:\n",
    "                v = order_df['version'].values[-1]\n",
    "            if v is None:\n",
    "                continue\n",
    "            v_node = self.get_node('bill_version', v)\n",
    "            if v_node is None:\n",
    "                continue\n",
    "            last = row['legislator_name'].strip().lower()\n",
    "            legislator = legislators_last_names.get((row['chamber'], last, row['term']), None)\n",
    "            if legislator is None:\n",
    "                if len(last.split(' ')) > 1:\n",
    "                    legislator = row['legislator_name']\n",
    "                else:\n",
    "                    continue\n",
    "            legislator_id = legislator_codes.get(legislator, None)\n",
    "            leg_term_node = self.get_node('legislator_term', f\"{legislator_id}_{row['term']}_{row['chamber']}\")\n",
    "            if leg_term_node is None:\n",
    "                continue\n",
    "            vote = row['vote_code']\n",
    "            motion_id = row['motion_id']\n",
    "            motion_text = motion_codes.get(motion_id, None)\n",
    "            if motion_text is None:\n",
    "                continue\n",
    "            if row['location_code'] not in ['AFLOOR', 'SFLOOR']:\n",
    "                actions = v_node.actions\n",
    "                if row['location_code'] in actions:\n",
    "                    if row['vote_date_time'] not in actions[row['location_code']]:\n",
    "                        actions[row['location_code']].append(row['vote_date_time'])\n",
    "            if motion_text is None:\n",
    "                motion_embedding = ''\n",
    "            else:\n",
    "                motion_embedding = motion_embeddings.get(motion_text, None)\n",
    "            self.add_edge(Vote(leg_term_node, v_node, vote, motion_embedding, row['vote_date_time'], 1))\n",
    "\n",
    "\n",
    "    def add_readings(self):\n",
    "        for version in self.versions:\n",
    "            v_node = self.get_node('bill_version', version)\n",
    "            if v_node is None:\n",
    "                continue\n",
    "            actions = v_node.actions\n",
    "            for location, dates in actions.items():\n",
    "                if location.startswith('CS') or location.startswith('CX'):\n",
    "                    com = committee_matches.get(location)\n",
    "                    if com is None:\n",
    "                        continue\n",
    "                    committee_id = committee_codes.get(com.lower(), None)\n",
    "                    for date in dates:\n",
    "                        year = date.year\n",
    "                        if year % 2 == 0:\n",
    "                            year -= 1\n",
    "                        com_node = self.get_node('committee', f\"{committee_id}_{year}\")\n",
    "                        if com_node is None:\n",
    "                            continue\n",
    "                        self.add_edge(Reading(v_node, com_node, date, 1))\n",
    "\n",
    "    def add_sponsorships(self, sponsors):\n",
    "        for _, row in tqdm(sponsors.iterrows(), total=sponsors.shape[0]):\n",
    "            version = row['bill_ID']\n",
    "            version_node = self.get_node('bill_version', version)\n",
    "            if version_node is None:\n",
    "                continue\n",
    "            if row['House'] == 'UNKNOWN':\n",
    "                com = author_com_matches.get(row['Name'], None)\n",
    "                year = row['term'].split('-')[0]\n",
    "                com_node = self.get_node('committee', f\"{com}_{year}\")\n",
    "                if com_node is None:\n",
    "                   continue\n",
    "                self.add_edge(Sponsorship(com_node, version_node, row['Contribution'], 1))\n",
    "            else:\n",
    "                if row['Name'] in ['Mark Stone', 'Cristina Garcia', 'John Campbell', 'Bill Campbell', 'Eduardo Garcia']:\n",
    "                    leg_id = legislator_codes.get(row['Name'])\n",
    "                    leg_node = self.get_node('legislator_term', f\"{leg_id}_{row['term']}_{row['House'].lower()}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        name = re.sub(r'\\'', '', re.sub(r'-', ' ', row['Name'])).lower().strip()\n",
    "                    except:\n",
    "                        continue\n",
    "                    leg_name = legislators_last_names.get((row['House'].lower(), name, row['term']), None)\n",
    "                    if leg_name is None:\n",
    "                        continue\n",
    "                    leg_id = legislator_codes.get(leg_name)\n",
    "                    leg_node = self.get_node('legislator_term', f\"{leg_id}_{row['term']}_{row['House'].lower()}\")\n",
    "                self.add_edge(Sponsorship(leg_node, version_node, row['Contribution'], 1))\n",
    "\n",
    "\n",
    "    def add_lobbyists(self, lobbyists):\n",
    "        for key in tqdm(lobbyists.keys(), total=len(lobbyists.keys())):\n",
    "            lobbyist = LobbyFirm(key, lobbyists[key])\n",
    "            self.add_node(lobbyist)\n",
    "\n",
    "    def add_donations(self, donations):\n",
    "        for _, row in tqdm(donations.iterrows(), total=donations.shape[0]):\n",
    "            firm = row['FIRM_NAME']\n",
    "            firm_node = self.get_node('lobby_firm', firm)\n",
    "            if firm_node is None:\n",
    "                continue\n",
    "            if row['clean_beneficiary'] in committee_codes:\n",
    "                com = committee_codes.get(row['clean_beneficiary'])\n",
    "                year = row['term'].split('-')[0]\n",
    "                com_node = self.get_node('committee', f\"{com}_{year}\")\n",
    "                if com_node is None:\n",
    "                    continue\n",
    "                self.add_edge(Donation(firm_node, com_node, row['BENE_AMT'], row['EXPN_DATE'], 'Lobbying', 1))\n",
    "                firm_node.add_donation(row['BENE_AMT'])\n",
    "\n",
    "            else:\n",
    "                dicti = pol_names_terms.get((row['clean_beneficiary'], row['term']), None)\n",
    "                chamber = dicti['chamber'] if dicti is not None else None\n",
    "                name = dicti['name'] if dicti is not None else None\n",
    "                if chamber is None or name is None:\n",
    "                    continue\n",
    "                leg_id = legislator_codes.get(name)\n",
    "                leg_node = self.get_node('legislator_term', f\"{leg_id}_{row['term']}_{chamber}\")\n",
    "                if leg_node is None:\n",
    "                    continue\n",
    "                self.add_edge(Donation(firm_node, leg_node, row['BENE_AMT'], row['EXPN_DATE'], 'Lobbying', 1))\n",
    "                firm_node.add_donation(row['BENE_AMT'])\n",
    "\n",
    "    def add_donors(self, donors):\n",
    "        for donor in tqdm(donors.keys(), total=len(donors.keys())):\n",
    "            donor_embedding = donors[donor]\n",
    "            donor = Donor(donor, donor_embedding)\n",
    "            self.add_node(donor)\n",
    "\n",
    "    def add_contributions(self, contributions):\n",
    "        for _, row in tqdm(contributions.iterrows(), total=contributions.shape[0]):\n",
    "            expender = row['ExpenderName']\n",
    "            expender_node = self.get_node('donor', expender)\n",
    "            if expender_node is None:\n",
    "                continue\n",
    "            recipient = row['full_name']\n",
    "            recipient_id = legislator_codes.get(recipient)\n",
    "            recipient_node = self.get_node('legislator_term', f\"{recipient_id}_{row['Term']}_{row['chamber']}\")\n",
    "            if recipient_node is None:\n",
    "                continue\n",
    "            self.add_edge(Donation(expender_node, recipient_node, row['Amount'], row['DateEnd'], 'CampaignContribution', 1))\n",
    "            expender_node.add_donation(row['Amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45373/45373 [03:51<00:00, 196.06it/s]\n",
      "100%|██████████| 508/508 [00:00<00:00, 1009.19it/s]\n",
      "100%|██████████| 674859/674859 [00:12<00:00, 55946.84it/s]\n"
     ]
    }
   ],
   "source": [
    "builder = GraphBuilder()\n",
    "builder.add_bills(bill_ids, bill_titles, bill_subjects, title_embeddings, subject_embeddings, features)\n",
    "builder.add_legislators(legislators)\n",
    "builder.add_sponsorships(sponsors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1882/1882 [00:02<00:00, 659.85it/s]\n",
      "100%|██████████| 1325/1325 [00:00<00:00, 888055.74it/s]\n",
      "100%|██████████| 104110/104110 [00:01<00:00, 66174.77it/s]\n",
      "100%|██████████| 1136/1136 [00:00<00:00, 856657.56it/s]\n",
      "100%|██████████| 7028/7028 [00:00<00:00, 62291.20it/s]\n"
     ]
    }
   ],
   "source": [
    "builder.add_committees(politicians)\n",
    "builder.add_lobbyists(lobbying_firms_embeddings)\n",
    "builder.add_donations(lob)\n",
    "builder.add_donors(donor_embeddings)\n",
    "builder.add_contributions(campaign_contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6636032/6636032 [10:42<00:00, 10323.58it/s] \n"
     ]
    }
   ],
   "source": [
    "builder.add_votes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, edges = builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_type_map = {\n",
    "    'LEAD_AUTHOR': 'LEAD_AUTHOR',\n",
    "    'PRINCIPAL_COAUTHOR': 'PRINCIPAL_COAUTHOR',\n",
    "    'COAUTHOR': 'COAUTHOR',\n",
    "    'data=\"COAUTHOR\"': 'COAUTHOR',\n",
    "    'data=\"LEAD_AUTHOR\"': 'LEAD_AUTHOR',\n",
    "    'data=\"PRINCIPAL_COAUTHOR\"': 'PRINCIPAL_COAUTHOR',\n",
    "    'nan': 'AUTHOR'\n",
    "}\n",
    "author_levels = {\n",
    "    'AUTHOR': 1,\n",
    "    'COAUTHOR': 1,\n",
    "    'PRINCIPAL_COAUTHOR': 2,\n",
    "    'LEAD_AUTHOR': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "measure_types = bill_vers['MeasureType'].unique()\n",
    "parties = politicians['Party'].unique()\n",
    "chambers = politicians['chamber'].unique()\n",
    "outcome_mapping = {'CHAPTERED': 1, 'VETOED': 0, 'FAILED': -1, 'ENROLLED': 1}\n",
    "measure_encoder = LabelEncoder()\n",
    "measure_encoder.fit(measure_types)\n",
    "party_encoder = LabelEncoder()\n",
    "party_encoder.fit(parties)\n",
    "chamber_encoder = LabelEncoder()\n",
    "chamber_encoder.fit(chambers)\n",
    "pos = list(positions.values()) + ['member']\n",
    "pos_encoder = LabelEncoder()\n",
    "pos_encoder.fit(pos)\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_encoders = {\n",
    "    'measure_type': measure_encoder,\n",
    "    'party': party_encoder,\n",
    "    'chamber': chamber_encoder,\n",
    "    'position': pos_encoder\n",
    "}\n",
    "\n",
    "def get_concat_features(attrs, node=False):\n",
    "    features = []\n",
    "    for k, v in attrs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            features.append(v.detach().cpu().numpy())\n",
    "        elif isinstance(v, np.ndarray):\n",
    "            try:\n",
    "                features.append(v[:, 0])\n",
    "            except:\n",
    "                features.append(v.reshape(-1))\n",
    "        else:\n",
    "            if (v is None) or (v == ''):\n",
    "                vals = 0\n",
    "                if node == True:\n",
    "                    if k in ['title', 'subject', 'digest', 'occupation', 'name']:\n",
    "                        vals = np.zeros((1, 384), dtype=np.float32).reshape(-1)\n",
    "            elif k == 'date':\n",
    "                vals = v.timestamp() if v is not None and v is not pd.NaT else 0\n",
    "            elif k == 'author_type':\n",
    "                vals = author_levels[author_type_map[str(v)]] if str(v) in author_levels else 1\n",
    "            elif k == 'term':\n",
    "                vals = int(v.split('-')[0]) if v is not None else 2000\n",
    "            elif k == 'district':\n",
    "                t = re.sub(r'[^\\d]', '', str(v)) if str(v) != '' else 0\n",
    "                vals = float(t) if t != '' else 0.0\n",
    "            elif k in ['VoteRequired', 'LocalProgram', 'FiscalCommittee', 'TaxLevy', 'Urgency']:\n",
    "                vals = int(str(v).lower() == 'yes') if v is not None else 0\n",
    "            elif k in feature_encoders:\n",
    "                if k == 'position':\n",
    "                    vals = pos_encoder.transform([positions.get(v, 'member')])[0] if v is not None else 0\n",
    "                else:\n",
    "                    vals = feature_encoders[k].transform([v])[0] if v is not None else 0\n",
    "            else:\n",
    "                try:\n",
    "                    t = float(v)\n",
    "                    vals = v\n",
    "                except:\n",
    "                    vals = 0\n",
    "            features.append([vals])\n",
    "    try:\n",
    "        x = np.concatenate(features, axis=-1).astype(np.float32).reshape(1, -1)\n",
    "        return x\n",
    "    except:\n",
    "        return 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "node_id_map = defaultdict(dict)\n",
    "nodes_by_type = defaultdict(list)\n",
    "edges_by_type = defaultdict(list)\n",
    "clean_edges = []\n",
    "\n",
    "for node in nodes:\n",
    "    nodes_by_type[node.type].append(node)\n",
    "for edge in edges:\n",
    "    try:\n",
    "        edge_type = (edge.source.type, edge.relation, edge.target.type)\n",
    "        edges_by_type[edge_type].append(edge)\n",
    "        clean_edges.append(edge)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "node_attrs = defaultdict(list)\n",
    "\n",
    "for ntype, nlist in nodes_by_type.items():\n",
    "    outcomes = []\n",
    "    bad_count = 0\n",
    "    bad_ids = []\n",
    "    for node in nlist:\n",
    "        attrs = node.features\n",
    "        x_attr = get_concat_features(attrs, node=True)\n",
    "        if not isinstance(x_attr, np.ndarray):\n",
    "            if x_attr == 'bad':\n",
    "                bad_count += 1\n",
    "                bad_ids.append(node.id)\n",
    "                continue\n",
    "        elif 'bad' in x_attr:\n",
    "            bad_count += 1\n",
    "            bad_ids.append(node.id)\n",
    "            continue\n",
    "        node_attrs[ntype].append(x_attr)\n",
    "        if ntype == 'bill':\n",
    "            outcome = node.outcome\n",
    "            o = outcome_mapping.get(outcome, -1)\n",
    "            outcomes.append(o)\n",
    "    if ntype == 'bill':\n",
    "        data[ntype].y = np.array(outcomes, dtype=np.int64)\n",
    "    data[ntype].x = np.vstack(node_attrs[ntype])\n",
    "    data[ntype].num_nodes = len(nlist) - bad_count\n",
    "    node_id_map[ntype] = {node.id: i for i, node in enumerate(nlist) if node.id not in bad_ids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indices = defaultdict(list)\n",
    "edge_attrs = defaultdict(list)\n",
    "\n",
    "for etype, elist in edges_by_type.items():\n",
    "    for edge in elist:\n",
    "        try:\n",
    "            src = node_id_map[edge.source.type][edge.source.id]\n",
    "            dst = node_id_map[edge.target.type][edge.target.id]\n",
    "        except:\n",
    "            continue\n",
    "        edge_indices[etype].append([src, dst])\n",
    "        edge_attrs[etype].append(get_concat_features(edge.attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "for etype in edge_indices.keys():\n",
    "    edge_idx = torch.tensor(edge_indices[etype], dtype=torch.long).t().contiguous()\n",
    "    data[etype].edge_index = edge_idx\n",
    "    if etype in edge_attrs.keys():\n",
    "        if edge_attrs[etype] is None:\n",
    "            continue\n",
    "        elif np.any(edge_attrs[etype] == np.array([['bad']])):\n",
    "            edge_attrs[etype] = [np.array([[1]]) if i[0] == 'bad' else np.array([[0]]) for i in edge_attrs[etype]]\n",
    "    edge_attr_np = np.vstack(edge_attrs[etype])\n",
    "    try:\n",
    "        edge_attr = torch.from_numpy(edge_attr_np).float()\n",
    "    except:\n",
    "        print(edge_attr_np, etype)\n",
    "        break\n",
    "    data[etype].edge_attr = edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, 'data2.pt', _use_new_zipfile_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('node_id_map.json', 'w') as f:\n",
    "    json.dump(node_id_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
